{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ForgettingExperimentsPrototyping02.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xdKbAVAZLL5i",
        "JfeXT5AIcEiu",
        "kxxpz0Xt9q7U",
        "k1Efw9G_m_NQ",
        "A1F9UaZtoC_D",
        "__G2IL3BxO4C",
        "J4aQINAGX9SV",
        "XhDGumesrduI",
        "F5mvvKXlrjJr",
        "cnpFsViG2ZQN",
        "R_vApCQ51OlF",
        "RfTpGGsyNb-d",
        "vsHUYGJ1NQ_4",
        "QVjCyB0zMfGr",
        "qTTKgxZXTfPu",
        "VPLbRXCh2uD5",
        "hLmSL9HbPzQc",
        "TIY1nUvVm1Ve",
        "s34DXhO0m5XT",
        "kO-oq4RrP5C5",
        "TSsOxYO_I4Yo",
        "u4mPZRXAMP85",
        "evNg3ZqdCWEh",
        "jOPxRBpLciZL",
        "ufGZLoc7hokL",
        "w1yCWFaGrVrr",
        "QPbhYtsK5Crf",
        "bTX7A8KSMPMc",
        "fi_Uu1JiX_m6",
        "qGz9e7sExjm2",
        "7w1C3IYgjsRQ",
        "FGf3iq85HrPy",
        "6srqpyvbrUir",
        "X3f80Ldoq8Gd",
        "twkPKC459Vm7"
      ],
      "mount_file_id": "13sSgtsBJo31se1Ewj4t8F_yXeAHKTlDP",
      "authorship_tag": "ABX9TyNjasX23k/lKf1cX3ng62KP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15b4f8e413f34476913f6c3d139c6b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f88737a5019540af89365750e9fd1b85",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d229702e78b849beb1922ce2c5f7b9cc",
              "IPY_MODEL_f49dff685af7482f8e279c7db409a0bc"
            ]
          }
        },
        "f88737a5019540af89365750e9fd1b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d229702e78b849beb1922ce2c5f7b9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0494128c835e4859a85011783c8fa4bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9471c9450ebe4eb1b44a21ef0e3b29f3"
          }
        },
        "f49dff685af7482f8e279c7db409a0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b31da9bf022740cdbd9cc2e11f765967",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [02:02&lt;00:00, 1390430.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60ae916855b743a5ad6c466f9ac9ad3d"
          }
        },
        "0494128c835e4859a85011783c8fa4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9471c9450ebe4eb1b44a21ef0e3b29f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b31da9bf022740cdbd9cc2e11f765967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60ae916855b743a5ad6c466f9ac9ad3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67aaf2c1caac413a8563c80c115b51ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4bca9075b8fe4702961b9a2ce17044c1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1606e7fd011d423488301a56b50d0e6e",
              "IPY_MODEL_28d7896ed93f484295e70670860d0cfb",
              "IPY_MODEL_840a97d46b904a5382d957e4a8615677"
            ]
          }
        },
        "4bca9075b8fe4702961b9a2ce17044c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1606e7fd011d423488301a56b50d0e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31a9ab0de46b40dca391239c199b007d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82c90101d9434ddc927dbcc61b4c4fee"
          }
        },
        "28d7896ed93f484295e70670860d0cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_248e1d244111472e9c380185b26f2599",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10da766354044f04a89ced9d5f7da342"
          }
        },
        "840a97d46b904a5382d957e4a8615677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14874a6390e04e96ad9472b1434a9c31",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:04&lt;00:00, 47308419.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c97d5e712f6442efafab7a159a91893a"
          }
        },
        "31a9ab0de46b40dca391239c199b007d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82c90101d9434ddc927dbcc61b4c4fee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "248e1d244111472e9c380185b26f2599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10da766354044f04a89ced9d5f7da342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14874a6390e04e96ad9472b1434a9c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c97d5e712f6442efafab7a159a91893a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39438d4c06e5441e91b4d8a5e33a8dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d51a33872b3d49819d32e51627942bf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0f1dab00d9874020b18fc00639955a20",
              "IPY_MODEL_6076b94f793c4501b6554ba4f326493d"
            ]
          }
        },
        "d51a33872b3d49819d32e51627942bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f1dab00d9874020b18fc00639955a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0a7127ab99884f3195c33b1537a4fd78",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31ea81610fa64df1bd15b66bd35091c2"
          }
        },
        "6076b94f793c4501b6554ba4f326493d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d55a2e9ebdd84104b31128fd7b20aefa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [1:24:41&lt;00:00, 33554.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9936469fdaa747949307583e96a59fd2"
          }
        },
        "0a7127ab99884f3195c33b1537a4fd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31ea81610fa64df1bd15b66bd35091c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d55a2e9ebdd84104b31128fd7b20aefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9936469fdaa747949307583e96a59fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a1eb70b47be446f9956b6842bb450ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37f8accd8f584ccea0b009a3d84412b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_404ed0be6f89472ab7c97a0c9695879e",
              "IPY_MODEL_759a62f4a8e7425293fdd823e076ccac",
              "IPY_MODEL_84a7809041fc4dd4887e94377cd82f57"
            ]
          }
        },
        "37f8accd8f584ccea0b009a3d84412b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "404ed0be6f89472ab7c97a0c9695879e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1df876c8f594c0c8e6822a075cd08b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f70b00dc6694726a737e6f58590eaf2"
          }
        },
        "759a62f4a8e7425293fdd823e076ccac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6660faf481e4f3083a8dc9d0e8e1657",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c32ea2be7f724f1f94d0823a7b25723d"
          }
        },
        "84a7809041fc4dd4887e94377cd82f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a6713c8b7b84b3e866db02a1a1a3a75",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:02&lt;00:00, 79605677.15it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb00a2996ef44ef9bd490b4236c3dfd2"
          }
        },
        "a1df876c8f594c0c8e6822a075cd08b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f70b00dc6694726a737e6f58590eaf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6660faf481e4f3083a8dc9d0e8e1657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c32ea2be7f724f1f94d0823a7b25723d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a6713c8b7b84b3e866db02a1a1a3a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb00a2996ef44ef9bd490b4236c3dfd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60bfbf765a804650b4e4e089ea7fe56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14bb6a1a0d1b466eab8e1a2d80ae08d9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f565a7f6ec934c8c8be6375ae9f1b7b7",
              "IPY_MODEL_8940f65f5792489a9b7847fb428ab3d6"
            ]
          }
        },
        "14bb6a1a0d1b466eab8e1a2d80ae08d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f565a7f6ec934c8c8be6375ae9f1b7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5bd28a0c4c534be0a23896225e80a0cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bdcbb1f40d8b45c6a66c679954cceeb8"
          }
        },
        "8940f65f5792489a9b7847fb428ab3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0a70b26bfa44fb6a4850ad5e43513c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 25687587.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35f5793010fe43c2bda76c91bbae1455"
          }
        },
        "5bd28a0c4c534be0a23896225e80a0cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bdcbb1f40d8b45c6a66c679954cceeb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0a70b26bfa44fb6a4850ad5e43513c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35f5793010fe43c2bda76c91bbae1455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "035ba7db34bc4c31996774e572debcbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_00553b164fe941cda24fa9f4d27f67e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6f34c72409ba4f4b93bc61fe8124eced",
              "IPY_MODEL_13dab17f70ab43ff9223145bf9eb00cb"
            ]
          }
        },
        "00553b164fe941cda24fa9f4d27f67e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f34c72409ba4f4b93bc61fe8124eced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eee3903f08b045d6a6b2dcb02428dcd0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f704720ce9754447af5b206342012175"
          }
        },
        "13dab17f70ab43ff9223145bf9eb00cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e85e88686d945a5956d4d67c5cd1359",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [05:51&lt;00:00, 484708.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d546c4ce870c4f8abdec65eea824632a"
          }
        },
        "eee3903f08b045d6a6b2dcb02428dcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f704720ce9754447af5b206342012175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e85e88686d945a5956d4d67c5cd1359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d546c4ce870c4f8abdec65eea824632a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8de3b2d654045e988864e058bd8393e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef6f44a7964641039d02c948ba04dfce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_04f77a870711447eb62d95dfcc8e34be",
              "IPY_MODEL_d790bcb920fd4a4f8799548038d3e231",
              "IPY_MODEL_9cba270f415e424bba34f02142130ceb"
            ]
          }
        },
        "ef6f44a7964641039d02c948ba04dfce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04f77a870711447eb62d95dfcc8e34be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03ebe18e32bb43f9b8e0b4df7d8832de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61d10446d0f544ebb9de911502b77c03"
          }
        },
        "d790bcb920fd4a4f8799548038d3e231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_87387cb30dfe43bd951e2ac5799bec1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72b992f259ed4e899ef8c70c763e2b73"
          }
        },
        "9cba270f415e424bba34f02142130ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_022b8888f76f4ba7a8770cb761f25b66",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 30883389.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dba4e1a00b054aa883a001f011717153"
          }
        },
        "03ebe18e32bb43f9b8e0b4df7d8832de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61d10446d0f544ebb9de911502b77c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87387cb30dfe43bd951e2ac5799bec1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72b992f259ed4e899ef8c70c763e2b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "022b8888f76f4ba7a8770cb761f25b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dba4e1a00b054aa883a001f011717153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilanand91/Forget/blob/main/ForgettingExperimentsPrototyping02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdKbAVAZLL5i"
      },
      "source": [
        "# Reproducing some results from [Toneva et al.](https://openreview.net/pdf?id=BJlxm30cKm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOnSH8ppLYBf"
      },
      "source": [
        "We'd like to set up some code to do the following:\n",
        "1.   Automatically generate models using OpenLTH.\n",
        "2.   Reproduce some of the forgetting experiments on CIFAR10/100 from Toneva et al.\n",
        "3.   Automate some of the scripting and experiment management, instead of having to write new SLURM scripts for each experiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfeXT5AIcEiu"
      },
      "source": [
        "## Load OpenLTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amiRFteO2SYd"
      },
      "source": [
        "Get OpenLTH (only need to do once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBEx58xlLKCH",
        "outputId": "d8551a2e-dbb3-4ebf-a7ad-66317a493591"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 5.51 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17mWG2ozv_3U",
        "outputId": "2160f799-7b20-4586-9b0f-04a066b630e1"
      },
      "source": [
        "!python open_lth/open_lth.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: open_lth.py subcommand\n",
            "open_lth.py: error: the following arguments are required: subcommand\n",
            "==================================================================================\n",
            "OpenLTH: A Framework for Research on Lottery Tickets and Beyond\n",
            "----------------------------------------------------------------------------------\n",
            "Choose a command to run:\n",
            "    * open_lth/open_lth.py train [...] => Train a model.\n",
            "    * open_lth/open_lth.py lottery [...] => Run a lottery ticket hypothesis experiment.\n",
            "    * open_lth/open_lth.py lottery_branch [...] => Run a lottery branch.\n",
            "==================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvCkME7Iy9cx"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/open_lth/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biqrbMg1ocj0"
      },
      "source": [
        "## Set model+data hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZwXBYN92bTu"
      },
      "source": [
        "We need to first define model hyperparameters, which can be done by using the `hparams` object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNISpoiy4Lu-"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHRzvUnE2q1D"
      },
      "source": [
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRSLoQddBM-9"
      },
      "source": [
        "model_1 = registry.get(model_hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKfTFGVM5Mry"
      },
      "source": [
        "from datasets import registry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yahrywV56YGv"
      },
      "source": [
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJEzEXEfB4nT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0c78c0-a43c-4f77-a376-93fb6dc92977"
      },
      "source": [
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-G5bGJJ5vgw"
      },
      "source": [
        "## Load data and plot some images to check normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS1HUi2774d2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vazj95Yd5wzY"
      },
      "source": [
        "#don't need this - OpenLTH automatically gives the DataLoader\n",
        "# train, val = random_split(train_set, [55000,5000])\n",
        "# train_loader = DataLoader(train, batch_size=32)\n",
        "# val_loader = DataLoader(val, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSouDtRwWZIU",
        "outputId": "c71f1d18-a820-4346-f092-e458818bc9fc"
      },
      "source": [
        "images, labels = next(iter(train_set))\n",
        "images[3].size() #images are 3 x 32 x 32, but to plot, it seems like we need 32 x 32 x 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "SyN9l2Z79Yc7",
        "outputId": "d9a00bbf-12e4-4146-cd51-201d6bf6d2c4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f07e93d2950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAS-FBagEvRd"
      },
      "source": [
        "Grid format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "M6vIMtus9hqo",
        "outputId": "6c9bb4a4-c214-4edc-b1de-ee1e4f98143f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "gridlist = [images[0]*.23 + 0.45,images[1]*0.23 + 0.45,(images[3]*.23)+0.45,(images[4]*.23)+0.45]\n",
        "gridimgs = utils.make_grid(gridlist)\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "show(gridimgs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxxpz0Xt9q7U"
      },
      "source": [
        "## Train and save base model at 60 ep, with no modifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_yFwBD2FFw-"
      },
      "source": [
        "put the model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll2uhQnJFFNS"
      },
      "source": [
        "model_1_cuda = model_1.cuda() #remember to change runtime type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziIfUshJDkER"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_cuda.parameters(), lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJgY_WMDx1Uh"
      },
      "source": [
        "#ONLY RUN THIS CELL ONCE\n",
        "from pathlib import Path\n",
        "Path(\"/content/models/base/\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-6CIsdT9qgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b7981f-bf7e-4254-d865-f8eac1942c05"
      },
      "source": [
        "#log output\n",
        "train_output = open(\"/content/models/base/train_output.txt\", \"x\")\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 60\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "    model_1_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_cuda(x)\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 2.09 \n",
            "\n",
            "Training accuracy: 0.22 \n",
            "\n",
            "Epoch 2, train loss: 1.83 \n",
            "\n",
            "Training accuracy: 0.31 \n",
            "\n",
            "Epoch 3, train loss: 1.72 \n",
            "\n",
            "Training accuracy: 0.35 \n",
            "\n",
            "Epoch 4, train loss: 1.63 \n",
            "\n",
            "Training accuracy: 0.39 \n",
            "\n",
            "Epoch 5, train loss: 1.57 \n",
            "\n",
            "Training accuracy: 0.41 \n",
            "\n",
            "Epoch 6, train loss: 1.52 \n",
            "\n",
            "Training accuracy: 0.43 \n",
            "\n",
            "Epoch 7, train loss: 1.48 \n",
            "\n",
            "Training accuracy: 0.45 \n",
            "\n",
            "Epoch 8, train loss: 1.43 \n",
            "\n",
            "Training accuracy: 0.47 \n",
            "\n",
            "Epoch 9, train loss: 1.39 \n",
            "\n",
            "Training accuracy: 0.49 \n",
            "\n",
            "Epoch 10, train loss: 1.35 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 11, train loss: 1.32 \n",
            "\n",
            "Training accuracy: 0.52 \n",
            "\n",
            "Epoch 12, train loss: 1.28 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 13, train loss: 1.25 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 14, train loss: 1.22 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 15, train loss: 1.20 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 16, train loss: 1.17 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 17, train loss: 1.14 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 18, train loss: 1.12 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 19, train loss: 1.10 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 20, train loss: 1.08 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 21, train loss: 1.06 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 22, train loss: 1.04 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 23, train loss: 1.02 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 24, train loss: 1.01 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 25, train loss: 0.99 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 26, train loss: 0.97 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 27, train loss: 0.96 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 28, train loss: 0.95 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 29, train loss: 0.94 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 30, train loss: 0.92 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 31, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 32, train loss: 0.90 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 33, train loss: 0.88 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 34, train loss: 0.88 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 35, train loss: 0.87 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 36, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 37, train loss: 0.85 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 38, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 39, train loss: 0.83 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 40, train loss: 0.82 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 41, train loss: 0.81 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 42, train loss: 0.80 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 43, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 44, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 45, train loss: 0.77 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 46, train loss: 0.77 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 47, train loss: 0.76 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 48, train loss: 0.75 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 49, train loss: 0.75 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 50, train loss: 0.74 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 51, train loss: 0.73 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 52, train loss: 0.72 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 53, train loss: 0.72 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 54, train loss: 0.71 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 55, train loss: 0.70 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 56, train loss: 0.70 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 57, train loss: 0.69 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 58, train loss: 0.68 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 59, train loss: 0.68 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 60, train loss: 0.67 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTz_x5YCyQ5G",
        "outputId": "f0746586-fb6f-4379-e5de-2b565c0eb7ea"
      },
      "source": [
        "model_1_cuda.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vow127kZzP7Q",
        "outputId": "0f05a9f1-a959-4dff-a36d-4eef7006fdb3"
      },
      "source": [
        "torch.tensor(accuracies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7734, 0.8203, 0.7812, 0.7891, 0.7500, 0.6797, 0.6953, 0.7422, 0.7734,\n",
              "        0.8203, 0.7422, 0.7656, 0.7891, 0.7734, 0.7734, 0.7344, 0.7734, 0.7812,\n",
              "        0.7734, 0.8125, 0.8359, 0.7734, 0.8359, 0.7109, 0.7656, 0.7344, 0.6484,\n",
              "        0.7891, 0.7812, 0.7812, 0.7578, 0.7969, 0.7188, 0.8047, 0.8125, 0.7422,\n",
              "        0.8203, 0.7578, 0.7422, 0.7031, 0.7188, 0.7656, 0.7344, 0.6875, 0.7188,\n",
              "        0.7266, 0.7891, 0.7734, 0.7344, 0.8047, 0.7734, 0.7734, 0.7188, 0.7812,\n",
              "        0.7812, 0.7578, 0.7500, 0.7422, 0.7891, 0.6875, 0.8828, 0.7188, 0.7578,\n",
              "        0.7656, 0.7578, 0.8359, 0.7031, 0.7656, 0.7422, 0.7266, 0.7188, 0.7969,\n",
              "        0.8047, 0.7578, 0.8047, 0.7578, 0.7734, 0.7734, 0.7969, 0.7422, 0.7891,\n",
              "        0.7109, 0.7969, 0.8125, 0.8125, 0.7031, 0.6953, 0.7109, 0.7500, 0.8203,\n",
              "        0.8203, 0.7500, 0.7656, 0.8203, 0.7266, 0.8125, 0.7188, 0.7344, 0.7500,\n",
              "        0.7422, 0.8125, 0.7266, 0.7656, 0.7734, 0.8125, 0.8438, 0.7969, 0.6641,\n",
              "        0.7422, 0.7422, 0.7578, 0.7734, 0.7891, 0.8047, 0.7266, 0.7656, 0.7188,\n",
              "        0.7188, 0.7656, 0.7969, 0.8047, 0.7656, 0.7266, 0.7188, 0.7344, 0.7656,\n",
              "        0.7812, 0.7344, 0.7188, 0.6719, 0.7734, 0.7812, 0.7969, 0.7812, 0.7812,\n",
              "        0.7969, 0.7734, 0.8047, 0.7578, 0.8125, 0.7734, 0.7422, 0.7266, 0.7734,\n",
              "        0.7188, 0.7891, 0.7734, 0.8281, 0.7500, 0.7266, 0.7344, 0.7656, 0.7812,\n",
              "        0.8047, 0.7656, 0.8125, 0.7266, 0.7500, 0.7656, 0.7578, 0.7578, 0.7891,\n",
              "        0.8047, 0.7969, 0.7578, 0.8281, 0.7891, 0.7969, 0.8047, 0.7344, 0.7500,\n",
              "        0.7891, 0.6953, 0.7812, 0.7656, 0.6797, 0.7812, 0.7500, 0.7578, 0.7422,\n",
              "        0.7578, 0.7656, 0.8594, 0.7109, 0.7266, 0.8047, 0.8047, 0.8203, 0.7422,\n",
              "        0.7734, 0.7344, 0.7656, 0.8125, 0.7812, 0.7500, 0.8203, 0.7500, 0.7031,\n",
              "        0.7188, 0.7578, 0.7500, 0.8047, 0.7422, 0.7969, 0.7734, 0.7578, 0.8203,\n",
              "        0.7812, 0.7891, 0.7344, 0.7969, 0.6719, 0.7266, 0.8594, 0.7266, 0.7500,\n",
              "        0.7344, 0.7734, 0.8438, 0.7344, 0.7656, 0.7656, 0.7891, 0.7500, 0.7500,\n",
              "        0.8359, 0.7812, 0.6641, 0.7656, 0.8047, 0.7656, 0.7266, 0.7891, 0.7891,\n",
              "        0.7812, 0.7422, 0.7734, 0.8203, 0.7812, 0.7422, 0.7031, 0.7578, 0.7266,\n",
              "        0.7500, 0.8359, 0.7266, 0.7812, 0.7500, 0.7734, 0.7188, 0.7344, 0.7734,\n",
              "        0.7969, 0.7969, 0.7969, 0.8203, 0.7109, 0.7656, 0.7500, 0.7891, 0.7500,\n",
              "        0.8125, 0.7266, 0.7891, 0.7422, 0.7500, 0.8203, 0.7812, 0.6953, 0.7578,\n",
              "        0.7500, 0.7812, 0.8203, 0.7188, 0.7891, 0.8047, 0.8203, 0.7734, 0.7656,\n",
              "        0.7188, 0.7734, 0.7500, 0.7500, 0.7656, 0.7344, 0.7812, 0.8047, 0.7266,\n",
              "        0.7656, 0.7734, 0.7734, 0.7188, 0.8125, 0.7266, 0.7188, 0.7656, 0.7344,\n",
              "        0.6562, 0.7969, 0.7344, 0.7891, 0.7422, 0.7656, 0.8047, 0.7812, 0.7188,\n",
              "        0.7188, 0.8047, 0.7891, 0.7422, 0.6875, 0.8047, 0.7578, 0.7891, 0.7734,\n",
              "        0.7656, 0.7734, 0.7656, 0.7578, 0.8203, 0.7344, 0.7812, 0.7578, 0.8047,\n",
              "        0.6328, 0.7656, 0.7500, 0.8047, 0.7656, 0.7656, 0.8281, 0.7812, 0.7109,\n",
              "        0.7812, 0.7969, 0.7500, 0.8047, 0.7188, 0.7422, 0.7812, 0.7578, 0.7500,\n",
              "        0.7344, 0.7422, 0.7422, 0.7656, 0.8281, 0.7656, 0.7969, 0.7656, 0.7188,\n",
              "        0.7578, 0.7031, 0.7188, 0.8125, 0.7031, 0.7891, 0.7422, 0.7266, 0.7578,\n",
              "        0.7422, 0.7422, 0.8203, 0.6797, 0.7891, 0.8359, 0.7812, 0.7422, 0.7891,\n",
              "        0.8125, 0.8203, 0.8047, 0.7969, 0.7578, 0.7344, 0.7344, 0.8047, 0.8125,\n",
              "        0.7734, 0.8047, 0.7734, 0.7969, 0.7188, 0.7422, 0.8359, 0.7500, 0.7969,\n",
              "        0.7422, 0.7500, 0.8359, 0.8125])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88LbmPAWOd2"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l_9R3MJYhV7"
      },
      "source": [
        "torch.save(model_1_cuda, \"/content/models/base/model_1_cuda_base.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "5XVi7P68Ykw6",
        "outputId": "f88151e6-3c15-4141-cfe3-12f66f0e5d5e"
      },
      "source": [
        "#Create a txt file with model information\n",
        "model_info_file = open(\"/content/models/base/base_model_info.txt\", \"x\")\n",
        "model_info_file.write(f\"Model hyperparameters: {model_hparams} \\n\")\n",
        "model_info_file.write(f\"Dataset parameters: {dataset_hparams} \\n\")\n",
        "model_info_file.write(f\"Epochs trained: {nb_epochs} \\n\")\n",
        "model_info_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1a540103f05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a txt file with model information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_info_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/models/base/base_model_info.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_info_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model hyperparameters: {model_hparams} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_info_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset parameters: {dataset_hparams} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_info_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epochs trained: {nb_epochs} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/models/base/base_model_info.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSJ_PcyTzyGg"
      },
      "source": [
        "## Now keep track of forgetting events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Efw9G_m_NQ"
      },
      "source": [
        "## Total number of forgetting events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aze0D12N1FEi",
        "outputId": "95ca2898-f73f-4281-b804-2aeefbc2de34"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_forget = registry.get(model_hparams)\n",
        "model_1_forget_cuda = model_1_forget.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1dLp_DEFF3"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5-h_jdPotKh"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_forget_cuda.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hux-xidpouy6"
      },
      "source": [
        "#ONLY RUN THIS CELL ONCE\n",
        "from pathlib import Path\n",
        "Path(\"/content/models/forget1/\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV-OZ-QAo4Rq",
        "outputId": "b8ce3662-4925-4f2d-90bf-35fcc6f0ab56"
      },
      "source": [
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 75\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_forget_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_forget_cuda(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_forget_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 2.14 \n",
            "\n",
            "Training accuracy: 0.20 \n",
            "\n",
            "Epoch 2, train loss: 1.82 \n",
            "\n",
            "Training accuracy: 0.31 \n",
            "\n",
            "Epoch 3, train loss: 1.70 \n",
            "\n",
            "Training accuracy: 0.36 \n",
            "\n",
            "Epoch 4, train loss: 1.63 \n",
            "\n",
            "Training accuracy: 0.39 \n",
            "\n",
            "Epoch 5, train loss: 1.56 \n",
            "\n",
            "Training accuracy: 0.42 \n",
            "\n",
            "Epoch 6, train loss: 1.51 \n",
            "\n",
            "Training accuracy: 0.44 \n",
            "\n",
            "Epoch 7, train loss: 1.45 \n",
            "\n",
            "Training accuracy: 0.47 \n",
            "\n",
            "Epoch 8, train loss: 1.39 \n",
            "\n",
            "Training accuracy: 0.49 \n",
            "\n",
            "Epoch 9, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 10, train loss: 1.30 \n",
            "\n",
            "Training accuracy: 0.52 \n",
            "\n",
            "Epoch 11, train loss: 1.26 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 12, train loss: 1.23 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 13, train loss: 1.20 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 14, train loss: 1.17 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 15, train loss: 1.15 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 16, train loss: 1.12 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 17, train loss: 1.10 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 18, train loss: 1.08 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 19, train loss: 1.06 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 20, train loss: 1.04 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 21, train loss: 1.03 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 22, train loss: 1.01 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 23, train loss: 0.99 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 24, train loss: 0.98 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 25, train loss: 0.96 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 26, train loss: 0.95 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 27, train loss: 0.94 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 28, train loss: 0.93 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 29, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 30, train loss: 0.90 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 31, train loss: 0.89 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 32, train loss: 0.88 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 33, train loss: 0.87 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 34, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 35, train loss: 0.85 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 36, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 37, train loss: 0.83 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 38, train loss: 0.82 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 39, train loss: 0.82 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 40, train loss: 0.81 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 41, train loss: 0.80 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 42, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 43, train loss: 0.78 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 44, train loss: 0.78 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 45, train loss: 0.77 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 46, train loss: 0.76 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 47, train loss: 0.75 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 48, train loss: 0.75 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 49, train loss: 0.74 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 50, train loss: 0.73 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 51, train loss: 0.72 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 52, train loss: 0.71 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 53, train loss: 0.71 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 54, train loss: 0.71 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 55, train loss: 0.70 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 56, train loss: 0.69 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 57, train loss: 0.69 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 58, train loss: 0.68 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 59, train loss: 0.68 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 60, train loss: 0.66 \n",
            "\n",
            "Training accuracy: 0.77 \n",
            "\n",
            "Epoch 61, train loss: 0.66 \n",
            "\n",
            "Training accuracy: 0.77 \n",
            "\n",
            "Epoch 62, train loss: 0.65 \n",
            "\n",
            "Training accuracy: 0.77 \n",
            "\n",
            "Epoch 63, train loss: 0.65 \n",
            "\n",
            "Training accuracy: 0.77 \n",
            "\n",
            "Epoch 64, train loss: 0.65 \n",
            "\n",
            "Training accuracy: 0.77 \n",
            "\n",
            "Epoch 65, train loss: 0.64 \n",
            "\n",
            "Training accuracy: 0.78 \n",
            "\n",
            "Epoch 66, train loss: 0.63 \n",
            "\n",
            "Training accuracy: 0.78 \n",
            "\n",
            "Epoch 67, train loss: 0.62 \n",
            "\n",
            "Training accuracy: 0.78 \n",
            "\n",
            "Epoch 68, train loss: 0.62 \n",
            "\n",
            "Training accuracy: 0.78 \n",
            "\n",
            "Epoch 69, train loss: 0.61 \n",
            "\n",
            "Training accuracy: 0.78 \n",
            "\n",
            "Epoch 70, train loss: 0.61 \n",
            "\n",
            "Training accuracy: 0.79 \n",
            "\n",
            "Epoch 71, train loss: 0.60 \n",
            "\n",
            "Training accuracy: 0.79 \n",
            "\n",
            "Epoch 72, train loss: 0.60 \n",
            "\n",
            "Training accuracy: 0.79 \n",
            "\n",
            "Epoch 73, train loss: 0.60 \n",
            "\n",
            "Training accuracy: 0.79 \n",
            "\n",
            "Epoch 74, train loss: 0.59 \n",
            "\n",
            "Training accuracy: 0.79 \n",
            "\n",
            "Epoch 75, train loss: 0.59 \n",
            "\n",
            "Training accuracy: 0.79 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNttkvidc_oD",
        "outputId": "6d0f2a4a-d2ee-4de4-f26c-e964fac1bbda"
      },
      "source": [
        "print(torch.flatten(forget_matrix.round()))\n",
        "max_val = torch.max(torch.flatten(forget_matrix))\n",
        "torch.histc(torch.flatten(forget_matrix.round()), bins = 12, min = 0, max = max_val)\n",
        "#torch.max(torch.flatten(forget_matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 3., 4.,  ..., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3456e+04, 9.4620e+03, 7.3750e+03, 6.1620e+03, 5.0850e+03, 3.8940e+03,\n",
              "        2.5570e+03, 1.3400e+03, 5.3600e+02, 1.4900e+02, 2.8000e+01, 4.0000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "FbM5UqokcZ--",
        "outputId": "543a97fd-b928-4677-c25a-655ab32d83ec"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen = len(torch.flatten(forget_matrix))\n",
        "hist = plt.hist(torch.flatten(forget_matrix), label = \"Events\", weights = np.ones(forgetlen)/forgetlen)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Hd470nIAwDf9",
        "outputId": "e8baa79d-c372-4aec-d2b6-4858cdfe187c"
      },
      "source": [
        "hist = plt.hist(torch.flatten(forget_matrix[5]),density=True, label = \"Events\") #in a single batch\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbsNy7VAQUpX",
        "outputId": "1375195d-117e-40ee-a4e7-1d9af49b0f2c"
      },
      "source": [
        "print(softmaxfunc(l)[1])\n",
        "print(sum(softmaxfunc(l)[1]))\n",
        "print(torch.argmax(softmaxfunc(l)[1]))\n",
        "print(torch.argmax(softmaxfunc(l)[1])==5)\n",
        "print(labels[1:40])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0428, 0.0738, 0.1496, 0.0976, 0.0977, 0.1973, 0.0257, 0.1183, 0.0573,\n",
            "        0.1399], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "tensor(1., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor([9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6, 2,\n",
            "        6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1F9UaZtoC_D"
      },
      "source": [
        "## Add noise to CIFAR10 labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ06x8Pwz4dS"
      },
      "source": [
        "#ONLY RUN THIS CELL ONCE\n",
        "from pathlib import Path\n",
        "Path(\"/content/models/forgetnoise/\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "3E26aW7zoIH3",
        "outputId": "c6d469a0-a715-4495-c4b3-866739753b54"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_forget_noise = registry.get(model_hparams)\n",
        "model_1_forget_noise_cuda = model_1_forget_noise.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_forget_noise_cuda.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-55ec512a0289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m train_set = registry.get(\n\u001b[1;32m     28\u001b[0m     \u001b[0mdataset_hparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open_lth/datasets/registry.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dataset_hparams, train)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0muse_augmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdataset_hparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_augment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistered_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_hparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_augmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistered_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_hparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open_lth/datasets/cifar10.py\u001b[0m in \u001b[0;36mget_train_set\u001b[0;34m(use_augmentation)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_augmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0maugment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_augmentation\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'labels'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvkMcXE_zxSS",
        "outputId": "5d6c925c-11c7-409b-a71e-94860a5b9d1e"
      },
      "source": [
        "import random\n",
        "\n",
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_noise = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 75\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_forget_noise_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_forget_noise_cuda(x)\n",
        "\n",
        "        #shuffle 20 percent of labels\n",
        "        indices = torch.randint(0, len(y), (round(0.2*len(y))+1,))\n",
        "        newlabels = torch.clone(y)\n",
        "\n",
        "        for idx in indices:\n",
        "            newlabels[idx] = random.randint(0,9)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==newlabels[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_noise[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, newlabels.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_forget_noise_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(newlabels.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        "    #train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    #train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 2.24 \n",
            "\n",
            "Training accuracy: 0.16 \n",
            "\n",
            "Epoch 2, train loss: 2.04 \n",
            "\n",
            "Training accuracy: 0.25 \n",
            "\n",
            "Epoch 3, train loss: 1.97 \n",
            "\n",
            "Training accuracy: 0.29 \n",
            "\n",
            "Epoch 4, train loss: 1.94 \n",
            "\n",
            "Training accuracy: 0.31 \n",
            "\n",
            "Epoch 5, train loss: 1.91 \n",
            "\n",
            "Training accuracy: 0.33 \n",
            "\n",
            "Epoch 6, train loss: 1.88 \n",
            "\n",
            "Training accuracy: 0.34 \n",
            "\n",
            "Epoch 7, train loss: 1.85 \n",
            "\n",
            "Training accuracy: 0.36 \n",
            "\n",
            "Epoch 8, train loss: 1.82 \n",
            "\n",
            "Training accuracy: 0.37 \n",
            "\n",
            "Epoch 9, train loss: 1.79 \n",
            "\n",
            "Training accuracy: 0.39 \n",
            "\n",
            "Epoch 10, train loss: 1.77 \n",
            "\n",
            "Training accuracy: 0.40 \n",
            "\n",
            "Epoch 11, train loss: 1.75 \n",
            "\n",
            "Training accuracy: 0.41 \n",
            "\n",
            "Epoch 12, train loss: 1.72 \n",
            "\n",
            "Training accuracy: 0.42 \n",
            "\n",
            "Epoch 13, train loss: 1.71 \n",
            "\n",
            "Training accuracy: 0.44 \n",
            "\n",
            "Epoch 14, train loss: 1.70 \n",
            "\n",
            "Training accuracy: 0.44 \n",
            "\n",
            "Epoch 15, train loss: 1.68 \n",
            "\n",
            "Training accuracy: 0.45 \n",
            "\n",
            "Epoch 16, train loss: 1.66 \n",
            "\n",
            "Training accuracy: 0.46 \n",
            "\n",
            "Epoch 17, train loss: 1.65 \n",
            "\n",
            "Training accuracy: 0.47 \n",
            "\n",
            "Epoch 18, train loss: 1.64 \n",
            "\n",
            "Training accuracy: 0.47 \n",
            "\n",
            "Epoch 19, train loss: 1.63 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 20, train loss: 1.62 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 21, train loss: 1.61 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 22, train loss: 1.60 \n",
            "\n",
            "Training accuracy: 0.49 \n",
            "\n",
            "Epoch 23, train loss: 1.58 \n",
            "\n",
            "Training accuracy: 0.50 \n",
            "\n",
            "Epoch 24, train loss: 1.57 \n",
            "\n",
            "Training accuracy: 0.50 \n",
            "\n",
            "Epoch 25, train loss: 1.56 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 26, train loss: 1.56 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 27, train loss: 1.55 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 28, train loss: 1.54 \n",
            "\n",
            "Training accuracy: 0.52 \n",
            "\n",
            "Epoch 29, train loss: 1.54 \n",
            "\n",
            "Training accuracy: 0.52 \n",
            "\n",
            "Epoch 30, train loss: 1.52 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 31, train loss: 1.52 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 32, train loss: 1.51 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 33, train loss: 1.51 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 34, train loss: 1.50 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 35, train loss: 1.50 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 36, train loss: 1.48 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 37, train loss: 1.48 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 38, train loss: 1.48 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 39, train loss: 1.47 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 40, train loss: 1.46 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 41, train loss: 1.46 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 42, train loss: 1.46 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 43, train loss: 1.45 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 44, train loss: 1.45 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 45, train loss: 1.43 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 46, train loss: 1.43 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 47, train loss: 1.43 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 48, train loss: 1.43 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 49, train loss: 1.42 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 50, train loss: 1.42 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 51, train loss: 1.41 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 52, train loss: 1.41 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 53, train loss: 1.40 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 54, train loss: 1.40 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 55, train loss: 1.40 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 56, train loss: 1.39 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 57, train loss: 1.38 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 58, train loss: 1.38 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 59, train loss: 1.38 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 60, train loss: 1.38 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 61, train loss: 1.38 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 62, train loss: 1.37 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 63, train loss: 1.36 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 64, train loss: 1.36 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 65, train loss: 1.36 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 66, train loss: 1.35 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 67, train loss: 1.35 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 68, train loss: 1.35 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 69, train loss: 1.35 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 70, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 71, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 72, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 73, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 74, train loss: 1.33 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 75, train loss: 1.32 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h95jnyfPmB6"
      },
      "source": [
        "from pathlib import Path\n",
        "Path(\"/content/models/forgetnoise/\").mkdir(parents=True, exist_ok=True)\n",
        "forget_output = open(\"/content/models/forgetnoise/forget_output_noise.txt\", \"x\")\n",
        "forget_output.write(f\"{forget_matrix_noise}\")\n",
        "forget_output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "X_Z1HnfLCnQe",
        "outputId": "eec4ef9c-251b-49d8-98fe-d53906d63f5c"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen_noise = len(torch.flatten(forget_matrix_noise))\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "hist = plt.hist(torch.flatten(forget_matrix_noise), label = \"Events\", weights = np.ones(forgetlen_noise)/forgetlen_noise)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(18, .15, r'CIFAR10, n_epochs = 75')\n",
        "plt.text(18, .13, r'20 % noise labels')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__G2IL3BxO4C"
      },
      "source": [
        "# What happens if most forgotten examples are dropped from dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atmr5Tbd6Nuc"
      },
      "source": [
        "from pathlib import Path\n",
        "Path(\"/content/models/forget_mask/\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKEUgwVg85Ii"
      },
      "source": [
        "first we need create a mask so that we can remove most forgotten examples. We set the mask threshold to be = 4 here (so we remove it if it's forgotten 4 or more times)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "15b4f8e413f34476913f6c3d139c6b1e",
            "f88737a5019540af89365750e9fd1b85",
            "d229702e78b849beb1922ce2c5f7b9cc",
            "f49dff685af7482f8e279c7db409a0bc",
            "0494128c835e4859a85011783c8fa4bb",
            "9471c9450ebe4eb1b44a21ef0e3b29f3",
            "b31da9bf022740cdbd9cc2e11f765967",
            "60ae916855b743a5ad6c466f9ac9ad3d"
          ]
        },
        "id": "gyELelFf6R4t",
        "outputId": "4cbff612-b715-43da-8e12-ac89475ad0cf"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_forget_mask = registry.get(model_hparams)\n",
        "model_1_forget_mask_cuda = model_1_forget_mask.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_forget_mask_cuda.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15b4f8e413f34476913f6c3d139c6b1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mdjpxHl6ZuB"
      },
      "source": [
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "forget_output = open(\"/content/models/forget_mask/forget_output.py\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix = torch.zeros(len(train_set),128)\n",
        "forget_mask = torch.ones(len(train_set),128, dtype=torch.bool)\n",
        "mask_threshold = 4\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_forget_mask_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_forget_mask_cuda(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix[batch_tracker, k] += 1\n",
        "\n",
        "                if forget_matrix[batch_tracker, k] >= mask_threshold:\n",
        "                    forget_mask[batch_tracker, k] = False\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_forget_mask_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "forget_output.write(f\"forget_matrix = {forget_matrix}\")\n",
        "\n",
        "forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5g4cqLfk0Zj"
      },
      "source": [
        "forget_output = open(\"/content/models/forget_mask/forget_output.py\", \"x\")\n",
        "\n",
        "forget_output.write(f\"forget_matrix = {forget_matrix}\")\n",
        "\n",
        "forget_output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "8Vr2G5JulEH-",
        "outputId": "0ee9156e-de2d-4e3c-e07e-5bbf52fe56df"
      },
      "source": [
        "from models.forget_mask import forget_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6656283e9c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget_mask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforget_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/models/forget_mask/forget_output.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m forget_matrix = tensor([[2., 4., 4.,  ..., 2., 1., 1.],\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "z9gfL9UYPlgA",
        "outputId": "406994be-daee-47ae-fce4-c2de11c1215f"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen = len(torch.flatten(forget_matrix))\n",
        "hist = plt.hist(torch.flatten(forget_matrix), label = \"Events\", weights = np.ones(forgetlen)/forgetlen)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgPlBDxP7Jh"
      },
      "source": [
        "Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoVMeBeaP59C"
      },
      "source": [
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVHNpvxUQRVi"
      },
      "source": [
        "model_1_forget_mask_cuda.eval()\n",
        "test_accuracy = list()\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l = model_1_forget_mask_cuda(x)\n",
        "        \n",
        "    test_accuracy.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        \n",
        "    print(f\"Test accuracy: {torch.tensor(test_accuracy).mean():.2f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLfGEbfbzAbD",
        "outputId": "3a217f72-b89b-433e-a5f4-886502f865d3"
      },
      "source": [
        "test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.7500),\n",
              " tensor(0.7266),\n",
              " tensor(0.7109),\n",
              " tensor(0.7344),\n",
              " tensor(0.8047),\n",
              " tensor(0.6875),\n",
              " tensor(0.6641),\n",
              " tensor(0.7500),\n",
              " tensor(0.6875),\n",
              " tensor(0.7344),\n",
              " tensor(0.6797),\n",
              " tensor(0.7031),\n",
              " tensor(0.7266),\n",
              " tensor(0.7344),\n",
              " tensor(0.8047),\n",
              " tensor(0.6875),\n",
              " tensor(0.6719),\n",
              " tensor(0.7188),\n",
              " tensor(0.6953),\n",
              " tensor(0.6797),\n",
              " tensor(0.7188),\n",
              " tensor(0.7109),\n",
              " tensor(0.7734),\n",
              " tensor(0.7500),\n",
              " tensor(0.7734),\n",
              " tensor(0.6406),\n",
              " tensor(0.6641),\n",
              " tensor(0.7031),\n",
              " tensor(0.6875),\n",
              " tensor(0.7266),\n",
              " tensor(0.7422),\n",
              " tensor(0.7344),\n",
              " tensor(0.7266),\n",
              " tensor(0.7266),\n",
              " tensor(0.7656),\n",
              " tensor(0.6797),\n",
              " tensor(0.7891),\n",
              " tensor(0.7656),\n",
              " tensor(0.7266),\n",
              " tensor(0.7578),\n",
              " tensor(0.6953),\n",
              " tensor(0.7109),\n",
              " tensor(0.7266),\n",
              " tensor(0.6953),\n",
              " tensor(0.7266),\n",
              " tensor(0.6875),\n",
              " tensor(0.6797),\n",
              " tensor(0.7031),\n",
              " tensor(0.6328),\n",
              " tensor(0.7656),\n",
              " tensor(0.6641),\n",
              " tensor(0.7344),\n",
              " tensor(0.6719),\n",
              " tensor(0.7188),\n",
              " tensor(0.6719),\n",
              " tensor(0.7422),\n",
              " tensor(0.7656),\n",
              " tensor(0.7578),\n",
              " tensor(0.7734),\n",
              " tensor(0.7188),\n",
              " tensor(0.7109),\n",
              " tensor(0.6250),\n",
              " tensor(0.7812),\n",
              " tensor(0.7188),\n",
              " tensor(0.6719),\n",
              " tensor(0.7031),\n",
              " tensor(0.6719),\n",
              " tensor(0.7109),\n",
              " tensor(0.6641),\n",
              " tensor(0.6562),\n",
              " tensor(0.7734),\n",
              " tensor(0.8203),\n",
              " tensor(0.6719),\n",
              " tensor(0.6875),\n",
              " tensor(0.7969),\n",
              " tensor(0.7109),\n",
              " tensor(0.6016),\n",
              " tensor(0.7500),\n",
              " tensor(0.7500)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLn5qqBC87p3"
      },
      "source": [
        "then retrain with mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CpAO3KxOqft"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams_2 = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_forget_with_mask = registry.get(model_hparams_2)\n",
        "model_1_forget_with_mask_cuda = model_1_forget_with_mask.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_forget_with_mask_cuda.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gocDPN2487X0",
        "outputId": "e02f418c-223c-499d-d2ba-ffca4bbab78a"
      },
      "source": [
        "model_1_forget_with_mask_cuda.eval()\n",
        "test_accuracy_2 = list()\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l1 = model_1_forget_with_mask_cuda(x)\n",
        "        l2 = model_1_forget_mask_cuda(x)\n",
        "        #print(l1 - l2)\n",
        "\n",
        "    test_accuracy_2.append(y.eq(l1.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        \n",
        "    print(f\"Test accuracy: {torch.tensor(test_accuracy_2).mean():.2f} \\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.12 \n",
            "\n",
            "Test accuracy: 0.09 \n",
            "\n",
            "Test accuracy: 0.09 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.09 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.11 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n",
            "Test accuracy: 0.10 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McsemVwkRcQo"
      },
      "source": [
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i_mask = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_mask = torch.zeros(len(train_set),128)\n",
        "forget_matrix_mask = torch.zeros(len(train_set),128)\n",
        "#forget_mask = torch.ones(len(train_set),128, dtype=torch.bool)\n",
        "#mask_threshold = 4\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#correct the last element of the forget mask so that it matches the last batch\n",
        "forget_mask_last = forget_mask[len(train_set)-1,0:50000%128]\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_forget_with_mask_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        if(batch_tracker==len(train_set)-1):\n",
        "            xp = x[forget_mask_last == True,:]\n",
        "            yp = y[forget_mask_last == True]\n",
        "        else:\n",
        "            xp = x[forget_mask[batch_tracker] == True,:] \n",
        "            yp = y[forget_mask[batch_tracker] == True] \n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_forget_with_mask_cuda(xp)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i_mask[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_mask[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i_mask[batch_tracker, k] < a_tilde_i_mask[batch_tracker, k]:\n",
        "                forget_matrix_mask[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i_mask[batch_tracker, k] = a_i_mask[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, yp.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_forget_with_mask_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(yp.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "CmcCZ-ZR9S5D",
        "outputId": "a23c2cec-8f8a-4e6e-cc00-cfa9e0e3b537"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen = len(torch.flatten(forget_matrix_mask))\n",
        "hist = plt.hist(torch.flatten(forget_matrix_mask), label = \"Events\", weights = np.ones(forgetlen)/forgetlen)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 40 \\n forgotten ones removed')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVXqbw7oAazT",
        "outputId": "c641e9fe-d7b1-4ce5-fb61-481c05c508c8"
      },
      "source": [
        "torch.tensor(accuracies).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7532)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ3-kiyY4dca"
      },
      "source": [
        "forget_mask_last = forget_mask[len(train_set)-1,0:(50000%128)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Y7-Ap9xg6fyE",
        "outputId": "33d186a2-f308-4643-fa7a-b5ff5c7c7f00"
      },
      "source": [
        "forget_mask[len(train_set)-1] = forget_mask_last[0:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-7fe8bdb049ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforget_mask_last\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (128) must match the existing size (80) at non-singleton dimension 0.  Target sizes: [128].  Tensor sizes: [80]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "jqHIoyM-wA1f",
        "outputId": "b0104dcf-b2d8-4a3a-ff10-2af5e1b8c0d2"
      },
      "source": [
        "trker = 0\n",
        "for batch in train_set:\n",
        "    images, labels = batch\n",
        "    model_1_forget_with_mask_cuda.eval()\n",
        "\n",
        "    print(f\"{trker}\")\n",
        "\n",
        "    if trker==len(train_set)\n",
        "        imagesmasked\n",
        "\n",
        "    imagesmasked = images[forget_mask[trker] == True,:] #torch.masked_select(images, forget_mask[0])\n",
        "    print(imagesmasked.size())\n",
        "    labelsmasked = labels[forget_mask[trker] == True] #torch.masked_select(images, forget_mask[0])\n",
        "    trker+=1\n",
        "#\n",
        "#model_1_forget_with_mask_cuda(imagesmasked.cuda())\n",
        "\n",
        "#forget_mask[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8c0002aef588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{trker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimagesmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#torch.masked_select(images, forget_mask[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagesmasked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabelsmasked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#torch.masked_select(images, forget_mask[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [128] at index 0 does not match the shape of the indexed tensor [80, 3, 32, 32] at index 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPsZwgDV3JEU",
        "outputId": "1e9c1ce8-e4c8-4185-8e72-6b92ba0a9235"
      },
      "source": [
        "len(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmHXhE2U4B1b",
        "outputId": "e082dec7-e7b2-4c39-d629-bcaa4d81ba5d"
      },
      "source": [
        "50000%128"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVZS9fibzSHi",
        "outputId": "79762f36-6139-45a4-e0e3-f81fcc25e733"
      },
      "source": [
        "newmask = forget_mask[390,0:80] #forget_mask[390]\n",
        "images.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxUtvYaj2e8d",
        "outputId": "1d06adfe-5f18-4726-8242-6a0b1bb02112"
      },
      "source": [
        "images.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIiGdh543xYY"
      },
      "source": [
        "other, other2 = next(iter(train_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sRHGhNe30it",
        "outputId": "c5b77b83-5225-4028-da70-671fe68f2e36"
      },
      "source": [
        "other.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGI1FXIn3AIk",
        "outputId": "408a7b16-35da-40f0-c5fd-08c88e38b448"
      },
      "source": [
        "len(images[forget_mask[0] == True,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfhLUo0xbKD"
      },
      "source": [
        "def select_mask(images, mask):\n",
        "    images_masked = torch.empty() #(len(images),3,32,32)\n",
        "    for i in range(len(mask)):\n",
        "        if mask[i]==True:\n",
        "            images_masked_2 = torch.cat(images_masked,images[i])\n",
        "    return images_masked_2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiONJKBXz2oT",
        "outputId": "0cb59c49-5eae-4f3d-9a03-bab675a19bd5"
      },
      "source": [
        "imagesmasked"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
              "\n",
              "\n",
              "        [[[-2.1179, -2.1179, -2.1179,  ..., -1.0904, -1.1589, -0.7993],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -0.7479, -1.0048, -0.6794],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -0.5082, -1.0048, -0.6965],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -0.3712, -1.1760, -1.5699],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -1.1075, -1.2788, -1.1932],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -0.5767, -0.1314,  0.1768]],\n",
              "\n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -0.9853, -1.0378, -0.6702],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -0.6176, -0.8627, -0.5301],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -0.3725, -0.8627, -0.5301],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -0.1450, -1.0553, -1.5805],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -0.9153, -1.1779, -1.1954],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -0.4601, -0.0574,  0.1877]],\n",
              "\n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -0.7413, -0.8633, -0.5495],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -0.4624, -0.7587, -0.4798],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -0.3055, -0.8633, -0.5844],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ...,  0.3219, -0.6890, -1.2990],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -0.4624, -0.7936, -0.8981],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -0.0441,  0.3219,  0.5136]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [ 2.1975,  2.1975,  2.1975,  ...,  2.2489, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [ 1.0159,  0.5022, -0.5082,  ...,  2.2489, -2.1179, -2.1179],\n",
              "          [ 0.8447,  0.7248,  0.1426,  ...,  2.2489, -2.1179, -2.1179],\n",
              "          [ 1.3242,  0.2111,  0.2967,  ...,  2.2489, -2.1179, -2.1179]],\n",
              "\n",
              "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [ 2.3761,  2.3761,  2.3761,  ...,  2.4286, -2.0357, -2.0357],\n",
              "          ...,\n",
              "          [ 0.4503,  0.0126, -0.8627,  ...,  2.4286, -2.0357, -2.0357],\n",
              "          [ 0.5203,  0.3277, -0.2850,  ...,  2.4286, -2.0357, -2.0357],\n",
              "          [ 1.2731, -0.0574, -0.1450,  ...,  2.4286, -2.0357, -2.0357]],\n",
              "\n",
              "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [ 2.5877,  2.5877,  2.5877,  ...,  2.6400, -1.8044, -1.8044],\n",
              "          ...,\n",
              "          [ 0.1128, -0.1661, -0.9156,  ...,  2.6400, -1.8044, -1.8044],\n",
              "          [ 0.0605, -0.0790, -0.5495,  ...,  2.6400, -1.8044, -1.8044],\n",
              "          [ 0.9145, -0.4450, -0.4450,  ...,  2.6400, -1.8044, -1.8044]]],\n",
              "\n",
              "\n",
              "        [[[-2.1179, -2.1179, -2.1179,  ...,  1.7180,  1.7352,  1.7352],\n",
              "          [-2.1179, -2.1179, -2.1179,  ...,  1.7352,  1.7352,  1.7352],\n",
              "          [-2.1179, -2.1179, -2.1179,  ...,  1.6495,  1.6838,  1.6838],\n",
              "          ...,\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -0.3027, -0.3541, -0.4054],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              "\n",
              "         [[-2.0357, -2.0357, -2.0357,  ...,  2.0084,  2.0259,  2.0434],\n",
              "          [-2.0357, -2.0357, -2.0357,  ...,  1.9909,  1.9909,  1.9909],\n",
              "          [-2.0357, -2.0357, -2.0357,  ...,  1.9384,  1.9734,  1.9734],\n",
              "          ...,\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -0.0924, -0.1450, -0.2150],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              "\n",
              "         [[-1.8044, -1.8044, -1.8044,  ...,  2.3611,  2.3786,  2.3960],\n",
              "          [-1.8044, -1.8044, -1.8044,  ...,  2.3611,  2.3611,  2.3611],\n",
              "          [-1.8044, -1.8044, -1.8044,  ...,  2.2914,  2.3263,  2.3263],\n",
              "          ...,\n",
              "          [-1.8044, -1.8044, -1.8044,  ...,  0.1128,  0.0605, -0.0092],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          ...,\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCsyS4JQxDLk"
      },
      "source": [
        "imagesmasked = select_mask(images, forget_mask[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2O_Z6dHI7Vo",
        "outputId": "f915fb81-21c4-40ef-8783-f892a054d4ce"
      },
      "source": [
        "test_forget_mask = torch.zeros(5, dtype=torch.bool)\n",
        "print(test_forget_mask)\n",
        "test_forget_mask[2] = True\n",
        "print(test_forget_mask)\n",
        "test_select = torch.rand(5)\n",
        "print(test_select)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([False, False, False, False, False])\n",
            "tensor([False, False,  True, False, False])\n",
            "tensor([0.4376, 0.2636, 0.6580, 0.5527, 0.8292])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG-2YBYXMODZ",
        "outputId": "ca26192f-a575-4b93-cdd8-765791656046"
      },
      "source": [
        "torch.masked_select(test_select, torch.BoolTensor(test_forget_mask))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6580])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4aQINAGX9SV"
      },
      "source": [
        "# Merging classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhDGumesrduI"
      },
      "source": [
        "## Merge neighboring classes (dogs and cats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2IRSzJeAzea"
      },
      "source": [
        "There are a bunch of options of how to merge. For example, we could take cats + dogs into a superclass 'pets' vs. two completely unrelated classes like airplanes and dogs into a superclass.\n",
        "\n",
        "On general grounds, we hypothesize grouping similar classes to do better but let's try and check this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tio0NcZ1BiO5"
      },
      "source": [
        "First train a model as usual on all classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIkNX8JeAyUG",
        "outputId": "d0347c49-9973-4631-da44-ad968c3c0830"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_class = registry.get(model_hparams)\n",
        "model_1_class_cuda = model_1_class.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uJlVzG_DDhC"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_class.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osSYgOwwDS5k",
        "outputId": "b2b150f0-bc0c-4195-8151-cfe4837b7646"
      },
      "source": [
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_class = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_class_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_class_cuda(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_class[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_class_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 2.13 \n",
            "\n",
            "Training accuracy: 0.20 \n",
            "\n",
            "Epoch 2, train loss: 1.83 \n",
            "\n",
            "Training accuracy: 0.31 \n",
            "\n",
            "Epoch 3, train loss: 1.72 \n",
            "\n",
            "Training accuracy: 0.35 \n",
            "\n",
            "Epoch 4, train loss: 1.64 \n",
            "\n",
            "Training accuracy: 0.38 \n",
            "\n",
            "Epoch 5, train loss: 1.58 \n",
            "\n",
            "Training accuracy: 0.41 \n",
            "\n",
            "Epoch 6, train loss: 1.52 \n",
            "\n",
            "Training accuracy: 0.44 \n",
            "\n",
            "Epoch 7, train loss: 1.46 \n",
            "\n",
            "Training accuracy: 0.46 \n",
            "\n",
            "Epoch 8, train loss: 1.41 \n",
            "\n",
            "Training accuracy: 0.49 \n",
            "\n",
            "Epoch 9, train loss: 1.36 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 10, train loss: 1.31 \n",
            "\n",
            "Training accuracy: 0.52 \n",
            "\n",
            "Epoch 11, train loss: 1.27 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 12, train loss: 1.24 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 13, train loss: 1.21 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 14, train loss: 1.18 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 15, train loss: 1.15 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 16, train loss: 1.13 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 17, train loss: 1.10 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 18, train loss: 1.08 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 19, train loss: 1.06 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 20, train loss: 1.04 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 21, train loss: 1.02 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 22, train loss: 1.01 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 23, train loss: 0.99 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 24, train loss: 0.98 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 25, train loss: 0.96 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 26, train loss: 0.95 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 27, train loss: 0.93 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 28, train loss: 0.92 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 29, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 30, train loss: 0.89 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 31, train loss: 0.88 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 32, train loss: 0.87 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 33, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 34, train loss: 0.85 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 35, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 36, train loss: 0.83 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 37, train loss: 0.82 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 38, train loss: 0.81 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 39, train loss: 0.80 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 40, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOrl7WrLBnmy"
      },
      "source": [
        "Next train a model on merged classes. First try merging optically similar classes: cats + dogs = pets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hh2J4QkJ2xe",
        "outputId": "eb676c2b-0b05-4181-c806-75093a2fcba2"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_class_merged = registry.get(model_hparams)\n",
        "model_1_class_merged_cuda = model_1_class_merged.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYPuKX7uKFZX",
        "outputId": "b4f4f3fe-bea0-4bdc-fb0d-063b4bd7ddd8"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_class_merged.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al\n",
        "\n",
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_class = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_class_merged_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        y[y==3] = y[y==5] = 5 #treat every cat and dog as dog\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_class_merged(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_class[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_class_merged_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 1.98 \n",
            "\n",
            "Training accuracy: 0.28 \n",
            "\n",
            "Epoch 2, train loss: 1.66 \n",
            "\n",
            "Training accuracy: 0.39 \n",
            "\n",
            "Epoch 3, train loss: 1.55 \n",
            "\n",
            "Training accuracy: 0.43 \n",
            "\n",
            "Epoch 4, train loss: 1.47 \n",
            "\n",
            "Training accuracy: 0.46 \n",
            "\n",
            "Epoch 5, train loss: 1.40 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 6, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 7, train loss: 1.29 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 8, train loss: 1.24 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 9, train loss: 1.20 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 10, train loss: 1.16 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 11, train loss: 1.12 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 12, train loss: 1.09 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 13, train loss: 1.06 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 14, train loss: 1.03 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 15, train loss: 1.00 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 16, train loss: 0.98 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 17, train loss: 0.96 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 18, train loss: 0.93 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 19, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 20, train loss: 0.89 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 21, train loss: 0.87 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 22, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 23, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 24, train loss: 0.83 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 25, train loss: 0.81 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 26, train loss: 0.80 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 27, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 28, train loss: 0.78 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 29, train loss: 0.77 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 30, train loss: 0.76 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 31, train loss: 0.75 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 32, train loss: 0.74 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 33, train loss: 0.73 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 34, train loss: 0.72 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 35, train loss: 0.71 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 36, train loss: 0.70 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 37, train loss: 0.69 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 38, train loss: 0.69 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 39, train loss: 0.68 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 40, train loss: 0.67 \n",
            "\n",
            "Training accuracy: 0.77 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "40B85tkPhBDU",
        "outputId": "6007487e-bd6f-445a-d301-1caf13dd76ae"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen = len(torch.flatten(forget_matrix_class))\n",
        "hist = plt.hist(torch.flatten(forget_matrix_class), label = \"Events\", weights = np.ones(forgetlen)/forgetlen)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75 merged classes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fko1sbVCABF"
      },
      "source": [
        "Now test both on merged classes criteria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBgVVg-7CV46",
        "outputId": "dc7b6150-0fed-4f72-ad7e-2bbdac5d7803"
      },
      "source": [
        "model_1_class_merged_cuda.eval()\n",
        "model_1_class_cuda.eval()\n",
        "\n",
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "accuracies_model_1 = list()\n",
        "accuracies_model_2 = list()\n",
        "\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    l1 = model_1_class_cuda(x.cuda())\n",
        "    l2 = model_1_class_merged_cuda(x.cuda())\n",
        "\n",
        "    y[y==3] = y[y==5] = 5\n",
        "\n",
        "    for k in range(len(l1)):\n",
        "        l1_max = torch.max(l1[k,3], l1[k,5])\n",
        "        l1_min = torch.min(l1[k,3], l1[k,5])\n",
        "        l1_remain1 = l1[k,0:3]\n",
        "        l1_remain2 = l1[k,6:]\n",
        "\n",
        "        l1[k] = torch.tensor((l1_remain1[0], l1_remain1[1], l1_remain1[2], l1_min, l1[k,4], l1_max, l1_remain2[0], l1_remain2[1], l1_remain2[2], l1_remain2[3])).cuda()\n",
        "\n",
        "    accuracies_model_1.append(y.eq(l1.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    accuracies_model_2.append(y.eq(l2.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    print(f'Test accuracy for model 1: {torch.tensor(accuracies_model_1).mean():.2f}')\n",
        "    print(f'Test accuracy for model 2: {torch.tensor(accuracies_model_2).mean():.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy for model 1: 0.81\n",
            "Test accuracy for model 2: 0.78\n",
            "Test accuracy for model 1: 0.80\n",
            "Test accuracy for model 2: 0.80\n",
            "Test accuracy for model 1: 0.80\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.78\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.75\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.75\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.75\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.75\n",
            "Test accuracy for model 1: 0.78\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.78\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.78\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.77\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCxBTHm-cRDa",
        "outputId": "6b023518-d3e3-484c-b2bd-92c808784b1e"
      },
      "source": [
        "model_1_class_merged_cuda.eval()\n",
        "model_1_class_cuda.eval()\n",
        "\n",
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "x, y = next(iter(test_set))\n",
        "l1 = model_1_class_cuda(x.cuda())\n",
        "l2 = model_1_class_merged_cuda(x.cuda())\n",
        "\n",
        "for k in range(len(l1)):\n",
        "        l1_max = torch.max(l1[k,3], l1[k,5])\n",
        "        l1_remain1 = l1[k,0:3]\n",
        "        l1_remain2 = l1[k,6:]\n",
        "\n",
        "        l1[k] = torch.tensor((l1_remain1[0], l1_remain1[1], l1_remain1[2], 0., l1[k,4], l1_max, l1_remain2[0], l1_remain2[1], l1_remain2[2], l1_remain2[3])).cuda()\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1)\n",
        "tempnum = 9\n",
        "print(softmaxfunc(l1)[tempnum].round())\n",
        "print(softmaxfunc(l2)[tempnum].round())\n",
        "print(y[tempnum])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0',\n",
            "       grad_fn=<RoundBackward>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
            "       grad_fn=<RoundBackward>)\n",
            "tensor(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxUrNRGYd6Eb",
        "outputId": "3a160ebd-c334-4e1a-ee33-76961371efd9"
      },
      "source": [
        "softmaxfunc(l1)[tempnum]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0073, 0.0201, 0.0262, 0.0321, 0.0225, 0.7338, 0.1283, 0.0181, 0.0021,\n",
              "        0.0095], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrZJwiWig1oq"
      },
      "source": [
        "they are roughly comparable in this case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfAlVK7Xgt5q",
        "outputId": "4345558c-34f8-4a5b-c8ad-bbed78b54096"
      },
      "source": [
        "print(torch.tensor(accuracies_model_1).mean())\n",
        "print(torch.tensor(accuracies_model_2).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7629)\n",
            "tensor(0.7640)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5mvvKXlrjJr"
      },
      "source": [
        "## Merge random classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKf7RhuTrqzp"
      },
      "source": [
        "Now try merging optically unrelated classes e.g. horse + automobile = means of transport"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3rcvx3arlZ0",
        "outputId": "28f3e65d-78df-41fc-dd43-959ac198c322"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_class = registry.get(model_hparams)\n",
        "model_1_class_cuda = model_1_class.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paYFz0-xrzjU",
        "outputId": "5f280d07-4ffd-423e-da22-51fce0b335a7"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_class.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al\n",
        "\n",
        "a_i = torch.zeros(len(train_set),128)\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_class = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_class_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_class_cuda(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_class[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_class_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 2.14 \n",
            "\n",
            "Training accuracy: 0.20 \n",
            "\n",
            "Epoch 2, train loss: 1.86 \n",
            "\n",
            "Training accuracy: 0.30 \n",
            "\n",
            "Epoch 3, train loss: 1.76 \n",
            "\n",
            "Training accuracy: 0.34 \n",
            "\n",
            "Epoch 4, train loss: 1.69 \n",
            "\n",
            "Training accuracy: 0.37 \n",
            "\n",
            "Epoch 5, train loss: 1.63 \n",
            "\n",
            "Training accuracy: 0.39 \n",
            "\n",
            "Epoch 6, train loss: 1.58 \n",
            "\n",
            "Training accuracy: 0.41 \n",
            "\n",
            "Epoch 7, train loss: 1.53 \n",
            "\n",
            "Training accuracy: 0.43 \n",
            "\n",
            "Epoch 8, train loss: 1.48 \n",
            "\n",
            "Training accuracy: 0.46 \n",
            "\n",
            "Epoch 9, train loss: 1.43 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 10, train loss: 1.38 \n",
            "\n",
            "Training accuracy: 0.50 \n",
            "\n",
            "Epoch 11, train loss: 1.33 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 12, train loss: 1.29 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 13, train loss: 1.26 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 14, train loss: 1.22 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 15, train loss: 1.19 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 16, train loss: 1.17 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 17, train loss: 1.14 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 18, train loss: 1.12 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 19, train loss: 1.10 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 20, train loss: 1.08 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 21, train loss: 1.07 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 22, train loss: 1.05 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 23, train loss: 1.03 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 24, train loss: 1.02 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 25, train loss: 1.00 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 26, train loss: 0.99 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 27, train loss: 0.98 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 28, train loss: 0.96 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 29, train loss: 0.95 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 30, train loss: 0.94 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 31, train loss: 0.93 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 32, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 33, train loss: 0.90 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 34, train loss: 0.89 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 35, train loss: 0.88 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 36, train loss: 0.88 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 37, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 38, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 39, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 40, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "uFc9C-sOwdnt",
        "outputId": "303607a2-a3d7-4183-aa45-b494826ba124"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen = len(torch.flatten(forget_matrix_class))\n",
        "hist = plt.hist(torch.flatten(forget_matrix_class), label = \"Events\", weights = np.ones(forgetlen)/forgetlen)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MxHeTrmszuF",
        "outputId": "74959125-f31a-4a9d-a52c-32bdbb32d8af"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_class_merged = registry.get(model_hparams)\n",
        "model_1_class_merged_cuda = model_1_class_merged.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_class_merged.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al\n",
        "\n",
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_class = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_class_merged_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        y[y==0] = y[y==7] = 7 #treat every horse and airplane as airplane\n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_class_merged(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_class[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_class_merged_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, train loss: 2.02 \n",
            "\n",
            "Training accuracy: 0.24 \n",
            "\n",
            "Epoch 2, train loss: 1.75 \n",
            "\n",
            "Training accuracy: 0.32 \n",
            "\n",
            "Epoch 3, train loss: 1.64 \n",
            "\n",
            "Training accuracy: 0.37 \n",
            "\n",
            "Epoch 4, train loss: 1.57 \n",
            "\n",
            "Training accuracy: 0.40 \n",
            "\n",
            "Epoch 5, train loss: 1.51 \n",
            "\n",
            "Training accuracy: 0.43 \n",
            "\n",
            "Epoch 6, train loss: 1.44 \n",
            "\n",
            "Training accuracy: 0.46 \n",
            "\n",
            "Epoch 7, train loss: 1.39 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 8, train loss: 1.34 \n",
            "\n",
            "Training accuracy: 0.50 \n",
            "\n",
            "Epoch 9, train loss: 1.30 \n",
            "\n",
            "Training accuracy: 0.51 \n",
            "\n",
            "Epoch 10, train loss: 1.27 \n",
            "\n",
            "Training accuracy: 0.53 \n",
            "\n",
            "Epoch 11, train loss: 1.24 \n",
            "\n",
            "Training accuracy: 0.54 \n",
            "\n",
            "Epoch 12, train loss: 1.21 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 13, train loss: 1.18 \n",
            "\n",
            "Training accuracy: 0.57 \n",
            "\n",
            "Epoch 14, train loss: 1.15 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 15, train loss: 1.13 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 16, train loss: 1.11 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 17, train loss: 1.09 \n",
            "\n",
            "Training accuracy: 0.60 \n",
            "\n",
            "Epoch 18, train loss: 1.06 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 19, train loss: 1.04 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 20, train loss: 1.03 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 21, train loss: 1.01 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 22, train loss: 0.99 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 23, train loss: 0.97 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 24, train loss: 0.96 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 25, train loss: 0.94 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 26, train loss: 0.92 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 27, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 28, train loss: 0.90 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 29, train loss: 0.89 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 30, train loss: 0.87 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 31, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 32, train loss: 0.85 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 33, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 34, train loss: 0.83 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 35, train loss: 0.82 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 36, train loss: 0.82 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 37, train loss: 0.81 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 38, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 39, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 40, train loss: 0.78 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rs817ics0TDD",
        "outputId": "fb7ae3b5-12a9-48c9-fa44-ac73d8b5d228"
      },
      "source": [
        "import numpy as np\n",
        "forgetlen = len(torch.flatten(forget_matrix_class))\n",
        "hist = plt.hist(torch.flatten(forget_matrix_class), label = \"Events\", weights = np.ones(forgetlen)/forgetlen)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75, classes merged')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsZFLopy0f3x",
        "outputId": "54610ddb-5681-4186-e5fc-bb42a05738d0"
      },
      "source": [
        "model_1_class_merged_cuda.eval()\n",
        "model_1_class_cuda.eval()\n",
        "\n",
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "accuracies_model_1 = list()\n",
        "accuracies_model_2 = list()\n",
        "\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    l1 = model_1_class_cuda(x.cuda())\n",
        "    l2 = model_1_class_merged_cuda(x.cuda())\n",
        "\n",
        "    y[y==0] = y[y==7] = 7\n",
        "\n",
        "    for k in range(len(l1)):\n",
        "        l1_max = torch.max(l1[k,0], l1[k,7])\n",
        "        l1_remain1 = l1[k,1:7]\n",
        "        l1_remain2 = l1[k,8:]\n",
        "\n",
        "        l1[k] = torch.tensor((0, l1_remain1[0], l1_remain1[1], l1_remain1[2], l1_remain1[3], l1_remain1[4], l1_remain1[5], l1_max, l1_remain2[0], l1_remain2[1])).cuda()\n",
        "\n",
        "    accuracies_model_1.append(y.eq(l1.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    accuracies_model_2.append(y.eq(l2.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    print(f'Test accuracy for model 1: {torch.tensor(accuracies_model_1).mean():.2f}')\n",
        "    print(f'Test accuracy for model 2: {torch.tensor(accuracies_model_2).mean():.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.76\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.74\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.74\n",
            "Test accuracy for model 2: 0.75\n",
            "Test accuracy for model 1: 0.74\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.71\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.71\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.71\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.70\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.71\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.74\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.73\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.72\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n",
            "Test accuracy for model 1: 0.72\n",
            "Test accuracy for model 2: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWsQzlDe2JaC",
        "outputId": "ce44a60a-fa21-44f4-c535-65354c2be409"
      },
      "source": [
        "print(torch.tensor(accuracies_model_1).mean())\n",
        "print(torch.tensor(accuracies_model_2).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7233)\n",
            "tensor(0.7265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sD8kEvS2YQo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpFsViG2ZQN"
      },
      "source": [
        "## Merge a bunch of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EECkfTZl2lCD",
        "outputId": "fc711cc6-51fe-44ad-d1f7-904aac45e39c"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_class_merged = registry.get(model_hparams)\n",
        "model_1_class_merged_cuda = model_1_class_merged.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_class_merged.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al\n",
        "\n",
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_class = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_class_merged_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        y[y==0] = y[y==1] = y[y==2] = y[y==3] = 3 \n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_class_merged(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_class[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_class_merged_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, train loss: 1.69 \n",
            "\n",
            "Training accuracy: 0.39 \n",
            "\n",
            "Epoch 2, train loss: 1.46 \n",
            "\n",
            "Training accuracy: 0.42 \n",
            "\n",
            "Epoch 3, train loss: 1.37 \n",
            "\n",
            "Training accuracy: 0.46 \n",
            "\n",
            "Epoch 4, train loss: 1.31 \n",
            "\n",
            "Training accuracy: 0.48 \n",
            "\n",
            "Epoch 5, train loss: 1.27 \n",
            "\n",
            "Training accuracy: 0.50 \n",
            "\n",
            "Epoch 6, train loss: 1.22 \n",
            "\n",
            "Training accuracy: 0.52 \n",
            "\n",
            "Epoch 7, train loss: 1.17 \n",
            "\n",
            "Training accuracy: 0.55 \n",
            "\n",
            "Epoch 8, train loss: 1.13 \n",
            "\n",
            "Training accuracy: 0.56 \n",
            "\n",
            "Epoch 9, train loss: 1.10 \n",
            "\n",
            "Training accuracy: 0.58 \n",
            "\n",
            "Epoch 10, train loss: 1.06 \n",
            "\n",
            "Training accuracy: 0.59 \n",
            "\n",
            "Epoch 11, train loss: 1.03 \n",
            "\n",
            "Training accuracy: 0.61 \n",
            "\n",
            "Epoch 12, train loss: 1.00 \n",
            "\n",
            "Training accuracy: 0.62 \n",
            "\n",
            "Epoch 13, train loss: 0.98 \n",
            "\n",
            "Training accuracy: 0.63 \n",
            "\n",
            "Epoch 14, train loss: 0.95 \n",
            "\n",
            "Training accuracy: 0.64 \n",
            "\n",
            "Epoch 15, train loss: 0.93 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 16, train loss: 0.91 \n",
            "\n",
            "Training accuracy: 0.65 \n",
            "\n",
            "Epoch 17, train loss: 0.89 \n",
            "\n",
            "Training accuracy: 0.66 \n",
            "\n",
            "Epoch 18, train loss: 0.87 \n",
            "\n",
            "Training accuracy: 0.67 \n",
            "\n",
            "Epoch 19, train loss: 0.86 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 20, train loss: 0.84 \n",
            "\n",
            "Training accuracy: 0.68 \n",
            "\n",
            "Epoch 21, train loss: 0.83 \n",
            "\n",
            "Training accuracy: 0.69 \n",
            "\n",
            "Epoch 22, train loss: 0.81 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 23, train loss: 0.80 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 24, train loss: 0.79 \n",
            "\n",
            "Training accuracy: 0.70 \n",
            "\n",
            "Epoch 25, train loss: 0.78 \n",
            "\n",
            "Training accuracy: 0.71 \n",
            "\n",
            "Epoch 26, train loss: 0.76 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 27, train loss: 0.75 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 28, train loss: 0.74 \n",
            "\n",
            "Training accuracy: 0.72 \n",
            "\n",
            "Epoch 29, train loss: 0.73 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 30, train loss: 0.72 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 31, train loss: 0.71 \n",
            "\n",
            "Training accuracy: 0.73 \n",
            "\n",
            "Epoch 32, train loss: 0.70 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 33, train loss: 0.70 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 34, train loss: 0.69 \n",
            "\n",
            "Training accuracy: 0.74 \n",
            "\n",
            "Epoch 35, train loss: 0.68 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 36, train loss: 0.67 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 37, train loss: 0.67 \n",
            "\n",
            "Training accuracy: 0.75 \n",
            "\n",
            "Epoch 38, train loss: 0.66 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 39, train loss: 0.65 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n",
            "Epoch 40, train loss: 0.65 \n",
            "\n",
            "Training accuracy: 0.76 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0-MX5kT-ZmK",
        "outputId": "7861cb2a-d026-496e-eae2-b39892b192b1"
      },
      "source": [
        "model_1_class_merged_cuda.eval()\n",
        "model_1_class_cuda.eval()\n",
        "\n",
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "accuracies_model_1 = list()\n",
        "accuracies_model_2 = list()\n",
        "\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    l1 = model_1_class_cuda(x.cuda())\n",
        "    l2 = model_1_class_merged_cuda(x.cuda())\n",
        "\n",
        "    y[y==0] = y[y==1] = y[y==2] = y[y==3] = 3\n",
        "\n",
        "    for k in range(len(l1)):\n",
        "        l1_max = torch.max(l1[k,0:4])\n",
        "        l1_remain1 = l1[k,4:10]\n",
        "\n",
        "        l1[k] = torch.tensor((0, 0, l1_max, l1_max, l1_remain1[0], l1_remain1[1], l1_remain1[2], l1_remain1[3],l1_remain1[4], l1_remain1[5])).cuda()\n",
        "\n",
        "    accuracies_model_1.append(y.eq(l1.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    accuracies_model_2.append(y.eq(l2.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    print(f'Test accuracy for model 1: {torch.tensor(accuracies_model_1).mean():.2f}')\n",
        "    print(f'Test accuracy for model 2: {torch.tensor(accuracies_model_2).mean():.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy for model 1: 0.40\n",
            "Test accuracy for model 2: 0.82\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.79\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.79\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.78\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.43\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.46\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.76\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.44\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n",
            "Test accuracy for model 1: 0.45\n",
            "Test accuracy for model 2: 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qijzklKm_ThT",
        "outputId": "86ae139f-4a76-4dd0-ba4c-26cbf6b671f4"
      },
      "source": [
        "print(torch.tensor(accuracies_model_1).mean())\n",
        "print(torch.tensor(accuracies_model_2).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7507)\n",
            "tensor(0.7626)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ybtueh1NnJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vApCQ51OlF"
      },
      "source": [
        "## Merge all but 1 class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KANyoj4-1c6w",
        "outputId": "1794b34f-efde-49a4-c8c0-d444c025616d"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_1_class_merged = registry.get(model_hparams)\n",
        "model_1_class_merged_cuda = model_1_class_merged.cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_1_class_merged.parameters(), lr=1e-3, momentum=0.9) #note changes here to match Toneva et al\n",
        "\n",
        "#log output\n",
        "#train_output = open(\"/content/models/forget1/train_output.txt\", \"x\")\n",
        "#forget_output = open(\"/content/models/forget1/forget_output.txt\", \"x\")\n",
        "\n",
        "#intialize forget list which tracks how much each example has been forgotten\n",
        "#initialized to 0 because at first, nothing is forgotten\n",
        "a_i = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i = torch.zeros(len(train_set),128)\n",
        "forget_matrix_class = torch.zeros(len(train_set),128)\n",
        "batch_tracker = 0 #track which batch we're in\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "\n",
        "#training and validation loop\n",
        "nb_epochs = 40\n",
        "for epoch in range(nb_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "\n",
        "    model_1_class_merged_cuda.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        y[y==0] = y[y==1] = y[y==2] = y[y==3] = y[y==4] = y[y==5] = y[y==6] = y[y==7] = y[y==8] = 8 \n",
        "\n",
        "        #1. forward pass\n",
        "            #with torch.no_grad():\n",
        "        l = model_1_class_merged(x)\n",
        "\n",
        "        #check if it's correctly classified\n",
        "        l_softmax = softmaxfunc(l)\n",
        "        for k in range(len(l_softmax)):\n",
        "            if torch.argmax(l_softmax[k])==y[k]:\n",
        "                a_i[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i[batch_tracker, k] < a_tilde_i[batch_tracker, k]:\n",
        "                forget_matrix_class[batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i[batch_tracker, k] = a_i[batch_tracker, k]\n",
        "\n",
        "\n",
        "        #2. compute objective function\n",
        "        J = loss(l, y.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_1_class_merged_cuda.zero_grad()\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(l.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "    batch_tracker = 0\n",
        "\n",
        " #   train_output.write(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f} \\n\")\n",
        " #   train_output.write(f\"Training accuracy: {torch.tensor(accuracies).mean():.2f} \\n\")\n",
        "\n",
        "#forget_output.write(f\"{forget_matrix}\")\n",
        "\n",
        "#forget_output.close()\n",
        "#train_output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1, train loss: 0.41 \n",
            "\n",
            "Training accuracy: 0.87 \n",
            "\n",
            "Epoch 2, train loss: 0.26 \n",
            "\n",
            "Training accuracy: 0.90 \n",
            "\n",
            "Epoch 3, train loss: 0.24 \n",
            "\n",
            "Training accuracy: 0.90 \n",
            "\n",
            "Epoch 4, train loss: 0.23 \n",
            "\n",
            "Training accuracy: 0.91 \n",
            "\n",
            "Epoch 5, train loss: 0.22 \n",
            "\n",
            "Training accuracy: 0.91 \n",
            "\n",
            "Epoch 6, train loss: 0.21 \n",
            "\n",
            "Training accuracy: 0.92 \n",
            "\n",
            "Epoch 7, train loss: 0.20 \n",
            "\n",
            "Training accuracy: 0.92 \n",
            "\n",
            "Epoch 8, train loss: 0.19 \n",
            "\n",
            "Training accuracy: 0.92 \n",
            "\n",
            "Epoch 9, train loss: 0.19 \n",
            "\n",
            "Training accuracy: 0.92 \n",
            "\n",
            "Epoch 10, train loss: 0.18 \n",
            "\n",
            "Training accuracy: 0.93 \n",
            "\n",
            "Epoch 11, train loss: 0.18 \n",
            "\n",
            "Training accuracy: 0.93 \n",
            "\n",
            "Epoch 12, train loss: 0.17 \n",
            "\n",
            "Training accuracy: 0.93 \n",
            "\n",
            "Epoch 13, train loss: 0.17 \n",
            "\n",
            "Training accuracy: 0.93 \n",
            "\n",
            "Epoch 14, train loss: 0.16 \n",
            "\n",
            "Training accuracy: 0.94 \n",
            "\n",
            "Epoch 15, train loss: 0.16 \n",
            "\n",
            "Training accuracy: 0.94 \n",
            "\n",
            "Epoch 16, train loss: 0.15 \n",
            "\n",
            "Training accuracy: 0.94 \n",
            "\n",
            "Epoch 17, train loss: 0.15 \n",
            "\n",
            "Training accuracy: 0.94 \n",
            "\n",
            "Epoch 18, train loss: 0.15 \n",
            "\n",
            "Training accuracy: 0.94 \n",
            "\n",
            "Epoch 19, train loss: 0.15 \n",
            "\n",
            "Training accuracy: 0.94 \n",
            "\n",
            "Epoch 20, train loss: 0.14 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 21, train loss: 0.14 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 22, train loss: 0.14 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 23, train loss: 0.14 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 24, train loss: 0.13 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 25, train loss: 0.13 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 26, train loss: 0.13 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 27, train loss: 0.13 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 28, train loss: 0.13 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 29, train loss: 0.12 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 30, train loss: 0.12 \n",
            "\n",
            "Training accuracy: 0.95 \n",
            "\n",
            "Epoch 31, train loss: 0.12 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 32, train loss: 0.12 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 33, train loss: 0.12 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 34, train loss: 0.11 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 35, train loss: 0.12 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 36, train loss: 0.11 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 37, train loss: 0.11 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 38, train loss: 0.11 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 39, train loss: 0.11 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n",
            "Epoch 40, train loss: 0.11 \n",
            "\n",
            "Training accuracy: 0.96 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d6QNp5W7rgz",
        "outputId": "535bfed1-b6c8-4de1-fa07-e48e7614901b"
      },
      "source": [
        "model_1_class_merged_cuda.eval()\n",
        "model_1_class_cuda.eval()\n",
        "\n",
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "accuracies_model_1 = list()\n",
        "accuracies_model_2 = list()\n",
        "\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    l1 = model_1_class_cuda(x.cuda())\n",
        "    l2 = model_1_class_merged_cuda(x.cuda())\n",
        "\n",
        "    y[y==0] = y[y==1] = y[y==2] = y[y==3] = y[y==4] = y[y==5] = y[y==6] = y[y==7] = y[y==8] =  8\n",
        "\n",
        "    for k in range(len(l1)):\n",
        "        l1_max = torch.max(l1[k,0:9])\n",
        "        l1_remain1 = l1[k,9]\n",
        "\n",
        "        l1[k] = torch.tensor((0, 0, 0, 0, 0, 0, 0, 0, l1_max, l1_remain1)).cuda()\n",
        "\n",
        "    accuracies_model_1.append(y.eq(l1.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    accuracies_model_2.append(y.eq(l2.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    print(f'Test accuracy for model 1: {torch.tensor(accuracies_model_1).mean():.2f}')\n",
        "    print(f'Test accuracy for model 2: {torch.tensor(accuracies_model_2).mean():.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.93\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.95\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.94\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.95\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.95\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.95\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.95\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.95\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.97\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.97\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.97\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n",
            "Test accuracy for model 1: 0.97\n",
            "Test accuracy for model 2: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-68Y96dW85Ua",
        "outputId": "ad32fc3d-4cbe-4836-a98c-443f23de3461"
      },
      "source": [
        "torch.tensor(accuracies_model_1).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMu3MrHV869W",
        "outputId": "7b647209-0f61-4af6-f2db-435fa6c7c8d3"
      },
      "source": [
        "torch.tensor(accuracies_model_2).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9626)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Gz3NKy8pVK",
        "outputId": "b8f0c8a3-63e1-4a5d-a785-2bf4d299408f"
      },
      "source": [
        "testvec=[0,1,2,3,4,5,6,7,8,9]\n",
        "print(testvec[0:9])\n",
        "print(testvec[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfTpGGsyNb-d"
      },
      "source": [
        "\n",
        "# Distillation (with MSE loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2ZVuKiRQgPK"
      },
      "source": [
        "(Note: restart runtime before running)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNy5qaaiNkCx"
      },
      "source": [
        "In this section we want to consider two models, A and B.\n",
        "\n",
        "Model A trains as usual on CIFAR10. Model B attempts to copy model A. However, when model A forgets an example, model B aim to copy the logits (output vectors) of model A before A forgot. We could do this latter step in a number of ways, but one simple way is to just use the output vectors for the example at the epoch before it was forgottten. Another way is to look for the minimum loss.\n",
        "\n",
        "To code this up we need the following:\n",
        "\n",
        "\n",
        "1.   A way to keep track of logits between epochs. For now, we will just need the \"most recent\" set of logits.\n",
        "2.   A method to compute the cross entropy between logits (note that torch's CrossEntropy() assumes you are giving it a list of classes, but we can easily come up with a slight modification to this)\n",
        "3.   We will need to construct a temp set of logits, say l_A_prime, which are optimal and feed those into the loss function to get B's loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQTM4WR5PtZC",
        "outputId": "a7ee93fa-f5b5-45bc-83a7-5ef99f18a579"
      },
      "source": [
        "#First load models, dataset\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'open_lth' already exists and is not an empty directory.\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfSicNVQTSZb",
        "outputId": "af137344-28b1-464b-c12d-934596303575"
      },
      "source": [
        "nb_epochs = 60\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "model_B_on_A_acc = torch.zeros(len(train_set),128)\n",
        "model_B_on_A_acc_tilde = torch.zeros(len(train_set),128)\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt image classification\n",
        "forget_matrix_B_on_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt A logits\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "#initialize previous logits\n",
        "model_A.eval()\n",
        "#first_x, first_y = next(iter(train_set))\n",
        "\n",
        "l_A_previous = torch.zeros(nb_epochs, len(train_set), 128, 10)\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "\n",
        "acc_A_global = list()\n",
        "acc_B_global = list()\n",
        "acc_A_unmodified_global = list()\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "batch_tracker = 0\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "    accuracies_A_unmodified = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        l_A = model_A(x)\n",
        "        l_A_prime = l_A.detach().clone()\n",
        "        l_B = model_B(x.detach().clone())\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            l_A_unmod = model_A(x)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            l_A_previous[epoch, batch_tracker, 0:len(l_A)] = l_A.detach().clone() #set previous logits equal to current logits to start with\n",
        "    \n",
        "        #batch_size_current = len(batch[1])\n",
        "        #output_vectors[epoch, batch_tracker, 0:batch_size_current] = l_A\n",
        "\n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "        if epoch >1:\n",
        "            l_A_previous_softmax = softmaxfunc(l_A_previous[epoch-1, batch_tracker])\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "                \n",
        "                #if we forgot the example, build new logits which involve previous ones\n",
        "                #we will use this to rewind the logits\n",
        "                #try:\n",
        "                if epoch > 1 and torch.argmax(l_A_previous_softmax[k])==y[k]:\n",
        "                    #print(f\"Forgot example! Replacing {l_A_prime[k]} with {l_A_previous[epoch-1,batch_tracker, k]} \\n\")\n",
        "                    #print(f\"Was it classified correctly before? {torch.argmax(l_A_previous_softmax[k])==y[k]}\")\n",
        "                    l_A_prime[k] = l_A_previous[epoch-1,batch_tracker, k]\n",
        "                #except IndexError:\n",
        "                    #print(f\"batch_Tracker {batch_tracker} k {k}\")\n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "    \n",
        "        #now do the same with model B\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y[k]:\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]:\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "            #also measure how much it forgets w.r.t. the task it is trained for\n",
        "            #(how much it forgets model A's logits)\n",
        "            if torch.argmax(l_B_softmax[k])==torch.argmax(l_A_softmax[k]):\n",
        "                model_B_on_A_acc[batch_tracker, k] = 0\n",
        "            else:\n",
        "                model_B_on_A_acc[batch_tracker, k] = 1\n",
        "            \n",
        "            if model_B_on_A_acc[batch_tracker, k] < model_B_on_A_acc_tilde[batch_tracker, k]:\n",
        "                forget_matrix_B_on_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            model_B_on_A_acc_tilde[batch_tracker, k] = model_B_on_A_acc[batch_tracker, k]\n",
        "\n",
        "\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y.cuda())\n",
        "        #print(J_A)\n",
        "\n",
        "        #we need to first see if the example got forgotten\n",
        "        #if so, we need to feed in l_A_previous to model_B's loss function\n",
        "        #otherwise, use the current logits\n",
        "\n",
        "        #model B's loss is to compute cross entropy between its output and the output of model A\n",
        "        \n",
        "        #J_B = loss_B(l_B, l_A_prime)\n",
        "        J_B = loss_B(l_B, l_A_prime.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "        #reset previous logits\n",
        "        #l_A_previous[] = l_A\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "\n",
        "        #J_A.backward(retain_graph=True)\n",
        "        J_B.backward() #bug!\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        accuracies_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        accuracies_A_unmodified.append(y.eq(l_A_unmod.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f}\\n\")\n",
        "    #print(f\"Training accuracy for A on unmodified set: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_A_unmodified).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "    acc_B_global.append(torch.tensor(accuracies_B).mean())\n",
        "    acc_A_unmodified_global.append(torch.tensor(accuracies_A_unmodified).mean())\n",
        "\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A, B: 2.10 , 0.17 \n",
            "\n",
            "Training accuracy for A, B: 0.22 , 0.18\n",
            "\n",
            "Epoch 2, train loss A, B: 1.86 , 0.17 \n",
            "\n",
            "Training accuracy for A, B: 0.31 , 0.25\n",
            "\n",
            "Epoch 3, train loss A, B: 1.72 , 0.24 \n",
            "\n",
            "Training accuracy for A, B: 0.36 , 0.29\n",
            "\n",
            "Epoch 4, train loss A, B: 1.63 , 0.29 \n",
            "\n",
            "Training accuracy for A, B: 0.39 , 0.32\n",
            "\n",
            "Epoch 5, train loss A, B: 1.57 , 0.31 \n",
            "\n",
            "Training accuracy for A, B: 0.41 , 0.34\n",
            "\n",
            "Epoch 6, train loss A, B: 1.51 , 0.33 \n",
            "\n",
            "Training accuracy for A, B: 0.44 , 0.36\n",
            "\n",
            "Epoch 7, train loss A, B: 1.46 , 0.35 \n",
            "\n",
            "Training accuracy for A, B: 0.46 , 0.38\n",
            "\n",
            "Epoch 8, train loss A, B: 1.42 , 0.37 \n",
            "\n",
            "Training accuracy for A, B: 0.48 , 0.40\n",
            "\n",
            "Epoch 9, train loss A, B: 1.38 , 0.39 \n",
            "\n",
            "Training accuracy for A, B: 0.50 , 0.41\n",
            "\n",
            "Epoch 10, train loss A, B: 1.34 , 0.42 \n",
            "\n",
            "Training accuracy for A, B: 0.51 , 0.43\n",
            "\n",
            "Epoch 11, train loss A, B: 1.30 , 0.45 \n",
            "\n",
            "Training accuracy for A, B: 0.53 , 0.44\n",
            "\n",
            "Epoch 12, train loss A, B: 1.27 , 0.46 \n",
            "\n",
            "Training accuracy for A, B: 0.54 , 0.46\n",
            "\n",
            "Epoch 13, train loss A, B: 1.23 , 0.47 \n",
            "\n",
            "Training accuracy for A, B: 0.55 , 0.47\n",
            "\n",
            "Epoch 14, train loss A, B: 1.21 , 0.48 \n",
            "\n",
            "Training accuracy for A, B: 0.56 , 0.49\n",
            "\n",
            "Epoch 15, train loss A, B: 1.17 , 0.49 \n",
            "\n",
            "Training accuracy for A, B: 0.58 , 0.50\n",
            "\n",
            "Epoch 16, train loss A, B: 1.15 , 0.51 \n",
            "\n",
            "Training accuracy for A, B: 0.59 , 0.51\n",
            "\n",
            "Epoch 17, train loss A, B: 1.12 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.60 , 0.52\n",
            "\n",
            "Epoch 18, train loss A, B: 1.10 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.61 , 0.53\n",
            "\n",
            "Epoch 19, train loss A, B: 1.07 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.62 , 0.54\n",
            "\n",
            "Epoch 20, train loss A, B: 1.05 , 0.56 \n",
            "\n",
            "Training accuracy for A, B: 0.62 , 0.55\n",
            "\n",
            "Epoch 21, train loss A, B: 1.03 , 0.57 \n",
            "\n",
            "Training accuracy for A, B: 0.63 , 0.56\n",
            "\n",
            "Epoch 22, train loss A, B: 1.02 , 0.58 \n",
            "\n",
            "Training accuracy for A, B: 0.64 , 0.57\n",
            "\n",
            "Epoch 23, train loss A, B: 1.00 , 0.58 \n",
            "\n",
            "Training accuracy for A, B: 0.65 , 0.57\n",
            "\n",
            "Epoch 24, train loss A, B: 0.98 , 0.58 \n",
            "\n",
            "Training accuracy for A, B: 0.65 , 0.58\n",
            "\n",
            "Epoch 25, train loss A, B: 0.96 , 0.59 \n",
            "\n",
            "Training accuracy for A, B: 0.66 , 0.59\n",
            "\n",
            "Epoch 26, train loss A, B: 0.95 , 0.60 \n",
            "\n",
            "Training accuracy for A, B: 0.66 , 0.60\n",
            "\n",
            "Epoch 27, train loss A, B: 0.93 , 0.60 \n",
            "\n",
            "Training accuracy for A, B: 0.67 , 0.60\n",
            "\n",
            "Epoch 28, train loss A, B: 0.92 , 0.59 \n",
            "\n",
            "Training accuracy for A, B: 0.67 , 0.61\n",
            "\n",
            "Epoch 29, train loss A, B: 0.91 , 0.60 \n",
            "\n",
            "Training accuracy for A, B: 0.68 , 0.61\n",
            "\n",
            "Epoch 30, train loss A, B: 0.89 , 0.61 \n",
            "\n",
            "Training accuracy for A, B: 0.68 , 0.62\n",
            "\n",
            "Epoch 31, train loss A, B: 0.88 , 0.61 \n",
            "\n",
            "Training accuracy for A, B: 0.69 , 0.63\n",
            "\n",
            "Epoch 32, train loss A, B: 0.87 , 0.62 \n",
            "\n",
            "Training accuracy for A, B: 0.69 , 0.63\n",
            "\n",
            "Epoch 33, train loss A, B: 0.86 , 0.62 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.64\n",
            "\n",
            "Epoch 34, train loss A, B: 0.85 , 0.63 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.64\n",
            "\n",
            "Epoch 35, train loss A, B: 0.84 , 0.63 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.65\n",
            "\n",
            "Epoch 36, train loss A, B: 0.82 , 0.64 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.65\n",
            "\n",
            "Epoch 37, train loss A, B: 0.82 , 0.64 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.65\n",
            "\n",
            "Epoch 38, train loss A, B: 0.81 , 0.64 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.66\n",
            "\n",
            "Epoch 39, train loss A, B: 0.80 , 0.65 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.66\n",
            "\n",
            "Epoch 40, train loss A, B: 0.78 , 0.66 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.66\n",
            "\n",
            "Epoch 41, train loss A, B: 0.78 , 0.67 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.67\n",
            "\n",
            "Epoch 42, train loss A, B: 0.77 , 0.67 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.67\n",
            "\n",
            "Epoch 43, train loss A, B: 0.76 , 0.67 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.67\n",
            "\n",
            "Epoch 44, train loss A, B: 0.75 , 0.68 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.68\n",
            "\n",
            "Epoch 45, train loss A, B: 0.74 , 0.68 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.68\n",
            "\n",
            "Epoch 46, train loss A, B: 0.74 , 0.70 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.69\n",
            "\n",
            "Epoch 47, train loss A, B: 0.73 , 0.70 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.69\n",
            "\n",
            "Epoch 48, train loss A, B: 0.72 , 0.71 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.69\n",
            "\n",
            "Epoch 49, train loss A, B: 0.71 , 0.72 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.69\n",
            "\n",
            "Epoch 50, train loss A, B: 0.70 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.69\n",
            "\n",
            "Epoch 51, train loss A, B: 0.70 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.70\n",
            "\n",
            "Epoch 52, train loss A, B: 0.69 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.70\n",
            "\n",
            "Epoch 53, train loss A, B: 0.69 , 0.74 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.71\n",
            "\n",
            "Epoch 54, train loss A, B: 0.68 , 0.74 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.71\n",
            "\n",
            "Epoch 55, train loss A, B: 0.68 , 0.75 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.71\n",
            "\n",
            "Epoch 56, train loss A, B: 0.66 , 0.76 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.71\n",
            "\n",
            "Epoch 57, train loss A, B: 0.66 , 0.77 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.72\n",
            "\n",
            "Epoch 58, train loss A, B: 0.65 , 0.77 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.72\n",
            "\n",
            "Epoch 59, train loss A, B: 0.65 , 0.77 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.72\n",
            "\n",
            "Epoch 60, train loss A, B: 0.64 , 0.78 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.72\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvxbkMghKprD"
      },
      "source": [
        "Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4URX4AMPKrL0",
        "outputId": "6fff8024-ced5-4bb5-88e1-60b96714c214"
      },
      "source": [
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = False\n",
        ")\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "test_accuracy_A = list()\n",
        "test_accuracy_B = list()\n",
        "\n",
        "batch_counter = 0\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l_A = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "        \n",
        "    test_accuracy_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    test_accuracy_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    if batch_counter%10 == 0:\n",
        "        print(f\"Test accuracy A: {torch.tensor(test_accuracy_A).mean():.2f} \\n\")\n",
        "        print(f\"Test accuracy B: {torch.tensor(test_accuracy_B).mean():.2f} \\n\")\n",
        "    \n",
        "    batch_counter+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy A: 0.77 \n",
            "\n",
            "Test accuracy B: 0.72 \n",
            "\n",
            "Test accuracy A: 0.77 \n",
            "\n",
            "Test accuracy B: 0.73 \n",
            "\n",
            "Test accuracy A: 0.76 \n",
            "\n",
            "Test accuracy B: 0.72 \n",
            "\n",
            "Test accuracy A: 0.76 \n",
            "\n",
            "Test accuracy B: 0.71 \n",
            "\n",
            "Test accuracy A: 0.76 \n",
            "\n",
            "Test accuracy B: 0.72 \n",
            "\n",
            "Test accuracy A: 0.76 \n",
            "\n",
            "Test accuracy B: 0.72 \n",
            "\n",
            "Test accuracy A: 0.76 \n",
            "\n",
            "Test accuracy B: 0.72 \n",
            "\n",
            "Test accuracy A: 0.76 \n",
            "\n",
            "Test accuracy B: 0.72 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pczXjHK5HzzS"
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "_bQS0EPGKo1V",
        "outputId": "c79a1766-a96f-44e6-a2be-6747a0b035ce"
      },
      "source": [
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], acc_A_global[1:], c =\"blue\", label = \"model A\")\n",
        "#plt.scatter(eplist[1:], acc_A_unmodified_global[1:], c =\"green\", label = \"model A on original set\")\n",
        "plt.scatter(eplist[1:], acc_B_global[1:], c =\"red\", label = \"model B\")\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"train accuracy\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGsCAYAAACVa3C8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3Rd5Xnn8d8jc3EcrsEOqwsjyaUmYMeOIcKh4RKwi4eElMsi07GrBGhKnXZCk0zpTGndlstgaGeymrkspq3bZsIMsoEYCJ6ES8GFUJM0WEpcp77QOo5sRGltHAgkDgHbz/yxt8yxfHTOe3T2Pvv2/ayldbT32Tp6tXWW/Ph9nvd5zd0FAACAzurKegAAAABVRBAGAACQAYIwAACADBCEAQAAZIAgDAAAIAMEYQAAABlINQgzs0vN7Hkz22ZmN9V5vtvMnjKz75jZRjP7SJrjAQAAyAtLq0+YmU2S9I+SLpE0Imm9pCXuvrnmmhWSvuPuf2pmsyQ94u69qQwIAAAgR45I8bXnS9rm7tslyczulXSFpM0117ik4+LPj5f0z81edOrUqd7b25vsSAEAAFIwNDT0srtPq/dcmkHYKZJeqDkekfSBMdfcIumvzew3Jb1T0i80e9He3l4NDg4mNUYAAIDUmNmO8Z7LujB/iaQvuft0SR+R9H/N7LAxmdlSMxs0s8Hdu3d3fJAAAABJSzMIe1HSqTXH0+NztX5V0v2S5O7flDRZ0tSxL+TuK9y9z937pk2rO6MHAABQKGkGYeslzTSzGWZ2lKTFktaMuWanpIWSZGZnKgrCmOoCAACll1oQ5u77JN0g6XFJWyTd7+6bzOw2M7s8vuxGSb9mZn8vaZWk6zyt5ZoAAAA5kmZhvtz9EUmPjDn3hzWfb5Z0XppjAAAAyKOsC/MBAAAqiSAMAAAgAwRhAAAAGSAIAwAAyABBGAAAQAYIwgAAADJAEAYAAJABgjAAAIAMEIQBAIDSGBiQenulrq7ocWBgYtd0AkEYAAAohGbB08CAtHSptGOH5B49Ll166HUh13SKFW2rxr6+Ph8cHMx6GAAAoINGg6e9e98+N2WKtGKF1N8fHff2RkHVWD090vBw+DVJMrMhd++r9xwzYQAAIHPNZrmWLTs0AJOi42XL3j7eubP+a9eeD7mmUwjCAABApkJShCHBU3d3/Wtqz4dc0ykEYQAAIHWNZrpCZrlCgqfly6MUZa0pU6LzrVzTKQRhAAAgVc1mukJmuUKCp/7+qEasp0cyix5ra8ZCr+kUCvMBAECqmhXDhxbLDwxEs2M7d0YzYMuXZxM8tYLCfAAAMCFJ9N1qNtMVmiLs74+CsgMHose8B2DNEIQBAIC6kuq71ayeK08pwk4iCAMAoKKSaAsRck1oPVeZZrlCEIQBAFBBSbWFCLmmqjNdzRCEAQBQQO3WaiXVFiK071YVZ7qaIQgDAKBgkqjVSqotRJ76bhUNQRgAAAWTRK1WyAxW0fpuFQ1BGAAAHdYsldhuy4eQa5JsC0GqcWIIwgAA6KBmacIkWj6EXMMMVvYIwgAASFC7bR+SavlAW4j8IwgDACAhSbR9SKrlAzNd+cfekQAAJCRkD8Sk9lFEMbB3JAAACUiiYL5ZmpCWD9VBEAYAQICkCuabpQlJI1YH6UgAABQFU8uWRbNW3d3RzFNt4BOSJhwN1GoL66dMIYiqMtKRAIDKa5RKTGofRWax0ApmwgAApddshiqJgnqgHmbCAACV1qz3VlL7KAKtIAgDAJResyArqX0UgVYQhAEAcq1ZW4iQa5oFWUnuowiEIggDAORWSMF8yDXNgixmuZAFCvMBALmVZMF8sxYUQBoozAcA5FISHehDrpFIJSJ/CMIAAKlptzdXSMF8yDVAHhGEAQBS0SzIatY2QgormKd1BIqKIAwAMCHNUolJ9OYKKZinqB4tC1ly2wEU5gMAWhayR2JXVzQDNpZZVJdFB3qkotkKjA5v8ElhPgAgUSGpxKR6cwHBQgoNQ968HUIQBgBoWRLb/JBGRMvazYFL4ctpO4AgDADQsqS2+aFtBIKFzHKFBFg5Wk5LEAYAOEyzCQe2+UHikpjlCgmwcpQHJwgDgIpp9m9dyIQDqUS0rN2mcUnkwKVcvXlZHQkAFRKyMIxVi0hcszdeifenarQ6kiAMACok5N+xZq0lgJY1e+OFvOk63FoiKbSoAIAKaZT1KVjdMvIgpLFpu5uAJrXSo2AIwgCgRJqV1hSsbhlZC6nVSmIT0Iqu9CAIA4ACaXcBWcHqlpG1kBWJSWwCWtE3HTVhAFAQSWwVNPo6OapbRpaavRlC3lChRYQVfeNRmA8ABdDs36gkF5ABiS2V5U3XEIX5AJCxJHpzJdUmCSXRbsF8EmnE0GtQn7un9iHpUknPS9om6aY6z39B0ob44x8lvdrsNd///vc7ABTJPfe4T5niHoVX0ceUKdH5UT09hz4/+tHT09o1o9+vp8fdLHqs/T4oiZA3VbNrzOq/ocwO/17N3lC86cYladDHiWlSS0ea2aQ4sLpE0oik9ZKWuPvmca7/TUlnufsnG70u6UgARZNUb66CtklCGpJIE5JG7Iis0pHzJW1z9+3u/qakeyVd0eD6JZJWpTgeAEhNJ3pzVXQBGeoJeVM1u4Y0YubSDMJOkfRCzfFIfO4wZtYjaYakv0lxPACQik725ipZmyRMVMibqtk1RPWZy0th/mJJq919f70nzWypmQ2a2eDu3bs7PDQAaIzeXGhZSFF9I0kVzBPVZyrNIOxFSafWHE+Pz9WzWA1Ske6+wt373L1v2rRpCQ4RANrXLOsTGmDx72FFhHahbxSkhbypiOxzL83C/CMUFeYvVBR8rZf0y+6+acx1Z0h6TNIMDxgMhfkAOi2J/l2okHbfMKzAKJVMCvPdfZ+kGyQ9LmmLpPvdfZOZ3WZml9dculjSvSEBGAB0WsikBfXNOCiJhm8h/btQCnTMB4AGQme5KrojC8ZKonVE6DZAKAQ65gPAOJqV3oR0ApCo56qMJN4wzaZOQ1Y+ohQIwgBUVkjmiH8PKySJvaWSaPhGfrsySEcCqKyQzBE10hWR1GbWSb1hyG+XBulIAJWUROaIVf4l0u5m1p18w5DfrgRmwgCUUlITGyiJZm+IkGJ43jCYAGbCAFROyMQGpTcV0uwNkeTeUkAggjAApUSqEYdIYjNr3jBIGEEYgMJqVOITuqqR0puSaFYAmNRm1rxhkCCCMAC51G63ADJHJZJE6wg2s0YOUZgPIHeS7BbAKv+CS3KFBW8IZKBRYT5BGIDcCfk3lZ1dSiKJ3dF5MyDHWB0JoFBCiurpZF8CSWx2LfFmQGERhAHIHboFVERIHxHeDCgxgjAAHdeszppuASXS6JedxGbXEm8GFBY1YQA6KnRrPWqoS6DZL5uCelQAhfkAcoOdXyqk2S+b3dFRARTmA8iNkAwUCqLdHdJJI6LiCMIAJKrdxuXIiSQapIb8smmQigojCAOQmKQalyNjIb9IdkgH2kZNGIDEUGddEkk2SOWXjYqjJgxAYtrtOCCRgcpcu7VcEjukAwkgCAMQrFmWinqvAkiqlotUI9A2gjAAwZqVAfHvcgEkVcvFykagbQRhAILRcaAg2s0Zh/4iSTUCbaEwH0AwGq0WQFJd6gEkgsJ8AEGS2NMRKWv2SyJnDBQGQRgASWH12qQbMxbySyJnDBQG6UgAkshS5UKznlohvyR+kUCukI4EkEhrKKQoiVkuiXQjUCAEYUAFJNUaCikKaR0Ruhcj6UagEAjCgApgm78CSHKWi9YRQCEQhAEVkGRrKExQs3wws1xA5VCYD1QAtdoZa9a7K/QaAIVDYT5QcaQaO6DRTFdIPphZLqByCMKAEmiW6eLf95Q1W/kQuvSUWi6gUkhHAgVHFisHmuV7yQcDlUU6Eii4djNdSFmzmS7ywQDqIAgDci6pTBdS1GxlI/lgAHUQhAE512ymiyarHZDEzubUewEYgyAMyDkyXRljZ3MAKSEIA3KOTFfKms1yhRbdMdMFoEUEYUDGyHRlKKlNswFgAgjCgAyR6cpYUptmA8AE0CcMyBDtozLW1RVFv2OZRdOKEo3YALSFPmFATpHpShmbZgPIMYIwIENkulIUkusNXVpK0R2AFBCEARmivUSK2DQbQM4RhAEpYmPtDLFpNoCcIwgDUhKSDZOIAdrSKMol1wsg5wjCgJSwsXbKmkW55HoB5BxBGJASVj6mrFmUS64XQM4RhAEpIRvWpmYFdSFRLrleADlGEAakhGxYG0IK6ohyARQcQRgwQax8TFFIQR1RLoCCY9siYALYySZlIdsJSdEvYtmyKAXZ3R0FYPwCAORIo22LCMKACWDPx5RxgwGUBHtHAglj5WObmuVySTUCqIBUgzAzu9TMnjezbWZ20zjX/JKZbTazTWa2Ms3xAEmhJrwNIUX3FNQBqIDU0pFmNknSP0q6RNKIpPWSlrj75pprZkq6X9ICd3/FzN7t7rsavS7pSOQBNWFtINUIoEKySkfOl7TN3be7+5uS7pV0xZhrfk3SXe7+iiQ1C8CAvGCipoEk+nsBQAWkGYSdIumFmuOR+Fyt0yWdbmbPmtnfmdml9V7IzJaa2aCZDe7evTul4QKHCmlBQR/QMejvBQDBsi7MP0LSTEkXSVoi6S/M7ISxF7n7Cnfvc/e+adOmdXiIqKLQzbcxBv29ACBYmkHYi5JOrTmeHp+rNSJpjbu/5e7fV1RDNjPFMQFB2Hx7gkK3EiKXCwCpBmHrJc00sxlmdpSkxZLWjLnmK4pmwWRmUxWlJ7enOCYgCGVLExSaaiSXCwDpBWHuvk/SDZIel7RF0v3uvsnMbjOzy+PLHpe0x8w2S3pK0n909z1pjQkIRdlSA42K5Ug1AkAwOuYDddCCYhwhN4athADgIDrmA2Ow+fYEhRTLkWoEgCDMhKFymOVqQ+jG2gAAScyEAYdg5WMbKJYDgMQQhKFyWPnYBgrvASAxBGGoHCZzGqBYDgA6hiAMlcNkzjhCtwmg8B4AEkEQhtJhMmeCKJYDgI5idSRKhZWPbWDlIwAkjtWRqAwmc9pAsRwAdBRBGEqFlY9toFgOADqKIAylwmROAxTLAUCuEIShVJjMGQcrHwEgdwjCUCpM5oyDYjkAyB1WRwJVwMpHAMgEqyNRGs3Kmiqr2Y2hWA4AcocgDIURWtZUOSE3hmI5AMgd0pEojN7eKL4Yq6cnqiGvrNAbMzAQ1YDt3BnNgC1fTrEcAKSMdCRKodI9wBqlG0NvDCsfASBXmgZhZjanEwMBmqlsWVOzdGNlbwwAFFvITNj/MrPnzOzfm9nxqY8IGEdly5qatZeo7I0BgGJrGoS5+wWS+iWdKmnIzFaa2SWpjwwYo7I9wJqlGyt7YwCg2IIL881skqQrJf0PSa9JMkm/5+4Ppje8w1GYj8phRQIAFFZbhflmNtfMviBpi6QFkn7R3c+MP/9CoiNF5dEHrA7SjQBQSiE1Yf9T0rclvc/dP+3u35Ykd/9nSb+f5uBQLZXtA8bG2gBQSU3TkWZ2jKSfuPv++LhL0mR339vwC1NCOrK8Kpl1G408awvvp0whyAKAkmi3T9iTkt5RczwlPgckqpJ9wNhYGwAqKyQIm+zuPxo9iD+f0uB6YEIq2e6qkpEnAEAKC8J+bGZnjx6Y2fsl/SS9IaGqKll/XsnIEwAghQVhn5P0ZTP7WzNbJ+k+STekOyxUUSXrzysZeQIApLBmreslnSHpNyT9uqQz3X0o7YGhmkq3vSErHwEA4wjdwPs9kmZJOlvSEjO7Jr0hoawq1wMstOdG6SJPAECIkBYVN0u6SFEQ9oikD0ta5+4fS310ddCiopgq2Ymhkj03AAC12m1R8TFJCyX9i7v/iqT3SWIjb7Skkp0YWPkIAGggJAj7ibsfkLTPzI6TtEvRZt5AsErGI6x8BAA0EBKEDZrZCZL+QtKQoi2MvpnqqFA6lYxHWPkIAGigYRBmZibpTnd/1d3/TNIlkq6N05JAsErGI6x8BAA00DAI86hq/5Ga42F335j6qFA6pYxHQpZ7svIRADCOIwKu+baZnRP3CwMmrL+/RDHI2OWeo+0npBL9kACANIXUhH1A0jfN7HtmttHMvmtmzIah2iq53BMAkKSQIOzfSDpN0gJJvyjpo/EjcFDlGrFWcrknACBJIUGYj/MBSApvDF8qlVzuCQBIUkgQ9jVJX40f10raLunRNAeFYqlkZq6Syz0BAEkK2cB7jrvPjR9nSpov+oShRmkzc41yrKVc7gkA6KSQ1ZGHcPdvm9kH0hgMiqm7u/4WiYXOzIWsfizVck8AQKc1DcLM7LdqDrsknS3pn1MbEQpn+fL6m3MXOjPXKMdK4AUASEBITdixNR9HK6oNuyLNQaFYSpmZK22OFQCQF01nwtz91k4MBMVWusxcKXOsAIA8aToTZmZPxBt4jx6faGaPpzssIGOsfgQApCwkHTnN3V8dPXD3VyS9O70hIW9K2Yi12Q9VyhwrACBPQlZH7jezbnffKUlm1iOatVZGKbdIDP2hSpdjBQDkibk3jqfM7FJJKyR9XZJJukDSUnfPJCXZ19fng4ODWXzrSurtrV8a1dMjDQ93ejQJKeUPBQDIIzMbcve+es+FFOY/ZmZnSzo3PvU5d385yQEiv0q5SLCUPxQAoGhCCvOvkvSWu3/V3b8qaZ+ZXZn+0JAHpdwisZQ/FACgaEIK82929x+OHsRF+jenNyTkSSkXCZbyhwIAFE1IEFbvmpa3O0IxlXKRYCl/KABA0YQU5n9R0quS7opPfVrSu9z9unSHVh+F+QAAoCgaFeaHzIT9pqQ3Jd0Xf/xUUSAW8o0vNbPnzWybmd1U5/nrzGy3mW2IP64PeV2goVI2NgMAlE3I6sgfSzosgGrGzCYpmj27RNKIpPVmtsbdN4+59D53v6HV1wfqKmVjMwBAGYWsjpxmZv/VzB4xs78Z/Qh47fmStrn7dnd/U9K9YuPvXCrVxNGyZW8HYKP27o3OAwCQIyHpyAFJWyXNkHSrpGFJ6wO+7hRJL9Qcj8TnxrrazDaa2WozO7XeC5nZUjMbNLPB3bt3B3xrhBqdONqxQ3J/e+KosIEYPcAAAAUREoSd5O5/pahX2Nfd/ZOSFiT0/f+fpF53nyvpCUl317vI3Ve4e5+7902bNi2hbw2phBNH9AADABRESBD2Vvz4kpldZmZnSXpXwNe9KKl2Zmt6fO4gd9/j7j+ND/9S0vsDXhcJKt3EET3AAAAFERKE3W5mx0u6UdJvKwqW/kPA162XNNPMZpjZUZIWS1pTe4GZ/UzN4eWStgSNGokp3cQRPcAAAAURsjryq/GnP5R0cegLu/s+M7tB0uOSJkn6ortvMrPbJA26+xpJnzGzyyXtk/QDSde1OH60afnyQxcTSiWYOOrvJ+gCAORe02ateUOz1uQNDEQ1YDt3RjNgy5cTwwAAkIR2m7Wi5Pr7peFh6cCB6DHXAVip+mkAAKqMPSBRHDRiBQCUSMjekUdLulpSr2qCNne/LdWRjYN0ZIX19kaB11g9PdEUHgAAOdMoHRkyE/awoqL8IUX7RgLZKF0/DQBAlYUEYdPd/dLURwI0091dfyassP00AABVFlKY/w0zm5P6SIBmaMQKACiRkCDsfElDZvZ8vMfjd81sY9oDQzJKtZiQRqwAgBIJSUd+OPVRIBWlXExII1YAQEmMOxNmZsfFn74+zgdyrnSbcwMAUCKN0pEr48chSYPx41DNMXKukIsJS5U/BQBgfOOmI939o/HjjM4NB0kq3GLCUuZPAQCoL2jbIjM70czmm9mFox9pDwztK9xiQvKnAIAKaVqYb2bXS/qspOmSNkg6V9I3JS1Id2ho1+jkUWE25y5k/hQAgIkJmQn7rKRzJO1w94slnSXp1VRHhcQUanPu8fKkuc2fAgAwcSFB2Bvu/oYU7SPp7lslvSfdYaGSCpc/BQBg4kKCsBEzO0HSVyQ9YWYPS6pT7g20iWasAIAKMXcPv9jsQ5KOl/SYu7+Z2qga6Ovr88FBOmQAAID8M7Mhd++r91zDmTAzm2RmW0eP3f3r7r4mqwAMhypcS63CDRgAgPQ0XB3p7vvjPSO73Z0lajlSuJZahRswAADpapqONLNnFK2IfE7Sj0fPu/vl6Q6tPtKRkd7e+o1Ye3qiVZC5U7gBAwDQvkbpyJANvP8g4fEgAYVrqVW4AQMAkK6Q1ZEfiWvBDn5I+kjaA0NjhWupVbgBAwCQrpAg7JI65z6c9EDQmsK11CrcgAEASNe4QZiZ/YaZfVfSe8xsY83H9yVt7NwQUU/hWmoVbsAAAKRr3MJ8Mzte0omS7pR0U81Tr7v7DzowtroozAcAAEUxocJ8d/+hpB9KWpLWwAAAAKoqpCYMAAAACSMIAwAAyABBGAAAQAYIwpAM9oUEAKAlIR3zgcbYFxIAgJYxE4b2LVv2dgA2au/e6DwAAKiLICynCpXdY19IAABaRhCWQ6PZvR07JPe3s3u5DcTYFxIAgJYRhOVQ4bJ77AsJAEDLCMJyqHDZPfaFBACgZayOzKHu7igFWe98bvX3E3QBANACZsJyKHfZvUKtEgAAoBgIwnIoV9m9wq0SAACgGMzdsx5DS/r6+nxwcDDrYVRHb2/93GhPjzQ83OnRAABQKGY25O599Z5jJgyNFW6VAAAAxUAQhsboAQYAQCoIwtBY7lYJAABQDgRhaCxXqwQAACgP+oShOXqAAQCQOGbCAAAAMkAQBgAAkAGCsIzQhB4AgGojCMtA7prQExECANBxBGEZWLZM2rv30HN790bnOy53ESEAANVAEJaBXDWhz1VECABAdRCEZSBXTehzFRECAFAdBGEZyFUT+lxFhAAAVAdBWAZy1YQ+VxEhAADVQcf8jOSmCf3oIJYti1KQ3d1RAJaLwQEAUF4EYchRRAgAQHWkmo40s0vN7Hkz22ZmNzW47mozczPrS3M8AAAAeZFaEGZmkyTdJenDkmZJWmJms+pcd6ykz0r6VlpjAQAAyJs0Z8LmS9rm7tvd/U1J90q6os51/1nSH0t6I8WxAAAA5EqaQdgpkl6oOR6Jzx1kZmdLOtXdv5biOAAAAHInsxYVZtYl6U8k3Rhw7VIzGzSzwd27d6c/uDJhX0gAAHIpzSDsRUmn1hxPj8+NOlbSeyU9bWbDks6VtKZecb67r3D3PnfvmzZtWopDLhn2hQQAILfSDMLWS5ppZjPM7ChJiyWtGX3S3X/o7lPdvdfdeyX9naTL3X0wxTFVC/tCAgCQW6kFYe6+T9INkh6XtEXS/e6+ycxuM7PL0/q+eZCbDCD7QgIAkFupNmt190ckPTLm3B+Oc+1FaY6lU0YzgKMTUKMZQCmDfqjd3dEA6p0HAACZYu/IhOUqA8i+kAAA5BZBWMJylQHM1U7hAACgFntHJix3GUD2hQQAIJeYCUsYGUAAABCCICxhZAABAEAI0pEpIAMIAACaYSasyHLTkAwAALSKmbCiylVDMgAA0CpmwooqVw3JAABAqwjCiipXDckAAECrCMKKarzGY2xJBABAIRCEFRUNyQAAKDSCsKKiIRkAAIXG6sgioyEZAACFxUwYAABABgjCAAAAMkAQBgAAkAGCMAAAgAwQhOUV+0ICAFBqBGEt6khsNLov5I4dkvvb+0ISiAEAUBoEYS3oWGzEvpAAAJQeQVgLOhYbsS8kAAClRxDWgo7FRuwLCQBA6RGEtaBjsRH7QgIAUHoEYS3oWGzEvpAAAJQee0e2YDQGWrYsSkF2d0cBWCqxEftCAgBQagRhLSI2AgAASSAdCQAAkAGCMAAAgAwQhAEAAGSAIAwAACADBGFZYYNuAAAqjdWRWRjdhHJ0D6TRTSglll4CAFARzIRlgQ26AQCoPIKwLLBBNwAAlUcQlgU26AYAoPIIwrLABt0AAFQeQVgW2KAbAIDKY3VkVtiEEgCASmMmDAAAIAMEYQAAABkgCAMAAMgAQRgAAEAGCMIAAAAyQBAGAACQAYIwAACADBCEpWFgQOrtlbq6oseBgaxHBAAAcoZmrUkbGJCWLpX27o2Od+yIjiWaswIAgIOYCUvasmVvB2Cj9u6NzgMAAMQIwpK2c2dr5wEAQCURhCWtu7u18wAAoJIIwpK2fLk0Zcqh56ZMic4DAADECMKS1t8vrVgh9fRIZtHjihUU5QMAgEMQhNVIrLNEf780PCwdOBA9EoABAIAxaFERo7MEAADoJGbCYnSWAAAAnUQQFqOzBAAA6KRUgzAzu9TMnjezbWZ2U53nf93MvmtmG8xsnZnNSnM8jdBZAgAAdFJqQZiZTZJ0l6QPS5olaUmdIGulu89x93mS/oukP0lrPM3QWQIAAHRSmjNh8yVtc/ft7v6mpHslXVF7gbu/VnP4Tkme4ngaorMEAADopDRXR54i6YWa4xFJHxh7kZl9WtJvSTpK0oJ6L2RmSyUtlaTuFPOD/f0EXQAAoDMyL8x397vc/TRJvyPp98e5ZoW797l737Rp0zo7QAAAgBSkGYS9KOnUmuPp8bnx3CvpyhTHAwAAkBtpBmHrJc00sxlmdpSkxZLW1F5gZjNrDi+T9E8pjgcAACA3UqsJc/d9ZnaDpMclTZL0RXffZGa3SRp09zWSbjCzX5D0lqRXJF2b1ngAAADyJNVti9z9EUmPjDn3hzWffzbN7w8AAJBXmRfmF05iu3wDAIAqYwPvVrDLNwAASAgzYa1gl28AAJAQgrBWsMs3AABICEFYK9jlGwAAJIQgrBXs8g0AABJCENYKdvkGAAAJYXVkq9jlGwAAJICZMAAAgAwwEwYAAA7z1ltvaWRkRG+88UbWQymEyZMna/r06TryyCODv4YgDAAAHGZkZETHHnusent7ZWZZDyfX3F179uzRyMiIZsyYEfx1pCMBAMBh3njjDZ100kkEYAHMTCeddFLLs4YEYQAAoC4CsHATuVcEYQAAoPR6e3v18ssvT/iaDRs2yMz02GOPJTYmgjAAAIAmVq1apfPPP1+rVq1K7DUJwgAAQNsGBqTeXqmrK3ocGGjv9YaHh3XGGWfouuuu0+mnn67+/n49+eSTOu+88zRz5kw999xzkqQf/OAHuvLKKzV37lyde+652nIBg8EAAAyWSURBVLhxoyRpz549WrRokWbPnq3rr79e7n7wte+55x7Nnz9f8+bN06c+9Snt37+/4VjcXV/+8pf1pS99SU888URiK0YJwgAAQFsGBqSlS6UdOyT36HHp0vYDsW3btunGG2/U1q1btXXrVq1cuVLr1q3T5z//ed1xxx2SpJtvvllnnXWWNm7cqDvuuEPXXHONJOnWW2/V+eefr02bNumqq67Szp07JUlbtmzRfffdp2effVYbNmzQpEmTNNBkoN/4xjc0Y8YMnXbaabrooov0ta99rb0fLEYQBgAA2rJsmbR376Hn9u6NzrdjxowZmjNnjrq6ujR79mwtXLhQZqY5c+ZoeHhYkrRu3Tp94hOfkCQtWLBAe/bs0WuvvaZnnnlGH//4xyVJl112mU488URJ0tq1azU0NKRzzjlH8+bN09q1a7V9+/aG41i1apUWL14sSVq8eHFiKUn6hAEAgLbEk0zB50MdffTRBz/v6uo6eNzV1aV9+/ZN6DXdXddee63uvPPOoOv379+vBx54QA8//LCWL19+sCfY66+/rmOPPXZCYxjFTBgAAGhLd3dr55N0wQUXHEwnPv3005o6daqOO+44XXjhhVq5cqUk6dFHH9Urr7wiSVq4cKFWr16tXbt2SYpqynbs2DHu669du1Zz587VCy+8oOHhYe3YsUNXX321HnroobbHThAGAADasny5NGXKoeemTInOp+2WW27R0NCQ5s6dq5tuukl33323pKhW7JlnntHs2bP14IMPqjuOCGfNmqXbb79dixYt0ty5c3XJJZfopZdeGvf1V61apauuuuqQc1dffXUiKUmrXS1QBH19fT44OJj1MAAAKLUtW7bozDPPDL5+YCCqAdu5M5oBW75c6u9PcYA5VO+emdmQu/fVu56aMAAA0Lb+/uoFXe0iHQkAAJABgjAAAIAMEIQBAABkgCAMAAAgAwRhtZLe+AoAAGAcBGGj0tr4CgAAZK63t1cvv/zyhK7p7e3VnDlzNG/ePM2ZM0cPP/xwImMiCBuV1sZXAACg8J566ilt2LBBq1ev1mc+85lEXpMgbFRaG18BAFAFCZf0DA8P64wzztB1112n008/Xf39/XryySd13nnnaebMmXruueckRdsOXXnllZo7d67OPfdcbdy4UZK0Z88eLVq0SLNnz9b111+v2ub099xzj+bPn6958+bpU5/6lPbv3x88rtdee+3gZuDtIggbleXGVwAAFFlKJT3btm3TjTfeqK1bt2rr1q1auXKl1q1bp89//vO64447JEXbE5111lnauHGj7rjjDl1zzTWSpFtvvVXnn3++Nm3apKuuuko740mVLVu26L777tOzzz6rDRs2aNKkSQf3nmzk4osv1nvf+1596EMf0u23397WzzWKjvmjli+P3jC1KclObXwFAECRNSrpaaON/owZMzRnzhxJ0uzZs7Vw4UKZmebMmaPh4WFJ0rp16/TAAw9IkhYsWKA9e/botdde0zPPPKMHH3xQknTZZZcdnL1au3athoaGdM4550iSfvKTn+jd735307E89dRTmjp1qr73ve9p4cKFuuiii3TMMcdM+GeTCMLeNvomqfrGVwAAtCqlkp6jjz764OddXV0Hj7u6urRv374Jvaa769prr9Wdd945oa8/7bTTdPLJJ2vz5s2aP3/+hF5jFOnIWv390vCwdOBA9EgABgBAcxmW9FxwwQUH04lPP/20pk6dquOOO04XXnihVq5cKUl69NFH9corr0iSFi5cqNWrV2vXrl2SopqyHTt2BH+/Xbt26fvf/756enraHjszYQAAoD0ZlvTccsst+uQnP6m5c+dqypQpuvvuuyVFtWJLlizR7Nmz9cEPflDdcUA4a9Ys3X777Vq0aJEOHDigI488UnfddVfToOriiy/WpEmT9NZbb+mP/uiPdPLJJ7c9dqtdLVAEfX19Pjg4mPUwAAAotS1btujMM88M/4KBgcqX9NS7Z2Y25O599a5nJgwAALSvv79yQVe7qAkDAADIAEEYAABABgjCAABAXUWrG8/SRO4VQRgAADjM5MmTtWfPHgKxAO6uPXv2aPLkyS19HYX5AADgMNOnT9fIyIh2796d9VAKYfLkyZo+fXpLX0MQBgAADnPkkUdqxowZWQ+j1EhHAgAAZIAgDAAAIAMEYQAAABko3LZFZrZbUvhOm/VNlfRyAsPB4bi36eL+pod7my7ub3q4t+lJ4t72uPu0ek8ULghLgpkNjrePE9rDvU0X9zc93Nt0cX/Tw71NT9r3lnQkAABABgjCAAAAMlDVIGxF1gMoMe5turi/6eHepov7mx7ubXpSvbeVrAkDAADIWlVnwgAAADJVqSDMzC41s+fNbJuZ3ZT1eIrOzL5oZrvM7B9qzr3LzJ4ws3+KH0/McoxFZWanmtlTZrbZzDaZ2Wfj89zfBJjZZDN7zsz+Pr6/t8bnZ5jZt+K/EfeZ2VFZj7WozGySmX3HzL4aH3NvE2Bmw2b2XTPbYGaD8Tn+LiTEzE4ws9VmttXMtpjZz6d5fysThJnZJEl3SfqwpFmSlpjZrGxHVXhfknTpmHM3SVrr7jMlrY2P0bp9km5091mSzpX06fj9yv1Nxk8lLXD390maJ+lSMztX0h9L+oK7/5ykVyT9aoZjLLrPStpSc8y9Tc7F7j6vpnUCfxeS898lPebuZ0h6n6L3cGr3tzJBmKT5kra5+3Z3f1PSvZKuyHhMhebuz0j6wZjTV0i6O/78bklXdnRQJeHuL7n7t+PPX1f0h+AUcX8T4ZEfxYdHxh8uaYGk1fF57u8Emdl0SZdJ+sv42MS9TRN/FxJgZsdLulDSX0mSu7/p7q8qxftbpSDsFEkv1ByPxOeQrJPd/aX483+RdHKWgykDM+uVdJakb4n7m5g4XbZB0i5JT0j6nqRX3X1ffAl/Iybuv0n6T5IOxMcniXubFJf012Y2ZGZL43P8XUjGDEm7Jf3vOJX+l2b2TqV4f6sUhKHDPFp6y/LbNpjZMZIekPQ5d3+t9jnub3vcfb+7z5M0XdFM+RkZD6kUzOyjkna5+1DWYymp8939bEWlNZ82swtrn+TvQluOkHS2pD9197Mk/VhjUo9J398qBWEvSjq15nh6fA7J+lcz+xlJih93ZTyewjKzIxUFYAPu/mB8mvubsDjd8JSkn5d0gpkdET/F34iJOU/S5WY2rKjsY4GiOhvubQLc/cX4cZekhxT9B4K/C8kYkTTi7t+Kj1crCspSu79VCsLWS5oZr9A5StJiSWsyHlMZrZF0bfz5tZIeznAshRXX0PyVpC3u/ic1T3F/E2Bm08zshPjzd0i6RFHd3VOSPhZfxv2dAHf/XXef7u69iv7O/o2794t72zYze6eZHTv6uaRFkv5B/F1IhLv/i6QXzOw98amFkjYrxftbqWatZvYRRbUKkyR90d2XZzykQjOzVZIuUrTL/L9KulnSVyTdL6lb0g5Jv+TuY4v30YSZnS/pbyV9V2/X1fyeorow7m+bzGyuogLbSYr+M3q/u99mZj+raPbmXZK+I+nj7v7T7EZabGZ2kaTfdvePcm/bF9/Dh+LDIyStdPflZnaS+LuQCDObp2hByVGStkv6FcV/I5TC/a1UEAYAAJAXVUpHAgAA5AZBGAAAQAYIwgAAADJAEAYAAJABgjAAAIAMEIQBQAAzu8jMvpr1OACUB0EYAABABgjCAJSKmX3czJ4zsw1m9ufxRt0/MrMvmNkmM1trZtPia+eZ2d+Z2UYze8jMTozP/5yZPWlmf29m3zaz0+KXP8bMVpvZVjMbiHc2AIAJIQgDUBpmdqakfyfpvHhz7v2S+iW9U9Kgu8+W9HVFuztI0v+R9DvuPlfR7gSj5wck3eXu75P0QUkvxefPkvQ5SbMk/ayifRIBYEKOaH4JABTGQknvl7Q+nqR6h6LNdg9Iui++5h5JD5rZ8ZJOcPevx+fvlvTleG++U9z9IUly9zckKX6959x9JD7eIKlX0rr0fywAZUQQBqBMTNLd7v67h5w0+4Mx1010v7bavQ73i7+hANpAOhJAmayV9DEze7ckmdm7zKxH0d+6j8XX/LKkde7+Q0mvmNkF8flPSPq6u78uacTMroxf42gzm9LRnwJAJfC/OACl4e6bzez3Jf21mXVJekvSpyX9WNL8+LldiurGJOlaSX8WB1nbJf1KfP4Tkv7czG6LX+PfdvDHAFAR5j7RWXkAKAYz+5G7H5P1OACgFulIAACADDATBgAAkAFmwgAAADJAEAYAAJABgjAAAIAMEIQBAABkgCAMAAAgAwRhAAAAGfj/xINVcGF67ygAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "cwjlUCDNiM-N",
        "outputId": "1730274b-f02d-4d32-d464-8484e2926447"
      },
      "source": [
        "import numpy as np\n",
        "cumulative_forgetting_A = torch.sum(forget_matrix_A, 0)\n",
        "cumulative_forgetting_B = torch.sum(forget_matrix_B, 0)\n",
        "\n",
        "forgetlen_A = len(torch.flatten(cumulative_forgetting_A))\n",
        "forgetlen_B = len(torch.flatten(cumulative_forgetting_B))\n",
        "\n",
        "hist_A = plt.hist(torch.flatten(cumulative_forgetting_A), alpha=0.5, label = \"Model A\", weights = np.ones(forgetlen_A)/forgetlen_A)\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B), alpha=0.5, label = \"Model B\", weights = np.ones(forgetlen_B)/forgetlen_B)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "smItJY0sxO2n",
        "outputId": "ffbef8d8-a73d-44c2-83f4-0ac0a5c0612d"
      },
      "source": [
        "cumulative_forgetting_B_on_A = torch.sum(forget_matrix_B_on_A, 0)\n",
        "\n",
        "forgetlen_B_on_A = len(torch.flatten(cumulative_forgetting_B_on_A))\n",
        "\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B_on_A), alpha=0.5, label = \"Model B (on A)\", weights = np.ones(forgetlen_B_on_A)/forgetlen_B_on_A)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwD8Xso_FkyT",
        "outputId": "dc1e9b46-957f-4473-8456-cdcd8701b1f9"
      },
      "source": [
        "forget_matrix_A[0:2].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 391, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfTqn4gKE8p4",
        "outputId": "621fc266-b748-4e02-b8eb-ba4c497575dc"
      },
      "source": [
        "torch.flatten(forget_matrix_A[0:2], 1).size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 50048])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o567LwCVBrc0"
      },
      "source": [
        "forget_fcn_epoch_A = list()\n",
        "forget_fcn_epoch_B = list()\n",
        "forget_fcn_epoch_B_on_A=list()\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    forget_fcn_epoch_A.append(torch.sum(torch.flatten(forget_matrix_A[i]),0))\n",
        "    forget_fcn_epoch_B.append(torch.sum(torch.flatten(forget_matrix_B[i]),0))\n",
        "    forget_fcn_epoch_B_on_A.append(torch.sum(torch.flatten(forget_matrix_B_on_A[i]),0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aayIhiYTG9tk",
        "outputId": "5a28f60f-8a66-4bf0-9a2d-3162a105bf6d"
      },
      "source": [
        "torch.flatten(forget_matrix_A[20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50048])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "NQvc_lvFEth1",
        "outputId": "c036450f-a65d-4c6f-d0f4-cb954d951530"
      },
      "source": [
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsJaXJTIIuT7"
      },
      "source": [
        "what is the scaling?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "zyX8AlOGIvVy",
        "outputId": "cd89c4d8-08b1-4243-e9d1-0bbfdd0c4109"
      },
      "source": [
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Hiv7YkBox4c3",
        "outputId": "b77a7c3b-a63e-406d-bca8-c9f672134886"
      },
      "source": [
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "O5Q50VEWx844",
        "outputId": "00d76b02-de20-4dba-82f1-589d5c69aec0"
      },
      "source": [
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B (on A)\")\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsHUYGJ1NQ_4"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL6M6qdNsSAV"
      },
      "source": [
        "testA = torch.flatten(cumulative_forgetting_A)\n",
        "testAnonzero = testA[testA!=0]\n",
        "\n",
        "testB = torch.flatten(cumulative_forgetting_B)\n",
        "testBnonzero = testB[testB!=0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "pSjwP297sg8r",
        "outputId": "991de4f6-dbe6-441d-9abd-c25d5ad268ef"
      },
      "source": [
        "plt.hist(testAnonzero, label = \"Events\", weights = np.ones(len(testAnonzero))/len(testAnonzero))\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "0HtxYoHNs06n",
        "outputId": "5494aa54-949f-4cd3-a2ff-b953a89e9576"
      },
      "source": [
        "plt.hist(testBnonzero, label = \"Events\", weights = np.ones(len(testBnonzero))/len(testBnonzero))\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "TF8-cyuXc47M",
        "outputId": "f530a2cd-b8a2-4cb6-ab11-ffd42df8e7fe"
      },
      "source": [
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B), label = \"Events\", weights = np.ones(forgetlen_B)/forgetlen_B)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(15, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SceY3k_rqNvf",
        "outputId": "2b4c038b-8725-4dfe-e010-55f424cd29ce"
      },
      "source": [
        "torch.sum(torch.sum(cumulative_forgetting_A,0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(232525.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWm9_JUGqY0w",
        "outputId": "a9a891be-d3f5-47d7-db2f-bf7ddcfb1af1"
      },
      "source": [
        "torch.sum(torch.sum(cumulative_forgetting_B,0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(235151.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyr0cAFicbuT",
        "outputId": "5766be3f-d83b-43d1-b276-ca1f474e65df"
      },
      "source": [
        "cumulative_forgetting_B -cumulative_forgetting_A"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -7.,  -4.,   0.,  ...,   1.,   2.,   4.],\n",
              "        [ -5.,   1., -12.,  ...,  -7.,   6.,   0.],\n",
              "        [  2.,   1.,  -7.,  ...,   1.,   1.,   0.],\n",
              "        ...,\n",
              "        [ -6.,   7.,  -2.,  ...,   0.,  -7.,  -2.],\n",
              "        [ -4.,   3.,   0.,  ...,   2.,  -2.,   1.],\n",
              "        [  2.,   0.,   0.,  ...,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEZkW0lTcg4b",
        "outputId": "c467d7ae-c24c-48ca-e61e-b4c4672a3a30"
      },
      "source": [
        "forget_matrix_A.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 391, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYBnlWSgprQ"
      },
      "source": [
        "x,y = next(iter(train_set))\n",
        "x=x.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kP7tfCUgeql"
      },
      "source": [
        "lb = model_B(x.cuda())\n",
        "la = model_A(x.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeXS8j4XfDj5",
        "outputId": "0cb09394-e2ac-4b5a-bc9b-f4c0ea52062a"
      },
      "source": [
        "la.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K06c_tdrCfZ",
        "outputId": "0e68683f-07a9-4026-b888-cee996fa4908"
      },
      "source": [
        "len(la)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVjCyB0zMfGr"
      },
      "source": [
        "# With Cross Entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKLGpWifMjmo",
        "outputId": "443e692d-ec58-46f6-9991-2c02ccdee685"
      },
      "source": [
        "#First load models, dataset\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.CrossEntropyLoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'open_lth' already exists and is not an empty directory.\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLEjIQGOyyme",
        "outputId": "827aa7f7-744b-48e6-bc22-b945871756b7"
      },
      "source": [
        "nb_epochs = 80\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "model_B_on_A_acc = torch.zeros(len(train_set),128)\n",
        "model_B_on_A_acc_tilde = torch.zeros(len(train_set),128)\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt image classification\n",
        "forget_matrix_B_on_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt A logits\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "#initialize previous logits\n",
        "model_A.eval()\n",
        "#first_x, first_y = next(iter(train_set))\n",
        "\n",
        "l_A_previous = torch.zeros(nb_epochs, len(train_set), 128, 10)\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "\n",
        "#initialize previous logits\n",
        "# for batch in train_set:\n",
        "#     x, y = batch\n",
        "#     x=x.cuda()\n",
        "#     l_A = model_A(x)\n",
        "#     with torch.no_grad():\n",
        "#         l_A_previous[0, batch_tracker, 0:len(l_A)] = l_A #set previous logits equal to current logits to start with\n",
        "#     batch_tracker += 1\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "batch_tracker = 0\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        l_A = model_A(x)\n",
        "        l_A_prime = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            l_A_previous[epoch, batch_tracker, 0:len(l_A)] = l_A #set previous logits equal to current logits to start with\n",
        "    \n",
        "        #batch_size_current = len(batch[1])\n",
        "        #output_vectors[epoch, batch_tracker, 0:batch_size_current] = l_A\n",
        "\n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "        if epoch >1:\n",
        "            l_A_previous_softmax = softmaxfunc(l_A_previous[epoch-1, batch_tracker])\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "                \n",
        "                #if we forgot the example, build new logits which involve previous ones\n",
        "                #we will use this to rewind the logits\n",
        "                #try:\n",
        "                if epoch > 1 and torch.argmax(l_A_previous_softmax[k])==y[k]:\n",
        "                    #print(f\"Forgot example! Replacing {l_A_prime[k]} with {l_A_previous[epoch-1,batch_tracker, k]} \\n\")\n",
        "                    #print(f\"Was it classified correctly before? {torch.argmax(l_A_previous_softmax[k])==y[k]}\")\n",
        "                    l_A_prime[k] = l_A_previous[epoch-1,batch_tracker, k]\n",
        "                #except IndexError:\n",
        "                    #print(f\"batch_Tracker {batch_tracker} k {k}\")\n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "    \n",
        "        #now do the same with model B\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y[k]:\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]:\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "            #also measure how much it forgets w.r.t. the task it is trained for\n",
        "            #(how much it forgets model A's logits)\n",
        "            if torch.argmax(l_B_softmax[k])==torch.argmax(l_A_softmax[k]):\n",
        "                model_B_on_A_acc[batch_tracker, k] = 0\n",
        "            else:\n",
        "                model_B_on_A_acc[batch_tracker, k] = 1\n",
        "            \n",
        "            if model_B_on_A_acc[batch_tracker, k] < model_B_on_A_acc_tilde[batch_tracker, k]:\n",
        "                forget_matrix_B_on_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            model_B_on_A_acc_tilde[batch_tracker, k] = model_B_on_A_acc[batch_tracker, k]\n",
        "\n",
        "\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y.cuda())\n",
        "        #print(J_A)\n",
        "\n",
        "        #we need to first see if the example got forgotten\n",
        "        #if so, we need to feed in l_A_previous to model_B's loss function\n",
        "        #otherwise, use the current logits\n",
        "\n",
        "        #model B's loss is to compute cross entropy between its output and the output of model A\n",
        "        \n",
        "        targets=torch.argmax(l_A_prime, 1)\n",
        "        #J_B = loss_B(l_B, l_A_prime)\n",
        "        J_B = loss_B(l_B, targets)\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "        #reset previous logits\n",
        "        #l_A_previous[] = l_A\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "\n",
        "        #J_A.backward(retain_graph=True)\n",
        "        J_B.backward() #bug!\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        accuracies_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f} \\n\")\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A, B: 2.12 , 1.61 \n",
            "\n",
            "Training accuracy for A, B: 0.22 , 0.17 \n",
            "\n",
            "Epoch 2, train loss A, B: 1.81 , 1.24 \n",
            "\n",
            "Training accuracy for A, B: 0.33 , 0.28 \n",
            "\n",
            "Epoch 3, train loss A, B: 1.69 , 1.04 \n",
            "\n",
            "Training accuracy for A, B: 0.37 , 0.33 \n",
            "\n",
            "Epoch 4, train loss A, B: 1.62 , 0.94 \n",
            "\n",
            "Training accuracy for A, B: 0.39 , 0.36 \n",
            "\n",
            "Epoch 5, train loss A, B: 1.56 , 0.89 \n",
            "\n",
            "Training accuracy for A, B: 0.42 , 0.39 \n",
            "\n",
            "Epoch 6, train loss A, B: 1.51 , 0.86 \n",
            "\n",
            "Training accuracy for A, B: 0.44 , 0.40 \n",
            "\n",
            "Epoch 7, train loss A, B: 1.46 , 0.85 \n",
            "\n",
            "Training accuracy for A, B: 0.46 , 0.42 \n",
            "\n",
            "Epoch 8, train loss A, B: 1.41 , 0.84 \n",
            "\n",
            "Training accuracy for A, B: 0.48 , 0.44 \n",
            "\n",
            "Epoch 9, train loss A, B: 1.37 , 0.83 \n",
            "\n",
            "Training accuracy for A, B: 0.50 , 0.46 \n",
            "\n",
            "Epoch 10, train loss A, B: 1.33 , 0.82 \n",
            "\n",
            "Training accuracy for A, B: 0.52 , 0.47 \n",
            "\n",
            "Epoch 11, train loss A, B: 1.29 , 0.80 \n",
            "\n",
            "Training accuracy for A, B: 0.53 , 0.49 \n",
            "\n",
            "Epoch 12, train loss A, B: 1.25 , 0.78 \n",
            "\n",
            "Training accuracy for A, B: 0.55 , 0.50 \n",
            "\n",
            "Epoch 13, train loss A, B: 1.22 , 0.77 \n",
            "\n",
            "Training accuracy for A, B: 0.56 , 0.52 \n",
            "\n",
            "Epoch 14, train loss A, B: 1.20 , 0.75 \n",
            "\n",
            "Training accuracy for A, B: 0.57 , 0.53 \n",
            "\n",
            "Epoch 15, train loss A, B: 1.17 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.58 , 0.54 \n",
            "\n",
            "Epoch 16, train loss A, B: 1.14 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.59 , 0.55 \n",
            "\n",
            "Epoch 17, train loss A, B: 1.12 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.59 , 0.56 \n",
            "\n",
            "Epoch 18, train loss A, B: 1.09 , 0.71 \n",
            "\n",
            "Training accuracy for A, B: 0.61 , 0.57 \n",
            "\n",
            "Epoch 19, train loss A, B: 1.07 , 0.71 \n",
            "\n",
            "Training accuracy for A, B: 0.61 , 0.57 \n",
            "\n",
            "Epoch 20, train loss A, B: 1.05 , 0.70 \n",
            "\n",
            "Training accuracy for A, B: 0.62 , 0.59 \n",
            "\n",
            "Epoch 21, train loss A, B: 1.04 , 0.69 \n",
            "\n",
            "Training accuracy for A, B: 0.63 , 0.59 \n",
            "\n",
            "Epoch 22, train loss A, B: 1.01 , 0.67 \n",
            "\n",
            "Training accuracy for A, B: 0.64 , 0.60 \n",
            "\n",
            "Epoch 23, train loss A, B: 1.00 , 0.67 \n",
            "\n",
            "Training accuracy for A, B: 0.64 , 0.61 \n",
            "\n",
            "Epoch 24, train loss A, B: 0.98 , 0.66 \n",
            "\n",
            "Training accuracy for A, B: 0.65 , 0.61 \n",
            "\n",
            "Epoch 25, train loss A, B: 0.97 , 0.65 \n",
            "\n",
            "Training accuracy for A, B: 0.65 , 0.62 \n",
            "\n",
            "Epoch 26, train loss A, B: 0.95 , 0.64 \n",
            "\n",
            "Training accuracy for A, B: 0.66 , 0.62 \n",
            "\n",
            "Epoch 27, train loss A, B: 0.95 , 0.63 \n",
            "\n",
            "Training accuracy for A, B: 0.66 , 0.63 \n",
            "\n",
            "Epoch 28, train loss A, B: 0.93 , 0.62 \n",
            "\n",
            "Training accuracy for A, B: 0.67 , 0.63 \n",
            "\n",
            "Epoch 29, train loss A, B: 0.92 , 0.62 \n",
            "\n",
            "Training accuracy for A, B: 0.67 , 0.64 \n",
            "\n",
            "Epoch 30, train loss A, B: 0.91 , 0.61 \n",
            "\n",
            "Training accuracy for A, B: 0.68 , 0.64 \n",
            "\n",
            "Epoch 31, train loss A, B: 0.90 , 0.60 \n",
            "\n",
            "Training accuracy for A, B: 0.68 , 0.65 \n",
            "\n",
            "Epoch 32, train loss A, B: 0.89 , 0.60 \n",
            "\n",
            "Training accuracy for A, B: 0.68 , 0.65 \n",
            "\n",
            "Epoch 33, train loss A, B: 0.88 , 0.59 \n",
            "\n",
            "Training accuracy for A, B: 0.69 , 0.65 \n",
            "\n",
            "Epoch 34, train loss A, B: 0.87 , 0.59 \n",
            "\n",
            "Training accuracy for A, B: 0.69 , 0.66 \n",
            "\n",
            "Epoch 35, train loss A, B: 0.86 , 0.59 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.66 \n",
            "\n",
            "Epoch 36, train loss A, B: 0.85 , 0.58 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.66 \n",
            "\n",
            "Epoch 37, train loss A, B: 0.84 , 0.58 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.67 \n",
            "\n",
            "Epoch 38, train loss A, B: 0.83 , 0.57 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.67 \n",
            "\n",
            "Epoch 39, train loss A, B: 0.82 , 0.57 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.67 \n",
            "\n",
            "Epoch 40, train loss A, B: 0.81 , 0.57 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.68 \n",
            "\n",
            "Epoch 41, train loss A, B: 0.80 , 0.56 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.68 \n",
            "\n",
            "Epoch 42, train loss A, B: 0.80 , 0.56 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.68 \n",
            "\n",
            "Epoch 43, train loss A, B: 0.79 , 0.56 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.69 \n",
            "\n",
            "Epoch 44, train loss A, B: 0.78 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.69 \n",
            "\n",
            "Epoch 45, train loss A, B: 0.77 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.69 \n",
            "\n",
            "Epoch 46, train loss A, B: 0.77 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.69 \n",
            "\n",
            "Epoch 47, train loss A, B: 0.76 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.70 \n",
            "\n",
            "Epoch 48, train loss A, B: 0.75 , 0.54 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.70 \n",
            "\n",
            "Epoch 49, train loss A, B: 0.74 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.70 \n",
            "\n",
            "Epoch 50, train loss A, B: 0.74 , 0.55 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.70 \n",
            "\n",
            "Epoch 51, train loss A, B: 0.73 , 0.54 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.71 \n",
            "\n",
            "Epoch 52, train loss A, B: 0.72 , 0.54 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.71 \n",
            "\n",
            "Epoch 53, train loss A, B: 0.71 , 0.54 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.71 \n",
            "\n",
            "Epoch 54, train loss A, B: 0.70 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.71 \n",
            "\n",
            "Epoch 55, train loss A, B: 0.70 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.72 \n",
            "\n",
            "Epoch 56, train loss A, B: 0.69 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.72 \n",
            "\n",
            "Epoch 57, train loss A, B: 0.69 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.72 \n",
            "\n",
            "Epoch 58, train loss A, B: 0.68 , 0.53 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.72 \n",
            "\n",
            "Epoch 59, train loss A, B: 0.68 , 0.52 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.73 \n",
            "\n",
            "Epoch 60, train loss A, B: 0.67 , 0.52 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.73 \n",
            "\n",
            "Epoch 61, train loss A, B: 0.66 , 0.52 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.73 \n",
            "\n",
            "Epoch 62, train loss A, B: 0.65 , 0.52 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.73 \n",
            "\n",
            "Epoch 63, train loss A, B: 0.65 , 0.52 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.73 \n",
            "\n",
            "Epoch 64, train loss A, B: 0.65 , 0.52 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.73 \n",
            "\n",
            "Epoch 65, train loss A, B: 0.64 , 0.51 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.74 \n",
            "\n",
            "Epoch 66, train loss A, B: 0.63 , 0.51 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.74 \n",
            "\n",
            "Epoch 67, train loss A, B: 0.62 , 0.51 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.74 \n",
            "\n",
            "Epoch 68, train loss A, B: 0.62 , 0.50 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.74 \n",
            "\n",
            "Epoch 69, train loss A, B: 0.62 , 0.51 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.74 \n",
            "\n",
            "Epoch 70, train loss A, B: 0.61 , 0.50 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.75 \n",
            "\n",
            "Epoch 71, train loss A, B: 0.61 , 0.50 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.75 \n",
            "\n",
            "Epoch 72, train loss A, B: 0.61 , 0.50 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.75 \n",
            "\n",
            "Epoch 73, train loss A, B: 0.60 , 0.49 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.75 \n",
            "\n",
            "Epoch 74, train loss A, B: 0.59 , 0.49 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.76 \n",
            "\n",
            "Epoch 75, train loss A, B: 0.59 , 0.48 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.76 \n",
            "\n",
            "Epoch 76, train loss A, B: 0.58 , 0.49 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.76 \n",
            "\n",
            "Epoch 77, train loss A, B: 0.58 , 0.49 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.76 \n",
            "\n",
            "Epoch 78, train loss A, B: 0.57 , 0.48 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.76 \n",
            "\n",
            "Epoch 79, train loss A, B: 0.57 , 0.48 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.76 \n",
            "\n",
            "Epoch 80, train loss A, B: 0.57 , 0.48 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.76 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "dkT5G0MDJAPX",
        "outputId": "4a7b36fc-31bf-40dc-851d-97f02a85b4de"
      },
      "source": [
        "import numpy as np\n",
        "cumulative_forgetting_A = torch.sum(forget_matrix_A, 0)\n",
        "cumulative_forgetting_B = torch.sum(forget_matrix_B, 0)\n",
        "\n",
        "forgetlen_A = len(torch.flatten(cumulative_forgetting_A))\n",
        "forgetlen_B = len(torch.flatten(cumulative_forgetting_B))\n",
        "\n",
        "hist_A = plt.hist(torch.flatten(cumulative_forgetting_A), alpha=0.5, label = \"Model A\", weights = np.ones(forgetlen_A)/forgetlen_A)\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B), alpha=0.5, label = \"Model B\", weights = np.ones(forgetlen_B)/forgetlen_B)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "1h493noY0ct3",
        "outputId": "ae980e27-0243-4b8b-b0fd-39a0465ab793"
      },
      "source": [
        "cumulative_forgetting_B_on_A = torch.sum(forget_matrix_B_on_A, 0)\n",
        "\n",
        "forgetlen_B_on_A = len(torch.flatten(cumulative_forgetting_B_on_A))\n",
        "\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B_on_A), alpha=0.5, label = \"Model B (on A)\", weights = np.ones(forgetlen_B_on_A)/forgetlen_B_on_A)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD4S44cxJSEW"
      },
      "source": [
        "forget_fcn_epoch_A = list()\n",
        "forget_fcn_epoch_B = list()\n",
        "forget_fcn_epoch_B_on_A=list()\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    forget_fcn_epoch_A.append(torch.sum(torch.flatten(forget_matrix_A[i]),0))\n",
        "    forget_fcn_epoch_B.append(torch.sum(torch.flatten(forget_matrix_B[i]),0))\n",
        "    forget_fcn_epoch_B_on_A.append(torch.sum(torch.flatten(forget_matrix_B_on_A[i]),0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "E3YB2ZMBJMPc",
        "outputId": "450e95c0-3014-4baa-db17-ff451f269328"
      },
      "source": [
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "QbRFwy1rJl4H",
        "outputId": "03d4220d-6915-4b85-b3b4-eeb98ed20ac3"
      },
      "source": [
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "rTu9VOY6JV5w",
        "outputId": "624cceed-4764-446c-af1d-1ccab6bc62e8"
      },
      "source": [
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B (on A)\")\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAGtCAYAAABwcoKLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xcdZXv/e/qpoE0gQY6wmsG7O7IRSB0DBAYlItAjjkIE7mNPDBNCCCEi9fnYR5HXz1zIDM0ejyMJ+iAGBXIIU0GDCCioI9kQAheMMGYCAljhO42Dk4mHadD6CC5rOePqupUV9eu2nXZddn1eb9e9Upq165dq2/Vq3+/31o/c3cBAACgvjVVOwAAAACUjqQOAAAgBkjqAAAAYoCkDgAAIAZI6gAAAGJgr2oHUG1Tpkzxrq6uaocBAACQ16pVqza7+7uyPdbwSV1XV5dWrlxZ7TAAAADyMrPBoMeYfgUAAIgBkjoAAIAYIKkDAACIgYZfUwcAQNzt2LFDGzdu1Ntvv13tUBDSvvvuq8MPP1wtLS2hn0NSBwBAzG3cuFH777+/urq6ZGbVDgd5uLuGh4e1ceNGTZ06NfTzmH4FACDm3n77bbW3t5PQ1QkzU3t7e8EjqyR1AAA0ABK6+lLM14ukDgAAIAZI6gAAQF3p6urS5s2biz5n9erVMjP94Ac/iCK8qiGpAwAADWXp0qU6/fTTtXTp0mqHUlYkdQAAYJz+fqmrS2pqSvzb31/a9QYGBnTMMcfoqquu0tFHH62enh49/fTTOu2003TUUUfpxRdflCRt2bJFF154oaZPn65TTz1Va9askSQNDw9r9uzZmjZtmq699lq5+9i1lyxZolNOOUUzZszQ9ddfr127duWMxd317W9/W/fff79+9KMfxarNC0kdAAAY098vzZ8vDQ5K7ol/588vPbHbsGGDbr75Zq1fv17r16/Xgw8+qBUrVuiOO+7Q7bffLkm65ZZbdMIJJ2jNmjW6/fbbdeWVV0qSFixYoNNPP10vv/yyLrroIg0NDUmS1q1bp4ceekgvvPCCVq9erebmZvXnCfQnP/mJpk6dqiOOOEJnnXWWvv/975f2gdUQkjoAADCmt1caHR1/bHQ0cbwUU6dOVXd3t5qamjRt2jTNmjVLZqbu7m4NDAxIklasWKG5c+dKks455xwNDw9r69ateu6553TFFVdIks4//3wddNBBkqTly5dr1apVOvnkkzVjxgwtX75cr732Ws44li5dqssuu0ySdNlll8VqCpbmwxHqX9uv3uW9GhoZUkdbh/pm9amnu6faYQEAECg5CBb6eFj77LPP2P+bmprG7jc1NWnnzp1FXdPdNW/ePH3hC18Idf6uXbv0yCOP6PHHH1dfX99Yk98333xT+++/f1Ex1BJG6iLSv7Zf85+Yr8GRQblcgyODmv/EfPWvLXH8GgCACHV0FHa8nM4444yx6dNnn31WU6ZM0QEHHKAzzzxTDz74oCTpqaee0h//+EdJ0qxZs7Rs2TJt2rRJUmJN3uDgYOD1ly9frunTp+t3v/udBgYGNDg4qEsuuUSPPfZYxB9ZZZDURaR3ea9Gd4wfvx7dMare5SWOXwMAEKG+Pqm1dfyx1tbE8ajdeuutWrVqlaZPn67Pfe5zWrx4saTEWrvnnntO06ZN06OPPqqOZIZ53HHH6bbbbtPs2bM1ffp0fehDH9Ibb7wReP2lS5fqoosuGnfskksuic0UrKVXkDSimTNn+sqVK8t+3aYFTXJN/NyaTLtv2V321wMAIMi6det07LHHhj6/vz+xhm5oKDFC19cn9bB6qOKyfd3MbJW7z8x2PmvqItLR1qHBkYlDwB1tFRi/BgCgBD09JHH1iOnXiPTN6lNry/jx69aWVvXNqsD4NQAAaDgkdRHp6e7RojmL1NnWKZOps61Ti+YsovoVAABEgunXCPV095DEAQCAimCkDgAAIAZI6gAAAGKApA4AANSVrq4ubd68uahzurq61N3drRkzZqi7u1uPP/54VGFWHGvqAABAQ3nmmWc0ZcoUvfrqq5o9e7YuuOCCaodUFozUAQCA8fr7pa4uqakp8W9/aVtcDgwM6JhjjtFVV12lo48+Wj09PXr66ad12mmn6aijjtKLL74oKbHN14UXXqjp06fr1FNP1Zo1ayRJw8PDmj17tqZNm6Zrr71W6RsnLFmyRKeccopmzJih66+/Xrt27Qod19atW3XQQQeV9LHVEpI6AACwR3+/NH++NDgouSf+nT+/5MRuw4YNuvnmm7V+/XqtX79eDz74oFasWKE77rhDt99+u6TEdmAnnHCC1qxZo9tvv11XXnmlJGnBggU6/fTT9fLLL+uiiy7S0NCQpMSOCw899JBeeOEFrV69Ws3NzWN7x+Zy9tln6/jjj9cHP/hB3XbbbSV9XLWE6VcAALBHb680On7vco2OJo6XsM3E1KlT1d3dLUmaNm2aZs2aJTNTd3e3BgYGJEkrVqzQI488Ikk655xzNDw8rK1bt+q5557To48+Kkk6//zzx0bXli9frlWrVunkk0+WJG3fvl2HHHJI3lhS06+//e1vNWvWLJ111lmaPHly0R9brSCpAwAAeyRHwUIfD2mfffYZ+39TU9PY/aamJu3cubOoa7q75s2bpy984QtFPf+II47QoYceqldeeUWnnHJKUdeoJUy/AgCAPToC9igPOl5GZ5xxxtj06bPPPqspU6bogAMO0JlnnqkHH3xQkvTUU0/pj3/8oyRp1qxZWrZsmTZt2iQpsSZvcHDivutBNm3apNdff12dnZ1l/kiqg5E6AACwR19fYg1d+hRsa2vieMRuvfVWXXPNNZo+fbpaW1u1ePFiSYm1dpdffrmmTZumD3zgA+pIJpjHHXecbrvtNs2ePVu7d+9WS0uL7rrrrrxJ2tlnn63m5mbt2LFDX/ziF3XooYdG/rFVgqVXkDSimTNn+sqVK6sdBgAAkVm3bp2OPfbY8E/o70+soRsaSozQ9fWVtJ4Oxcn2dTOzVe4+M9v5jNQBAIDxenpI4uoQa+oAAABigKQOAAAgBkjqAAAAYoCkDgAAIAZI6gAAAGKApA4AACAGSOoAAEBd6erq0ubNm4s6p6urS93d3ZoxY4a6u7v1+OOPZ32+u+ucc87R1q1byxKzJM2YMUOXXXbZuGN/8zd/o3/9138ty/VJ6gAAwDj9a/vVtbBLTQua1LWwS/1r+6sdUlk988wzWr16tZYtW6ZPfepTWc958skn9b73vU8HHHBAWV5z3bp12rVrl55//nm99dZbY8c/+clP6otf/GJZXoOkDgAAjOlf26/5T8zX4MigXK7BkUHNf2J+SYndwMCAjjnmGF111VU6+uij1dPTo6efflqnnXaajjrqKL344ouSEnu3XnjhhZo+fbpOPfVUrVmzRpI0PDys2bNna9q0abr22muVvhvWkiVLdMopp2jGjBm6/vrrtWvXrtBxbd26VQcddFD2z0N/vy644IKx+1/+8pd1/PHH6/jjj9fChQvHPq5jjz1W1113naZNm6bZs2dr+/btWa+3dOlSzZ07V7Nnzx43OtjZ2anh4WH94Q9/CB13EJI6AAAwpnd5r0Z3jI47NrpjVL3Le0u67oYNG3TzzTdr/fr1Wr9+vR588EGtWLFCd9xxh26//XZJiT1eTzjhBK1Zs0a33367rrzySknSggULdPrpp+vll1/WRRddpKGhIUmJ0a+HHnpIL7zwglavXq3m5mb19+dPPs8++2wdf/zx+uAHP6jbbrst6zkvvPCCTjrpJEnSqlWrdN999+nnP/+5fvazn+kb3/iGfvnLX0qSfvOb3+jjH/+4Xn75ZR144IF65JFHsl7voYce0mWXXabLL79cS5cuHffYiSeeqBdeeCHEZzE3tgkDAABjhkaGCjoe1tSpU9Xd3S1JmjZtmmbNmiUzU3d3twYGBiRJK1asGEuKzjnnHA0PD2vr1q167rnn9Oijj0qSzj///LHRteXLl2vVqlU6+eSTJUnbt2/XIYcckjeWZ555RlOmTNFvf/tbzZo1S2eddZYmT5487pwtW7Zo//33H4vroosu0n777SdJuvjii/X888/rIx/5iKZOnaoZM2ZIkk466aSxjyXdypUrNWXKFHV0dOiwww7TNddcoy1btujggw+WJB1yyCH693//99CfyyCM1AEAgDEdbR0FHQ9rn332Gft/U1PT2P2mpibt3LmzqGu6u+bNm6fVq1dr9erVevXVV3XrrbeGfv4RRxyhQw89VK+88sqEx/baay/t3r077zXSP67m5uasH8vSpUu1fv16dXV16YgjjtDWrVvHjei9/fbbmjRpUui4g5DUAQCAMX2z+tTa0jruWGtLq/pm9UX+2mecccbY9Omzzz6rKVOm6IADDtCZZ56pBx98UJL01FNP6Y9//KMkadasWVq2bJk2bdokKTG6Njg4GPr1Nm3apNdff12dnZ0THnvve9+r1157bSyu73znOxodHdVbb72lxx57TGeccUao19i9e7cefvhhrV27VgMDAxoYGNDjjz8+bgr23/7t33T88ceHjjsI068AAGBMT3ePpMTauqGRIXW0dahvVt/Y8SjdeuutuuaaazR9+nS1trZq8eLFkhJr7S6//HJNmzZNH/jAB9TRkRg1PO6443Tbbbdp9uzZ2r17t1paWnTXXXdlTdLSnX322WpubtaOHTv0xS9+UYceeuiEc84//3w9++yzOvLII3XiiSfqqquu0imnnCJJuvbaa3XCCSdknWrN9Pzzz+uwww7Tn//5n48dO/PMM/XKK6/ojTfe0JQpU7RhwwbNnDkz7KcpkKVXkDSimTNn+sqVK6sdBgAAkVm3bp2OPfbYaodRV9544w1deeWV+tGPfhTp6zz22GN66aWX9I//+I8THsv2dTOzVe6eNQNk+hU1Le69kgAAtenP/uzPdN1115W1+XA2O3fu1M0331yWazH9ipqV6pWUKq1P9UqSVJFpAACIE3eXmVU7jLpy6aWXRv4aH/3oR7MeL2YmlZE61KyoeiUBQKPZd999NTw8XFSigMpzdw0PD2vfffct6HmM1KFmRdUrCQAazeGHH66NGzfqP//zP6sdCkLad999dfjhhxf0HJI61KyOtg4NjkwsTS+1VxIANJqWlhZNnTq12mEgYky/omZVs1cSAAD1hqQONaunu0eL5ixSZ1unTKbOtk4tmrOIIgkAALKgTx196gAAQJ2gTx0AAEDMkdQBAADEAEkdAABADJDUAQAAxABJHQAAQAyQ1AEAAMQASR0AAEAMkNQBAADEAEkdAABADJDUAQAAxABJHQAAQAyQ1AEAAMQASR0AAEAMkNQBAADEAEkdAABADJDUAQAAxABJHQAAQAyQ1AEAAMQASR0AAEAMkNQBAADEAEkdAABADJDUAQAAxABJHQAAQAyQ1AEAAMQASR0AAEAMkNQBAADEAEkdAABADJDUAQAAxABJHQAAQAyQ1AEAAMRA7JI6M9vPzFaa2V9WOxYAAIBKqfmkzszuNbNNZvbrjOPnmtmrZrbBzD6X9tDfSnq4slECAABUV80ndZLul3Ru+gEza5Z0l6QPSzpO0uVmdpyZfUjSK5I2VTpIAACAatqr2gHk4+7PmVlXxuFTJG1w99ckycz+RdIFkiZL2k+JRG+7mT3p7rsrGC4AAEBV1HxSF+AwSb9Lu79R0l+4+yckycyukrQ5KKEzs/mS5ktSR0dHtJECAABUQD1MvxbM3e939+/leHyRu89095nvete7KhkaAABAJOo1qfu9pHen3T88eQwAAKAh1WtS9wtJR5nZVDPbW9Jlkr5b5ZgAAACqpuaTOjNbKumnkt5rZhvN7GPuvlPSJyT9UNI6SQ+7+8vVjBMAAKCaar5Qwt0vDzj+pKQnKxwOAABATar5kToAAADkR1IHAAAQAyR1AAAAMUBSBwAAEAMkdQAAADFAUgcAABADJHVARPrX9qtrYZeaFjSpa2GX+tf2VzskAECM1XyfOqAe9a/t1/wn5mt0x6gkaXBkUPOfmC9J6unuqWZoAICYYqQOiEDv8t6xhC5ldMeoepf3VikiAEDckdQBERgaGSroOAAApSKpAyLQ0dZR0HEAAEpFUgdEoG9Wn1pbWscda21pVd+svipFBACIO5I6IAI93T1aNGeROts6ZTJ1tnVq0ZxFFEkAACJj7l7tGKpq5syZvnLlymqHAQAAkJeZrXL3mdkeY6QOAAAgBho2qTOzOWa2aGRkpNqhAAAAlKxhkzp3f8Ld57e1tVU7FAAAgJI1bFIHAAAQJyR1AAAAMUBSBwAAEAMkdQAAADFAUgcAABADJHUAAAAxQFIHAAAQA3mTOjM7wsz2Sf7/LDP7lJkdGH1oAAAACCvMSN0jknaZ2ZGSFkl6t6QHI40KQFn0r+1X18IuNS1oUtfCLvWv7a92SACAiOwV4pzd7r7TzC6S9FV3/6qZ/TLqwACUpn9tv+Y/MV+jO0YlSYMjg5r/xHxJUk93TzVDAwBEIMxI3Q4zu1zSPEnfSx5riS4kAOXQu7x3LKFLGd0xqt7lvVWKCAAQpTBJ3dWS3i+pz91fN7Opkh6INiwApRoaGSroOACgvoWZfv2Qu38qdSeZ2L0dYUwAyqCjrUODI4NZjwMA4ifMSN28LMeuKnMcAMqsb1afWltaxx1rbWlV36y+KkUEAIhS4Ehdch3dX0uaambfTXtof0lbog4MQGlSxRC9y3s1NDKkjrYO9c3qo0gCAGIq1/TrTyS9IWmKpH9KO/6mpDVRBgWgPHq6e0jiAKBBBCZ17j4oaVCJIgkAAADUsDA7SlxsZr8xsxEz22pmb5rZ1koEBwAAgHDCVL9+SdIcd18XdTAAAAAoTpjq1/8goQMAAKhtYUbqVprZQ5K+I+lPqYPu/mhkUQEAAKAgYZK6AySNSpqddswlkdQBAADUiLxJnbtfXYlAANSv/rX99MMDgCoLU/16tJktN7NfJ+9PN7O/iz60aJnZHDNbNDIyUu1QgLrWv7Zf85+Yr8GRQblcgyODmv/EfPWv7a92aADQUMIUSnxD0ucl7ZAkd18j6bIog6oEd3/C3ee3tbVVOxSgrvUu79XojtFxx0Z3jKp3eW+VIgKAxhQmqWt19xczju2MIhgA9WdoZKig4wCAaIRJ6jab2RFKFEfIzP5Kie3DAEAdbR0FHQcARCNMUvdxSV+XdIyZ/V7SZyTdEGlUAOpG36w+tba0jjvW2tKqvll9VYoIABpTmKRu0N3/m6R3STrG3U9P7gsLAOrp7tGiOYvU2dYpk6mzrVOL5iwqufq1f22/uhZ2qWlBk7oWdhVdeFGu6wBArTN3z32C2ZCkH0h6SNK/er4n1JmZM2f6ypUrqx0GgDSpitr0AozWltaCk8VyXQcAaoWZrXL3mdkeCzNSd4ykp5WYhn3dzP7ZzE4vZ4AAkK5cFbVU5gJoJHmTOncfdfeH3f1iSScoscPEjyOPDEDDKldFLZW5ABpJmJE6mdkHzexuSask7Svp0kijAtDQylVRS2UugEYSZkeJASUqXp+X1O3ul7r7I1EHBqBxlauitlqVuRRnAKiGvHu/Spru7lsjjwQAklJFDKXuJ1uu6xQiszgjtW1aejwAEIUw1a9HS/qapEPd/Xgzmy7pI+5+WyUCjBrVrwBy6V/bX1BS2LWwS4MjE7s+dbZ1auAzAxFGCqARlFr9Gsu9XwEgn9So2+DIoFw+NuqWazq12OKMQqZsmd4FkA17vwJAgGJaohRTnFFI8lhMogmgMbD3KwAEKGbUrZjijEKSR3rvAQjC3q8AEKCYUbditk0rJHmk9x6AIHmrX939NUn/zcz2k9Tk7m9GHxYAVF/frL6s24zla4nS091TUKVrR1tH1uKKbMljIecCaCyhmg9Lkru/RUIHoJEUM+pWjEKmbKvVew9A7cvb0iTuaGkCoBYU0jql0DYrAOIjV0sTkjqSOgAAUCdyJXV519SZ2cVZDo9IWuvum0oNDgAQDUb0gMYSZk3dxyR9U1JP8vYNSX8r6QUzmxthbACAIoXtZ0cjYyA+wiR1e0k61t0vcfdLJB2nRM+6v1AiuQMA1Jgw/exoZAzES5ik7t3u/h9p9zclj21RcuswAEBtCdPPjkbGQLzkXVMn6Vkz+56kbyfvX5I8tp+k/4osMgBA0cL0s6ORMRAvYXeUuF/SjOTt/0j6eLJv3dkRxgYAKFKYfnbF7JgBoHblTeo8YZm7/9/J2zKPQR8UM5tjZotGRkaqHQoAlF2Yxsk0MgbiJW+fumRLk/8p6RBJlry5ux8QfXjRo08dgEYWpu0JrVGA2lFS82Ez2yBpjruviyK4aiOpA4BgqQrZzP1vo9guDUB+uZK6MGvq/iOuCR0AIDcqZIH6Eab6daWZPSTpO5L+lDro7o9GFhUAoCZQIQvUjzBJ3QGSRiXNTjvmkkjqACDmwrRGAVAbwlS/Xp3ldk0lggMAVFe1KmSDti9jWzMgWGBSZ2afTf77VTP7SuatciECAKolTGuUXIlWMUlY0PZlN33/JrY1A3IIrH41sznu/oSZzcv2uLsvjjSyCqH6FQCKl6s6VlJRlbNdC7uyTvk2W7N2+a4JxzvbOjXwmYESPgqgfuSqfg1cU+fuTyT/O+ru305/zMw+Wsb4AAB1Kl91bNBjPd09gf3vgoowsiV0EkUbQEqYQonPa8++r7mOAQAaTDHVsUMjQxNG+FJTqVJwcUbQSB1FG0BCrjV1Hzazr0o6LGM93f2SdlYsQgBAzcq1f2yux3KN8AUVZ8w/aT7bmgE55Kp+/XdJKyW9LWlV2u27kv579KEBAGpdrurYXI/lGuELKs64+/y78xZtAI0szDZhn3X3L2Uc+7S73xlpZBVCoQQAlCbX3rBBjwUVQ1D0AORW6t6vL7n7iRnHfunuJ5QxxqohqQOAyiv3nrJByWOuhBOoR0VVv5rZ5ZL+WtJUM/tu2kP7S9pS3hABAI0klViVI+EKKrp4YegFLf7V4qzFGCR2iKNcfeo6JU2V9AVJn0t76E1Ja9w9FsUSjNQBQH2jrx0aSa6RusBCCXcfdPdn3f39kgYktbj7jyWtkzQpkkgBAChQoX3tBkcG2WoMsZR371czu07SMklfTx46XNJ3ogwKAICwglqnNFtz1uMmY6sxxFLepE7SxyWdJmmrJLn7byQdEmVQAACEVUhfO5PJNX7ZUfoOGEA9C5PU/cnd30ndMbO9JOUumQUAoEIK6WuXmdClZJvC7V/bzzQt6kqYliZfkvRfkq6U9ElJN0l6xd1j8WcNhRIA0DjC9scrd8sVoFyKKpRI8zlJ/ylpraTrJT0p6e/KFx4AAJWRa5eLdLm2MQNqVWCfuhR33y3pG8kbAAB1K2x/vFzbmAG1Ksz061pNXEM3osS+sLe5+3BEsVUE068AgMydJ7a9s03D2yf+emuf1K7Je09mhwpUTVE7SqR5StIuSQ8m718mqVXSHyTdL2lOGWIEAKAqsu1I0dLUor2b99Y7u8bqBNXS1KI333lzLNljhwrUmmL3fn3J3U80s7Xu3h1phBFjpA4AGltQ8UTmqFzQ6B07VKCSSh2pazazU9z9xeTFTpaU6ugYi63CAACNK2id3JbtW7T5s5vH7jctyF5bmNqhgilZVFuY6tePSfqWmb1uZq9L+paka81sPyX2ha05Znasmd1jZsvM7MZqxwMAqF1BO1JkHg86jx0qUCtyJnVm1izpjOQU6wxJM9x9urv/wt3fcveH8zz/wGRitd7M1pnZ+4sJ0szuNbNNZvbrLI+da2avmtkGM/ucJLn7One/QdKlSuyGAQBAVmHbnGQ7jx0qUEtyJnXuvkvS5cn/j7j7SIHXv1PSD9z9GEnvk7Qu/UEzO8TM9s84dmSW69wv6dzMg8mk8y5JH5Z0nKTLzey45GMfkfR9JfrqAQCQVdCOFJlTqNnOC9qhYnBkkJ0oUHFhCiX+t6QWSQ9Jeit13N1fyvO8NkmrJb3HA17EzD4q6QZJ57n7n8zsOkkXu/uHs5zbJel77n582rH3S7rV3f978v7nk7F9Ie2c77v7+UFxUigBAChWUJFFOnaiQDmVuqPEDEnTJP2DpH9K3u4I8bypSuxEcZ+Z/dLMvplchzfG3b8t6YeSHjKzHknXSPpoiGunHCbpd2n3N0o6zMzOMrOvmNnXFTBSZ2ZzzGzRyEihg48AACRkm5LNxHQsKiXMjhJnl3DtEyV90t1/bmZ3KrHl2N9nXP9LZvYvkr4m6Qh331bk66Vf81lJz+Y55wlJT8ycOfO6Ul8PANCYMneoCJqOZScKVELekTozazOzL5vZyuTtn5JTq/lslLTR3X+evL9MiSQv8/pnSDpe0mOSbikgdkn6vaR3p90/PHkMAICK6Onu0cBnBrT7lt3qbOvMek5Q5SxQTmGmX++V9KYSlaSXStoq6b58T3L3P0j6nZm9N3lolqRX0s8xsxMkLZJ0gaSrJbWb2W2ho5d+IekoM5tqZnsrsdvFdwt4PgAAZRO2kjZd/9p+dS3sorACJQvTfPgId78k7f4CM1sd8vqflNSfTLheUyJxS9cq6VJ3/60kmdmVkq7KvIiZLZV0lqQpZrZR0i3u/i1332lmn1BiXV6zpHvd/eWQsQEAUFaZ07EdbR0676jz1Lu8V3MfnTt2/8nfPKmhkSEdPOlgvfnOm2PbkbH1GEoRpvr1p5L+X3dfkbx/mqQ73L2onnO1hupXAEBUMveVDYutxxCk1OrXGyTdZWYDZjYg6Z8lXV/G+AAAiKXe5b0FJ3QSfe5QnMDpVzP7tLvfKWmyu7/PzA6QJHffWrHoAACoY6VUvaZvOyYxHYv8co3Upda/fVVKJHMkdAAAhFeOqlf63CGsXEndOjP7jaT3mtmatNtaM1tTqQABAKhXYZoTtzS1qH1Su0wWeA7TsQgjMKlz98slnSFpg6Q5abe/TP4LAAByyLZf7I0zbxx3/74L79Pmz27O2edOGj8dmy2xozUK8la/xh3VrwCAWhG2WjazOjbb89hzNp5KrX4FAAAVkDmyF2RwZHDcaFy2KlvW4jUekjoAAGpImG3HJI2big2qsk0dZ2q2MQQmdWb2QPLfT1cuHAAAkJKv0GJ0x6jmPTZPrgfPViUAACAASURBVOxLqVyuKV+aomsev0aDI4N51+WhvuUaqTvJzP5c0jVmdpCZHZx+q1SAAAA0qvTp2CC7fFfOawxvHx7bhiwlNTXLCF68BBZKmNmnJN0o6T2Sfi+Nm9x3d39P9OFFj0IJAEA96FrYpcGRwbJe02TjRvkorqh9RRVKuPtX3P1YSfe6+3vcfWraLRYJHQAA9SJMz7tCZU7bUlxR3/IWSrj7jWb2PjP7RPI2vRKBAQCAPTIrY5utOet5QcfDyqysRf3Im9Qlp2H7JR2SvPWb2SejDgwAAIyXXhm7+KLFE0buWltaNf+k+ROOp3atCCtfMQVr8WpTmJYm10r6C3f/H+7+PySdKum6aMMCAAC5ZNutYtGcRbr7/LsnHE/tWpGr4CJT0FRsqtEx1bS1J++OEma2VtLJ7v528v6+kn7h7t0ViC9yFEoAABpFtp0nMoslMnW2dWpoZEgdbR3qm9Wn3uW9WQs2Mne5QDRK3VHiPkk/N7NbzexWST+T9K0yxgcAACog2+jeAxc/EDiCZ7IJI3JBFbhBDZBROXvlO8Hdv2xmz0o6PXnoanf/ZaRRAQCASPR092RtWRJmBG90x6iarTlrb7yOto7yB4uC5E3qJMndX5L0UsSxAACAKkgleamp1aDETUo0O25taR2XALa2tOq8o85T18KucVO19LurLPZ+BQAA6unuGeuFl2uXilRBRvoU7rz3zdPiXy2meKLKQo3UAQCA+Otd3jtuBC5Tao1d7/LecSNxXQu7JjwvtS9t+rUZxYtWzupXM2uW9LS7n125kCqL6lcAABKaFjQFVsJm21Js3vvm6cnfPFnw9mWpa3W2dZLgFajo6ld33yVpt5m1RRIZAACoGUHFDs3WnLVo4p6V9xS1H23qWoMjg7r6O1drypem0Mi4DMKsqdsmaa2ZfcvMvpK6RR0YAACorGz7y+ZaY5erv11YO3bv0PD2YdbilUGYpO5RSX8v6TlJq9JuAAAgRoJ2qShkJ4pSpdbikdgVLkyfusVmNklSh7u/WoGYAABAlZTSxy7X8ULs8l2a/8T8sXiK0b+2v+GKM/KO1JnZHEmrJf0geX+GmX036sAAAEBtyDaCd8PMG7JO1d4w84axkT2TFf2aQXvPhtGo+9OG2ft1laRzJD3r7ickj/3a3Y+vQHyRo/oVAIDi5BsNy/a4tKe9ycGTDtab77ypd3a9E/gaSy5eUvAIW9fCrtjuT5ur+jVMUvczdz/VzH6ZltStcffpEcRacSR1AABUT//afs17bF5gMUYx7U+CWrOYTLtv2V1yzNVUdEuTpJfN7K8lNZvZUWb2VUk/KWuEAACgofSv7VfXwi7NfXSuDtz3QO3dvHfW89Lbn1zx6BVq/odm2QJT18Iu3fT9m9S1sGtCO5Sg1ixx3582zEhdq6ReSbMlmaQfSvpHd387+vCix0gdAACVlVrzll540dLUoh27d5R87SZr0m7fnbVZ8qI5i+q+WKKkkTp3H3X3XkmzJJ3t7r1xSOjMbI6ZLRoZGal2KAAANJRs25Ht2L1DzdZc8rV3e2J61eVjhRqp1iw93T1jI4RxbHYcZqTuZEn3Sto/eWhE0jXuHotedYzUAQBQWbm2I2ttac25/2yh2ie1a/NnN0vKPkJYbyN4pa6p+5akm9y9y927JH1c0n1ljA8AADSQoLVtmc2OS2mJkjK8fXhsNC7bCGEprVNqTZikbpe7P5+64+4rJO2MLiQAABBnQduRpapbBz4zIL/F9cDFD6h9UnvJr5dK2oZGhrI+Xsz+tbUoMKkzsxPN7ERJPzazr5vZWWb2QTO7W9KzFYsQAADEStB2ZJlToD3dPdr82c1acvGSkrYqSyVzuapfp3xpyrh1dvW49i5wTZ2ZPZPjee7u50QTUmWxpg4AgPoR1Fi42ZoDe92lmg73r+3X3Efn5t3GrKWpRWY2rilyMf3yolDUmjp3PzvHLRYJHQAAqC9BU7eLL1qsJRcvCZzWlRIjf2H2pd2xe8eEXS7S++XV6pZje+U7wcwOlHSlpK708939U9GFBQAAMFFqhCzX9mS5Huts6yx5DV2quKLWKmbDtDT5iaSfSVoraWxvDXdfHG1olcH0KwAA8ZW5/+x5R52nxb9aXHLblGptOZZr+jXvSJ2kfd39/ylzTAAAAJHK7Es3ODKor638mvZr2U/tk9q1ZfsWHTzpYL35zpsTplvzqcUtx8K0NHnAzK4zsz8zs4NTt8gjAwAAKEG2vnSS9NaOt7R953Y9cPED2vzZzbr3gnsLum5rS6vOO+o8TfnSFNkCky0wTfnSlKqvswuT1L0j6X9J+qmkVckb85UAAKCmBfWlk8Y3He7p7gndMqV9UrvmvW+evvnSNzW8fXjs+PD2YV3z+DVVTezCrKl7TdIp7r65MiFVFmvqAACIp6D2J+k62zo1NDKkgycdPC5JC5KrdUrqegOfGSg01NBK3SZsg6TybcIGAABQAdnan6QzmQZHBuXyUAmdpJwJnZR7dDBqYQol3pK0OtmM+E+pg7Q0AQAAtSzVcuTTT316QtKWaiacKeh4WNUsoAgzUvcdSX2SfqI9a+pWRRkUAABAOWRuNZbaliwocUvtGiElErxC7N2891ij42rIu6Yu7lhTBwBA4wlab5e+Ji69x12TNeWcem2f1K47P3xn5A2JS1pTZ2avm9lrmbfyhwkAAFAZQduNpY+09XT3aOAzA9p9y24tvmhx4Mhd+6R2Td57suY+OlddC7uqVgEbZvp1pqSTk7czJH1F0pIogwIAAIhST3ePFs1ZNG5KdtGcRYEjbbn2jR3ePjxWcFHNvWGLmn5NDv2dFEE8Fcf0KwAACCNMi5SUqFqblDr9emLabaaZ3aBwVbMAAACxka9FSrpqtDYJk5z9U9r/d0oakHRpJNEAAADUqNTUbKp4oqOtQ9ve2Za1x101WpvkTerc/exKBAIAAFDrerp7xq2761/br/lPzB+3x2xmwUWl5E3qzGwfSZdI6ko/393/IbqwAAAAal+20bu+WX2RtzbJJsz06+OSRpRoOPynPOcCAAA0lMzRu2oJk9Qd7u7nRh4JAAAAihamT91PzKw78khiqL9f6uqSmpoS//ZXpxchAABoAGFG6k6XdJWZva7E9KtJcnefHmlkda6/X5o/XxpNrpscHEzcl6Se6o/QAgCAmMnbfNjMOrMdd/dw3fdqXFTNh7u6Eolcps5OaWCg7C8HAAAaQK7mw2FamsQieau0oYCeg0HHAQAAShFmTR2K0BHQczDoOAAAQClI6iLS1ye1Zuwk0tqaOA4AAFBuJHUR6emRFi1KrKEzS/y7aBFFEgAAIBphql9RpJ4ekjgAAFAZjNQBAADEAEkdahoNnAEACIfpV9QsGjgDABAeI3WoWb29exK6lNHRxHEAADAeSR1qFg2cUQtYAgCgXpDUoWbRwBnVlloCMDgoue9ZAkBiB6AWkdShZtHAGdXGEgAA9YSkDjWLBs6otnpfAsDUMdBYYpnUmdmxZnaPmS0zsxurHQ+K19MjDQxIu3cn/q2nhI5fqPWv2CUAtfC1r5Wp41r4XACNIvKkzsyazeyXZva9Eq5xr5ltMrNfZ3nsXDN71cw2mNnnJMnd17n7DZIulXRa8dEDxamVX6goTTFLAGrla18LU8e18rkAGkUlRuo+LWldtgfM7BAz2z/j2JFZTr1f0rlZnt8s6S5JH5Z0nKTLzey45GMfkfR9SU+WEjxQjFr4hYrSFbMEoNivfSEjWmHOrYWpY34OgMqKNKkzs8MlnS/pmwGnfFDSd8xsn+T510n6auZJ7v6cpC1Znn+KpA3u/pq7vyPpXyRdkHzOd939w5LqaMIOcVELv1BRHoUuASjma1/IiFbYc8NOHUc5PVrKzwHTtkDhoh6pWyjps5J2Z3vQ3b8t6YeSHjKzHknXSPpoAdc/TNLv0u5vlHSYmZ1lZl8xs68rYKTOzOaY2aKRkZECXg4Ih3YsjauYr30hI1phzw0zdRz19GgpaxKZtgUKF1lSZ2Z/KWmTu6/KdZ67f0nS25K+Jukj7r6t1Nd292fd/VPufr273xVwzhPuPr+tra3UlwMmqJV2LIx2VF4xX/tCRrTCnhtm6jhsgpjv+yjo8WJ/Dpi2BYrk7pHcJH1BiZGzAUl/kDQqaUmW886Q9GtJiyX9c47rdUn6dcax90v6Ydr9z0v6fCFxnnTSSQ5EYckS985Od7PEv0uWVP71W1vdE2MdiVtra+XjaESFfu07O8d/nVK3zs7Szs3HLPu1zMZ/LLm+j8I8XujPQZi4gEYlaaUH5UpBD5TzJuksSd/LcvwEJYoojlBi1HCppNsCrpEtqdtL0muSpkraW9KvJE0rJDaSOsRVOX/5V0O1k+JKKiQBL2eyHuZ7JN85UXyf1fv3LhClXEldtfvUtUq61N1/6+67JV0paTDzJDNbKumnkt5rZhvN7GOS5O47JX1CiXV56yQ97O4vVyx6oIZVslij3NO85VxTVQ9T0IVU2ZazKXeY6dF830dRfJ/VyvIFoO4EZXuNcmOkDnFVqdGOKKZ5yxU7U9D55RsRrcZIXZi4Sj0fqFeq9vRrLd9I6hBXlUpoovilXq41VUzjla7UNXW1EGO288uZAJJQopJI6kjq0KAq8csmikXt5UrGWHBfHvm+j6qd1BTy/VLo+sV8H1ctJLVoLLmSOks83rhmzpzpK1eurHYYQN3q6kqsecvU2Zlo1luM1Jq69LYWra2Frx2LIjZUV39/orXJ0FCi311fnzR3biKdymQmPfDA+PO3bZOGhyee294uTZ6857zzzpMWL87/Pcj3GCrNzFa5+8ysj5HUkdQBpShXApbtupm/vAu9XlSxoTqCvp6TJgUnatu3T+x5F4ZZ9kQxM1lragpOKHdnbbsPlCZXUlft6lcAda6c1ZiZ1y1ke65KxobqCGpKLGWvlk1/vFBB4x2ZVb3sHoNaQlIHoGTlSMCiUsuxNbpC280EtUnZsiV78r4l247hJcpM1mi/glpCUgcAqLhiehHmGhXLlrwHnd/ePj4BbG/Pfp7Z+PvZkjVGg1FLSOqiVA9dTwGgCorZ37XQUbGg8++8c3wCeOed2c+74YbwDaHTryfx1o/q2KvaAcRW5ore1J+hEn/CAWh4xexEkXrrDFtAE/b8Qq+bC2/9qCaqX6OqfqXOHQACxfUtspiPqxyV3mgcVL9WQyU33gSAOlOvBQb5VtUU+tZfzn2OAZK6qFDnDgCB6rHAIEwCFvQW39SUPREsZm0hEITp16imX+l6CgCxEmZqNdtbf6b0XwU0L0ahmH6thnr8MxQAECjM1GrmW39z88Tz00fiCh3ZKxbNGBoDI3VsEwYACKGYIoh8I3GFjuzlkl5wcfDBiWNbtiT+/+ab0jvvFH5N1B5G6gAAKFExxR35RuJ6e6V588KP7GVKjcCZSXPn7lnvNzycuKX+n57Q5bsm6hdJHQAAIRSzqiZbIihJu3btKbZYvDhx3u7dwevosk39phduSMH71QbJVZHLVG19IqkDACCkQvcSLtcau2zHs1XOFiLbNaNqsUKiWBkkdQAARCg9Ecw3EpdtZK+lRdq2bWJCVErb06Bp41wtVopNzOjFVzkkdQAAVEi+kbjMkb329sS/qfVxg4OJtXNmieQqrJaWPddKnzZOT9SmTMleCCLtScTSE7NUHJkJXmby9+lP04uvUqh+pfoVAFAhhbYwDaq4DWKWSLra2xP3t2yZuPVYqkp2cHDP+fk0NyfWAQZJfQxS/mre9FjpxVe4XNWve1U6GAAAGlUqsQq712uYKdbm5kRyFGbf2MykMkxC19qaP0kbHU1U8R54YPh1fmywVH6M1DFSBwCoUWFG6goZ8Sp05E+SlizZM7JXLvTJKx596gAAqENBLVHSFTLiVWhxRXv7+KnaYrW3528Fk6sQg+rZcJh+BQCgRqVP12ZbA5ev+XGmjo7wI24tLYmdKIaHE/fd97x+2LV4qRjvvHPimr65cxPxnHee9PDDe15H2lOYkZI+ZZz+GCN94zH9yvQrAKBOpG8FFmYNXbbnZxYyBBVXbNs2PtFK6exMvO68ebmLJ6TEer/Fi8cndGELKVKvJRW+PVuc5Zp+JakjqQMANJCwiWE59q3NXO9XzJq+XMyKS27rGWvqUL9YSAEAZRV2V4xCeuoFybxGKQ2Ts0n1zLviikSfvUb/FUFSh9pFG3IAqJpsRRqZa/hSCeKSJfnPlaJtYzI8nPgVcdNNjTsWQFKH2pVrvxoAQKQyd7cIqlrNda40PsE677z81bylGB2V7rkn91hAnCeAWFPHmrralW9BB4D4KrUiAFUXtHvGvHmJhC9fkUU5pYo7Pv3picUf9dYzjzV1qE/5FnQAiCeWXsRC0GTLk08mKmKzTdfeeGP+kbz29sJH+1LfQtmqeUdHE2vyUqN29TySR1KH2hVmQQeA+GHpRSwEFUUMDQVP19599/jii8yGx6med4sW7WnBEkZzc/42KoOD0tVXS9dcU79/TzD9yvRrbWMKBmg8LL2IhaD2JYX0l8v8FXDeeYmRvvT7lZrKTU3hVvtXENOvqF9ha+8BVE+556tYehEL5ZhsSf8V0NeXmLZNH0VbvDh/QrfffomRulLVw6gdSR0AoHhRrH8rJhuo54VQMVVI9WwYQbPy+RK2t94q30je6Gii0KNWv82YfmX6FQCKV445tmwKWXoRVGZZTyWNyCtoVl5KfLnDbj2WS0tLIgF9551w51fj24zpVwBA8XKNguVaDV+KQpZe5CqsYAQvNoJm31MjgOkjgoUUUaS0t0v33Sfde2/uXTLS1Vr9DkkdACBYvunVWlj/FpRApmKt11JGjJNrVj7zb4AtW3Jfq719fBK4ZIm0eXPisdQA8X77hYurnHvZloqkDgAQLF97kVpoPRSUQGbrY1FrQysIrZA1ern+pki1RUlPAqXE3rFXXLHnb4C33gof2+TJiedXe0CYpA4AECzf9Gq5V8MXIyixDFodPzjIlGydCjsrn+1bQkqM0GV+e6YGo7M1Jg7rrbcSz6/2gDBJHQDEVZj1ZPnOCTO9mus3bSXWtAUllkELo8yYko25bN8SqSnWzEQw22B0qao2IOzuDX076aSTHABiZ8kS99ZW90Tqkri1tiaOl/ucUmKIUrbXNxt/P3Xr7Ax/zc7OxHU6Oyv3sSAyQd8Spd7MoolX0koPyGkYqQOAOAqz1VaYc0qZXs13/ahH8bLFHtQTI9s0c2Z8N91E4UUMRVXTU5Ve2UHZXqPcGKkDUHa1MJoTNPyQPnwQ5pyoYih2FK/Uz21nZ7iRuihG+VCTsn2pJff99nNvb8/9pQ+6RTkgLUbqAKBCothhIdtr5BvhCrMWLup2JLmuH2aUMFM5Prdhq3WzxVfIKF8u9M6rKUHr77ZtS6zBc5ceeGDi40uWjO+H15TMqKpRKzQmKNtrlBsjdQDKKuxIULHCjnBFvV6u1FiLGSUs1+c222hf5rFChmUKef1qrzNE2VTrS6kcI3VVT6qqfSOpQ2RqYQoOlRf1lGYhiU2Y78Gov0+Drl9MghbV57aQqdbM44X+Fg/zcfPeURei/vstSK6kjr1f2fsVUWAvysYV1V6oKUEbYJolWorUi2J+RqL63AZd12z857q1NbGb+5NPjt+TVgq/T22+rx/vHXWjWj+KDbf3q5kda2b3mNkyM7ux2vGgARWzXgjxEPUOC7WwLVc5FFNVG9XnNmhNnPvE+O6+e+JWBIWs88v39eO9o27U5I9i0BBeqTdJ+0p6UdKvJL0saUEJ17pX0iZJv87y2LmSXpW0QdLnMh5rkrQk17WZfkUkop6CQ+UUMxUW5fRZo6/JiuJzW8o8WqHPzff1472jbjTUmjpJJmly8v8tkn4u6dSMcw6RtH/GsSOzXOtMSSdmJnWSmiX9VtJ7JO2dTCCPSz72EUlPSfrrXHGS1CES1VpsgfKq5Lt2IckKa67Kq5SvczFJWK6vH+8ddaUaP4pVSerGvYjUKuklSX+RcfyjkpZL2id5/zpJTwVcoytLUvd+ST9Mu/95SZ/POOf7uWIjqUMkGn00JS4q9QuW75fqK/a3c9D3SHNzcb/pS93Bg2Q/9qqW1CVH0lZL2ibpfwac81lJ35HUI+mnqdG9LOdlS+r+StI30+7PlfTPks6S9BVJX5f08YDrzZG06Mgjjyz7Jxxwd95gS1ULn79KTYUxOlO/gjrXpt9aWhJdbMN+L2d+7994Y7gqZv4waAi1MFJ3oKRnJB0f8Pi/SNoq6V05rhE6qSskNkbqgDIodwJWK7+gKpVssY6qvqV//zc3507wsn0v5/r5ydVupdQWMahLuZK6ilS/uvt/JZO6czMfM7MzJB0v6TFJtxR46d9Lenfa/cOTxwBUShQ7KNRKBWDUlawpNVlGh9B6evZUxIbpZZG5/22un59cO1uknxtUwVvobheoa5EldWb2LjM7MPn/SZI+JGl9xjknSFok6QJJV0tqN7PbCniZX0g6ysymmtneki6T9N1yxA8gpCgSsFr5BVXKZvaFqFTyiOiFTcRT38v5fn7yfc+nzi3HHwZsX1b3ohyp+zNJz5jZGiWSrx+5+/cyzmmVdKm7/9bdd0u6UtKEDpBmtlSJ9XbvNbONZvYxSXL3nZI+IemHktZJetjdX47sIwLqTSXepKNIwGpp5Cp9FGZgIJoGsJVKHhG9bAl6Nqnv5Xw/P2G+54eG8v9hkO+9oBJ7FiN6QfOyjXJjTR1iq1Lr0qJYy1Mra+qAYqSvkWtvd9977+Dv5Xw/P2EKMdLPzbY2L8zPE2vy6oaqXShRyzeSOsRWvbfjqIXqV6AcCi2ECCqkSC+SKORnLei9IL3YohrFOvyMF4WkjqQOjaiSb9K8OQPFi7rxdNB7QXpi2N6e/bH29tyvV+zPPqPxRcuV1Fni8cY1c+ZMX7lyZbXDAMov6o3l60V/f/jN1oE4CnovSNfeLm3fPrFoI1Nr6571nql1eOnPaW2V5s2Tnnwy988c709FM7NV7j4z22MVaWkCoAqoqGTxNxpXemHEtm3S3nvnPn/LlkSy1t6e+7z0ytygyt177sn/M1crFe4xQ1IHxBUVlbXT7w6ISraq1sw/ZoaHE//mStg6OhLvDZMn53/NVOIVlIBlzgCOjkpXXDG+6raWKtxjhOlXpl+B+GpqmvgLRkokuWGaxAK1LGj6c9KkRCKXqbMzMVKf7TmpP/iCfmayXWvbtuyvk0vqtaTccSAQ068AGhOjAYizoJHooERraCj/CH7Yn43BQWnr1onTuma5n5caKWcmIRIkdQDii3WFiLNC15+lErb0htp9fYkkKzV9e955E39mghK1HTuk/fcfn5jdcEP+5supuMvR2JtdMMYhqQMQX4wGIM6CRtXa28P9MZOtkGjx4kT1avrPzAMPBCd2W7aMT8zuvnvPz1yQpqbwyVeupC1fIVTmc2+6qbAEsB4TxqBeJ41yo08dAKAu5er1FqZ/XCENyotpZp5rN4wwPemCnt/ePr4hc1BD5Xw7cbS2ut9448TP05Il2fv2ZYu5Cj06RZ+6YBRKAADqVil9GAspJMpWlGGWeH6qAEOaGIuUGPnbtWvi62T2pMv8WHIVYrS25u6p19yc/TXzaWlJfFzvvJP98fb2RIXw0JB08MHSm29OPLe9XbrzzshmBHIVSpDUkdQBABpRoQ2AU0nX4OCehC4lWzKUqmadOzd/8pgtacwnKHHLjK0aIqzkpfoVAACMV2ghUaqwobNzYtK0Y8fEEatUpWvQ2j/3PWvd5s0rLKGTEgldturbaid0UtX6YZLUAQDQiIotJCqk6nZoKHvymDI4KH3ta8VNlUqJRLIpmco0N9dGQpdShd0xSOoAAGhUxbQVKaTPY2qninwVsUHa2/NvXZaaws2VGDY3SzfemPi3UqrQD5OkDgAAhJdt5K2lZeJUaPpUbip5zNecOPP5d94pbd4sLVlSUsg68MDEnrTFjggWqkr9MEnqAABAeNmmbe+7T7r33vxTuYWMXqXWpfX3J65TzEhfSmr/2yilEtYq9sOk+pXqVwAAKqOYKtdc+8WWQzmKKyJuY5KO6lcAAFB92Ub5brxxz/1sa94y94vNt8YurEK2Nstn8uSa2KmGkTpG6gAAqA1hGyJnNio+7zzpyScT1bSp/nWdncENjIMaH6c/v5AGxtkaNkeEkToAAFD7gtbcZR7PrNq9++7Ev+7Szp2JfwcGElOiYXrxpa6X/vzFiyc+N6jQowqVrtmQ1AEAgNpQaEPkfIrtxRf03GxTtVWqdM2G6VemXwEAqB2l7GdbCVWOj71fcyCpAwAA9YI1dQAAADFHUgcAABADJHUAAAAxQFIHAAAQAyR1AAAAMUBSBwAAEAMkdQAAADFAUgcAABADJHUAAAAxQFIHAAAQAyR1AAAAMUBSBwAAEAPm7tWOoarM7D8lDRb4tDZJIwWcP0XS5gJfA+MV+jmvJbUQe6ViiOJ1ynXNUq9TzPOLeQ7vF6WphZ+3YtVK7LxflHadYp8b9nmd7v6urI+4O7cCb5IWFXj+ymrHXO+3Qj/ntXSrhdgrFUMUr1Oua5Z6nWKeX+RzeL+o4teZ2Hm/KPU6xT63HLEz/VqcJ6odQAOq5895LcReqRiieJ1yXbPU6xTz/Fr42jeaev6c10rsvF+Udp1in1ty7A0//VoJZrbS3WdWOw4AtY/3CwDFYqSuMhZVOwAAdYP3CwBFYaQOAAAgBhipAwAAiAGSOgAAgBggqQMAAIgBkjoAAIAYIKmrAjPbz8wWm9k3zKyn2vEAqE1m9h4z+5aZLat2LABqH0ldmZjZvWa2ycx+nXH8XDN71cw2mNnnkocvlrTM3a+T9JGKBwugagp5r3D319z9Y9WJFEC9Iakrn/slnZt+wMyaJd0l6cOSjpN0uZkdJ+lwSb9LnrargjECqL77Ff69AgBCI6krE3d/TtKWjMOnSNqQI/g8aAAAAspJREFU/Gv7HUn/IukCSRuVSOwkvgZAQynwvQIAQiOhiNZh2jMiJyWSucMkPSrpEjP7mmpnrz8A1ZP1vcLM2s3sHkknmNnnqxMagHqxV7UDaETu/pakq6sdB4Da5u7Dkm6odhwA6gMjddH6vaR3p90/PHkMANLxXgGgZCR10fqFpKPMbKqZ7S3pMknfrXJMAGoP7xUASkZSVyZmtlTSTyW918w2mtnH3H2npE9I+qGkdZIedveXqxkngOrivQJAVMzdqx0DAAAASsRIHQAAQAyQ1AEAAMQASR0AAEAMkNQBAADEAEkdAABADJDUAQAAxABJHQBUgZmdZWbfq3YcAOKDpA4AACAGSOoAIAczu8LMXjSz1Wb2dTNrNrNtZva/zexlM1tuZu9KnjvDzH5mZmvM7DEzOyh5/Egze9rMfmVmL5nZEcnLTzazZWa23sz6zcyq9oECqHskdQAQwMyOlfR/STrN3WdI2iWpR9J+kla6+zRJP5Z0S/Ip/0fS37r7dElr0473S7rL3d8n6QOS3kgeP0HSZyQdJ+k9kk6L/IMCEFt7VTsAAKhhsySdJOkXyUG0SZI2Sdot6aHkOUskPWpmbZIOdPcfJ48vlvRtM9tf0mHu/pgkufvbkpS83ovuvjF5f7WkLkkrov+wAMQRSR0ABDNJi9398+MOmv19xnnFbqL9p7T/7xLvyQBKwPQrAARbLumvzOwQSTKzg82sU4n3zr9KnvPXkla4+4ikP5rZGcnjcyX92N3flLTRzC5MXmMfM2ut6EcBoCHwVyEABHD3V8zs7yT9f2bWJGmHpI9LekvSKcnHNimx7k6S5km6J5m0vSbp6uTxuZK+bmb/kLzGRyv4YQBoEOZe7KwBADQmM9vm7pOrHQcApGP6FQAAIAYYqQMAAIgBRuoAAABigKQOAAAgBkjqAAAAYoCkDgAAIAZI6gAAAGLg/wfK1vmy9d40owAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQbqVX8TJ4mE",
        "outputId": "d52c68cd-4f7c-4e9c-8bdf-9496ff576eeb"
      },
      "source": [
        "total_A = torch.sum(torch.flatten(cumulative_forgetting_A))\n",
        "total_B = torch.sum(torch.flatten(cumulative_forgetting_B))\n",
        "print(total_A)\n",
        "print(total_B)\n",
        "print(total_A/total_B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(281272.)\n",
            "tensor(228934.)\n",
            "tensor(1.2286)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkSiS8LTKVFM"
      },
      "source": [
        "trace the history of a specific example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc9EURgwMtuA",
        "outputId": "12fbbd5c-cefc-4df5-dc0e-dd38db4b15ed"
      },
      "source": [
        "torch.sum(forget_matrix_A[79],"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peLoMPSxNY2E",
        "outputId": "629a34b6-cb99-4c9e-b1f0-3ac0e8a2479d"
      },
      "source": [
        "cumulative_forgotten_ex_B.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([79])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCn0xvWzNnOY",
        "outputId": "ce6ed99b-2ed8-4b15-f1a4-ba7bd93fcbc9"
      },
      "source": [
        "len(eplist[1:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "j3Qnd_d8KW7F",
        "outputId": "228cc9f7-5b9b-4620-c21f-d2bf607854cd"
      },
      "source": [
        "cumulative_forgotten_ex_A = torch.flatten(torch.cumsum(forget_matrix_A[0:80,45,0],0))\n",
        "cumulative_forgotten_ex_B = torch.flatten(torch.cumsum(forget_matrix_B[0:80,45,0],0))\n",
        "\n",
        "plt.scatter(eplist[1:], cumulative_forgotten_ex_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], cumulative_forgotten_ex_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"times forgotten\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDgBXjDkOakt"
      },
      "source": [
        "check training accuracy between the different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAm0WR1MOb3p",
        "outputId": "00883373-784f-46f7-bbad-0041d98778ec"
      },
      "source": [
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = False\n",
        ")\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "test_accuracy_A = list()\n",
        "test_accuracy_B = list()\n",
        "\n",
        "batch_counter = 0\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l_A = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "        \n",
        "    test_accuracy_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    test_accuracy_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    if batch_counter%10 == 0:\n",
        "        print(f\"Test accuracy A: {torch.tensor(test_accuracy_A).mean():.2f} \\n\")\n",
        "        print(f\"Test accuracy B: {torch.tensor(test_accuracy_B).mean():.2f} \\n\")\n",
        "    \n",
        "    batch_counter+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy A: 0.84 \n",
            "\n",
            "Test accuracy B: 0.78 \n",
            "\n",
            "Test accuracy A: 0.79 \n",
            "\n",
            "Test accuracy B: 0.76 \n",
            "\n",
            "Test accuracy A: 0.78 \n",
            "\n",
            "Test accuracy B: 0.75 \n",
            "\n",
            "Test accuracy A: 0.78 \n",
            "\n",
            "Test accuracy B: 0.76 \n",
            "\n",
            "Test accuracy A: 0.79 \n",
            "\n",
            "Test accuracy B: 0.76 \n",
            "\n",
            "Test accuracy A: 0.79 \n",
            "\n",
            "Test accuracy B: 0.76 \n",
            "\n",
            "Test accuracy A: 0.79 \n",
            "\n",
            "Test accuracy B: 0.76 \n",
            "\n",
            "Test accuracy A: 0.79 \n",
            "\n",
            "Test accuracy B: 0.76 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZqYPcH3-O4P"
      },
      "source": [
        "# Distillation when we Rewind to Minimal Loss Logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZYsGSa1yoRZ"
      },
      "source": [
        "Tried a bunch of things here where model B is distilled from A using lowest loss logits from A. Summary:\n",
        "\n",
        "\n",
        "*   Cross entropy with targets still seems best loss function (??) for model B even though we are throwing out lots of information by argmax'ing over model A's logits. I tried wiht binary cross entropy with logits, as well as KLDivLoss.\n",
        "*   Model B does better if rewinding only happens after forgetting events (always rewinding to minimal loss logits seems to do worse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "67aaf2c1caac413a8563c80c115b51ab",
            "4bca9075b8fe4702961b9a2ce17044c1",
            "1606e7fd011d423488301a56b50d0e6e",
            "28d7896ed93f484295e70670860d0cfb",
            "840a97d46b904a5382d957e4a8615677",
            "31a9ab0de46b40dca391239c199b007d",
            "82c90101d9434ddc927dbcc61b4c4fee",
            "248e1d244111472e9c380185b26f2599",
            "10da766354044f04a89ced9d5f7da342",
            "14874a6390e04e96ad9472b1434a9c31",
            "c97d5e712f6442efafab7a159a91893a"
          ]
        },
        "id": "1AbnvZ3k-R3M",
        "outputId": "b02e42da-3de8-443a-8edf-9c84b72c7637"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.CrossEntropyLoss() #nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 740.00 KiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67aaf2c1caac413a8563c80c115b51ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTTKgxZXTfPu"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60QSIJOlOOza",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "4436f6a6-6627-44f9-99aa-fa943e2d9f78"
      },
      "source": [
        "losses_tracker = torch.zeros(3, len(train_set),128)\n",
        "btracker = 0\n",
        "l_A_previous = torch.zeros(3, len(train_set), 128, 10)\n",
        "\n",
        "\n",
        "import random \n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in train_set:\n",
        "        x,y = next(iter(train_set)) #load the batch\n",
        "        x=x.cuda()\n",
        "        y=y.cuda()\n",
        "\n",
        "        l_A = model_A(x) #compute logits\n",
        "        l_A_prime = l_A.detach()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            l_A_previous[epoch, btracker, 0:len(l_A)] = l_A.detach() #set previous logits equal to current logits to start with\n",
        "\n",
        "        loss_batch = loss_A(l_A, y.cuda()) #this computes the loss for the full batch\n",
        "\n",
        "        for k in range(len(l_A)):\n",
        "                losses_tracker[epoch, btracker, k] = loss_A(l_A[k: k+1].detach(),y[k: k+1].detach()) + random.random()\n",
        "            #losses_tracker[1,btracker,k] = loss_A(l_A[k: k+1].detach(),y[k: k+1].detach()) + random.random()\n",
        "            \n",
        "\n",
        "        sorted, idx = torch.sort(losses_tracker,0)\n",
        "        #modify current logits\n",
        "\n",
        "        for k in range(len(l_A_prime)):\n",
        "            for ep in range(len(3)):\n",
        "                for id in idx[ep, btracker]:\n",
        "                    if id==0:\n",
        "                        l_A_prime[k] = l_A_previous[ep, btracker, k]\n",
        "        \n",
        "        targets=torch.argmax(l_A_prime, 1)\n",
        "        l_B = model_B(l_A_prime, targets.cuda())\n",
        "        \n",
        "        btracker+=1\n",
        "    btracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ce186e0f729d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_A_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbtracker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14R1DU9oz5P7",
        "outputId": "ecb60324-6ddf-4340-98d5-c4a3796262f7"
      },
      "source": [
        "l_A_previous[0, 10, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.6937, -1.2217, -1.2623,  0.6020, -1.4037,  1.5340,  0.0858,  0.1984,\n",
              "         0.2974, -0.0422])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMBo-8cDzutP",
        "outputId": "926a708e-88c7-427c-e8b0-9574a2c7ed42"
      },
      "source": [
        "idx[0,10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 2, 2, 2, 0, 1, 1, 0, 1, 2, 0, 2, 2, 1,\n",
              "        2, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "        0, 0, 1, 0, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 1, 0, 0, 2, 1, 0, 2, 0, 2,\n",
              "        1, 1, 2, 0, 0, 2, 2, 1, 2, 2, 1, 2, 0, 2, 0, 1, 1, 0, 1, 2, 0, 2, 1, 0,\n",
              "        0, 2, 2, 0, 1, 2, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 1, 2, 0,\n",
              "        0, 2, 0, 0, 0, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "NdcaJgJopHog",
        "outputId": "f6d1e763-ad95-4b34-9a81-9cfbc67f7cfb"
      },
      "source": [
        "tmpbatch = 0\n",
        "ep\n",
        "[i for i in range(len(idx[tmpbatch])) if idx[tmpbatch, i] == 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-68d0938b06e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmpbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmpbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-68d0938b06e2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmpbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmpbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPA4FLyQw4pU"
      },
      "source": [
        "tmpbatch = 0\n",
        "for k in range(len(l_A_prime)):\n",
        "    for ep in range(len(3)):\n",
        "        for id in idx[ep, tmpbatch]:\n",
        "            if id==0:\n",
        "                l_A_prime[k] = l_A_previous[ep, tmpbatch, k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkcSd5W7u35D",
        "outputId": "8f5c58b9-6edb-4cdd-d37d-8289a08f90f7"
      },
      "source": [
        "idx[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 2, 0, 0, 2, 1, 0, 2, 1, 2, 2, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1, 1,\n",
              "        1, 2, 2, 2, 0, 0, 2, 0, 1, 2, 1, 2, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "        1, 1, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 1, 2, 2, 0, 2, 0, 1, 2, 1, 1, 1, 2,\n",
              "        2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 2, 1, 0, 1, 2, 2, 1, 0, 1, 1,\n",
              "        0, 2, 2, 1, 0, 1, 2, 0, 0, 1, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0,\n",
              "        0, 1, 0, 1, 1, 2, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCQZ9HVRx2pQ",
        "outputId": "48bb2954-3a4c-4fa3-cd89-260dc0f2f6a1"
      },
      "source": [
        "l_A[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.4280, -1.2980, -0.9409,  0.4770, -1.0139,  1.4150, -0.4508,  0.3935,\n",
              "         0.6328,  0.1319], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jww5t0u7mMRQ",
        "outputId": "3778f329-ad50-41cd-a22c-6f9ab95c73a4"
      },
      "source": [
        "print(idx[2,0])\n",
        "print(idx[0,0])\n",
        "print(idx[1,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 2, 1, 2, 1, 0, 1, 1, 1, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 0, 1, 2, 0,\n",
            "        2, 1, 1, 2, 0, 0, 2, 0, 1, 1, 0, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 1, 1, 0,\n",
            "        1, 1, 2, 1, 2, 1, 2, 2, 1, 0, 2, 2, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0,\n",
            "        2, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 2, 0, 0, 2, 1, 0, 1, 1,\n",
            "        1, 0, 2, 1, 2, 2, 0, 2, 0, 0, 1, 0, 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 2, 2,\n",
            "        1, 1, 0, 2, 0, 1, 0, 1])\n",
            "tensor([2, 1, 1, 0, 1, 0, 1, 2, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 2, 2, 0, 1,\n",
            "        0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0, 2, 2,\n",
            "        2, 2, 0, 2, 0, 0, 1, 0, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 2, 1, 0, 2,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1,\n",
            "        2, 2, 1, 0, 2, 2, 2, 2])\n",
            "tensor([1, 2, 0, 2, 0, 2, 2, 0, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 2,\n",
            "        1, 2, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 0, 2, 0, 1,\n",
            "        0, 0, 1, 0, 1, 2, 0, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 2, 0, 2,\n",
            "        0, 2, 0, 1, 0, 1, 2, 0, 0, 2, 2, 1, 1, 0, 2, 0, 1, 2, 2, 0, 0, 2, 2, 0,\n",
            "        2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 2, 0, 2, 0, 0, 2, 0, 1, 2, 2, 0, 1, 0,\n",
            "        0, 0, 2, 1, 1, 0, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6IpJyIOXLKT",
        "outputId": "01a5c6c9-d01f-4f52-be0d-f1b68ae5de23"
      },
      "source": [
        "l_A[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0431,  0.6108, -0.1781,  0.2541, -0.8880, -0.2022,  0.9137,  0.3573,\n",
              "         0.0535,  0.5141], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5-XrXCgXFqm",
        "outputId": "90410200-8d11-4569-865f-fdd93fbde5c5"
      },
      "source": [
        "l_A.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkrh3VxZk1EZ",
        "outputId": "364a21b4-396b-45c2-943a-c0251b6aeebe"
      },
      "source": [
        "idx.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 391, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWlRdjP_OitJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2391d469-31d3-4b45-ca9b-b4e73d6c5b04"
      },
      "source": [
        "print(idx.size())\n",
        "print(losses_tracker.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 391, 128])\n",
            "torch.Size([3, 391, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx7k_CeQCz1C",
        "outputId": "d1669707-d4cb-4eac-dd6e-4ea04a5e1819"
      },
      "source": [
        "l_A_previous.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 391, 128, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgM8IMXHdy7q",
        "outputId": "f981ffcd-1a6f-4b47-e4b9-19ad2f51d77e"
      },
      "source": [
        "l_A.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngtUtscvWAtW",
        "outputId": "730f6685-94ea-4b00-b28b-9617e54def67"
      },
      "source": [
        "len(l_A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IBZ-HWqOl89",
        "outputId": "136cd053-7c67-4871-b6a3-f52c7e2a67c1"
      },
      "source": [
        "print(idx[0,0])\n",
        "print(idx[1,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQaFFDm7OcKY"
      },
      "source": [
        "losses_tracker.size()\n",
        "losses_sorted, idx = torch.sort(losses_tracker[0:2], 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2YOLSlrESgd",
        "outputId": "cf1acc4b-6e4d-45cd-c021-fec5179d0613"
      },
      "source": [
        "l_A"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7503, -1.2346, -0.1005,  ...,  0.6142, -2.3046, -0.8745],\n",
              "        [ 0.4934,  1.9253, -0.8022,  ...,  0.3051,  1.2250,  1.7169],\n",
              "        [ 0.8733,  0.6798, -0.1956,  ...,  0.6543,  0.7015,  1.2924],\n",
              "        ...,\n",
              "        [-0.8566, -0.3387, -0.3000,  ...,  0.9149, -1.2799,  0.4120],\n",
              "        [ 2.7103,  1.0410, -0.2073,  ..., -0.2489,  3.2315,  1.6605],\n",
              "        [ 1.3707,  1.3316, -0.5506,  ...,  0.3252,  1.8202,  1.6500]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNJiFUsr43zx",
        "outputId": "3f8e47d4-7d39-4b86-df80-67a7e36073f4"
      },
      "source": [
        "l_A[idx[0,0]].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTceHlGE3Bk",
        "outputId": "96772569-5792-470d-f6c3-764b4a16e2b8"
      },
      "source": [
        "print(loss_B(l_B, targets.cuda()))\n",
        "softmaxfunc=nn.Softmax(dim=1)\n",
        "#testloss=nn.functional.binary_cross_entropy_with_logits()#nn.KLDivLoss()\n",
        "newloss = nn.functional.binary_cross_entropy_with_logits(softmaxfunc(l_B), softmaxfunc(l_A_prime.cuda()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5021, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "8X_eMPgeZDdf",
        "outputId": "f2389dca-c332-4275-b3d8-67056b925104"
      },
      "source": [
        "newloss.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3c92e8144409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ddWJm0TijC"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGRxTfQ4Arim"
      },
      "source": [
        "nb_epochs = 40\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "model_B_on_A_acc = torch.zeros(len(train_set),128)\n",
        "model_B_on_A_acc_tilde = torch.zeros(len(train_set),128)\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt image classification\n",
        "forget_matrix_B_on_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt A logits\n",
        "\n",
        "losses_tracker = torch.zeros(nb_epochs, len(train_set),128)\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "acc_A_global = list()\n",
        "acc_B_global = list()\n",
        "\n",
        "#initialize previous logits\n",
        "model_A.eval()\n",
        "#first_x, first_y = next(iter(train_set))\n",
        "\n",
        "l_A_previous = torch.zeros(nb_epochs, len(train_set), 128, 10)\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "batch_tracker = 0\n",
        "\n",
        "forget_thres = 4\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        l_A = model_A(x)\n",
        "        l_A_prime = l_A.clone().detach()\n",
        "        l_B = model_B(x.clone().detach())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            l_A_previous[epoch, batch_tracker, 0:len(l_A)] = l_A.clone().detach() #set previous logits equal to current logits to start with\n",
        "    \n",
        "        #batch_size_current = len(batch[1])\n",
        "        #output_vectors[epoch, batch_tracker, 0:batch_size_current] = l_A\n",
        "\n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "\n",
        "        if epoch >1:\n",
        "            l_A_previous_softmax = softmaxfunc(l_A_previous[epoch-1, batch_tracker])\n",
        "\n",
        "        for j in range(len(l_A)):\n",
        "            with torch.no_grad():\n",
        "                losses_tracker[epoch, batch_tracker, j] = loss_A(l_A[j: j+1].detach(),y[j: j+1].cuda().detach())\n",
        "        \n",
        "        #scan for lowest loss in previous epochs:\n",
        "        if epoch > 1:\n",
        "            losses_sorted, idx = torch.sort(losses_tracker[0:epoch+1], 0)\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "                \n",
        "                #if we forgot the example, build new logits which involve previous ones\n",
        "                #we will use this to rewind the logits to lowest loss ones\n",
        "                #try:\n",
        "            if epoch > 1 and torch.sum(forget_matrix_A,0)[batch_tracker, k] > forget_thres:\n",
        "                #and then change logits to lowest loss ones\n",
        "                for ep in range(epoch):\n",
        "                    for id in idx[ep, batch_tracker]:\n",
        "                        if id==0:\n",
        "                            l_A_prime[k] = l_A_previous[ep, batch_tracker, k]\n",
        "                            #print(f\"Rewound logits from {l_A_prime[k]} to {l_A_previous[ep, batch_tracker, k]} (loss from {losses_tracker[epoch, batch_tracker, k]} to {losses_tracker[ep, batch_tracker, k]}) \\n\")\n",
        "                    \n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "    \n",
        "        #now do the same with model B\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y[k]:\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]:\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "            #also measure how much it forgets w.r.t. the task it is trained for\n",
        "            #(how much it forgets model A's logits)\n",
        "            if torch.argmax(l_B_softmax[k])==torch.argmax(l_A_softmax[k]):\n",
        "                model_B_on_A_acc[batch_tracker, k] = 0\n",
        "            else:\n",
        "                model_B_on_A_acc[batch_tracker, k] = 1\n",
        "            \n",
        "            if model_B_on_A_acc[batch_tracker, k] < model_B_on_A_acc_tilde[batch_tracker, k]:\n",
        "                forget_matrix_B_on_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            model_B_on_A_acc_tilde[batch_tracker, k] = model_B_on_A_acc[batch_tracker, k]\n",
        "\n",
        "\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y.cuda())\n",
        "        #print(J_A)\n",
        "\n",
        "        #we need to first see if the example got forgotten\n",
        "        #if so, we need to feed in l_A_previous to model_B's loss function\n",
        "        #otherwise, use the current logits\n",
        "\n",
        "        #model B's loss is to compute cross entropy between its output and the output of model A\n",
        "        \n",
        "        targets=torch.argmax(l_A_prime, 1)\n",
        "        #J_B = loss_B(l_B, l_A_prime)\n",
        "        #l_A_prime_softmax = softmaxfunc(l_A_prime)\n",
        "        #J_B = loss_B(l_B, l_A_prime_softmax.cuda())\n",
        "        J_B = loss_B(l_B, targets.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "        #reset previous logits\n",
        "        #l_A_previous[] = l_A\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "\n",
        "        #J_A.backward(retain_graph=True)\n",
        "        J_B.backward() \n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        accuracies_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "    acc_B_global.append(torch.tensor(accuracies_B).mean())\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f} \\n\")\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuGARr_sngPT",
        "outputId": "595da26e-068f-49af-c358-27b04f30a325"
      },
      "source": [
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = False\n",
        ")\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "test_accuracy_A = list()\n",
        "test_accuracy_B = list()\n",
        "\n",
        "batch_counter = 0\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l_A = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "        \n",
        "    test_accuracy_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    test_accuracy_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    if batch_counter%10 == 0:\n",
        "        print(f\"Test accuracy A: {torch.tensor(test_accuracy_A).mean():.2f} \\n\")\n",
        "        print(f\"Test accuracy B: {torch.tensor(test_accuracy_B).mean():.2f} \\n\")\n",
        "    \n",
        "    batch_counter+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test accuracy A: 0.70 \n",
            "\n",
            "Test accuracy B: 0.59 \n",
            "\n",
            "Test accuracy A: 0.70 \n",
            "\n",
            "Test accuracy B: 0.62 \n",
            "\n",
            "Test accuracy A: 0.69 \n",
            "\n",
            "Test accuracy B: 0.63 \n",
            "\n",
            "Test accuracy A: 0.68 \n",
            "\n",
            "Test accuracy B: 0.62 \n",
            "\n",
            "Test accuracy A: 0.69 \n",
            "\n",
            "Test accuracy B: 0.64 \n",
            "\n",
            "Test accuracy A: 0.69 \n",
            "\n",
            "Test accuracy B: 0.63 \n",
            "\n",
            "Test accuracy A: 0.69 \n",
            "\n",
            "Test accuracy B: 0.63 \n",
            "\n",
            "Test accuracy A: 0.68 \n",
            "\n",
            "Test accuracy B: 0.63 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OClSHFu_eWw"
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbAmeOLHizm6",
        "outputId": "801c032e-41c4-45b6-ed2c-c6d6f2aa9d79"
      },
      "source": [
        "acc_A_global[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.3206),\n",
              " tensor(0.3656),\n",
              " tensor(0.3954),\n",
              " tensor(0.4189),\n",
              " tensor(0.4455),\n",
              " tensor(0.4656),\n",
              " tensor(0.4853),\n",
              " tensor(0.4971),\n",
              " tensor(0.5124),\n",
              " tensor(0.5254),\n",
              " tensor(0.5366),\n",
              " tensor(0.5498)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62jrsnb2GQVo"
      },
      "source": [
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], acc_A_global[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], acc_B_global[1:], c =\"red\", label = \"model B\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"train accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "nNVEZ4fRyDZc",
        "outputId": "23d9339a-9dbf-4126-eccd-a14f0083c473"
      },
      "source": [
        "import numpy as np\n",
        "cumulative_forgetting_A = torch.sum(forget_matrix_A, 0)\n",
        "cumulative_forgetting_B = torch.sum(forget_matrix_B, 0)\n",
        "\n",
        "forgetlen_A = len(torch.flatten(cumulative_forgetting_A))\n",
        "forgetlen_B = len(torch.flatten(cumulative_forgetting_B))\n",
        "\n",
        "hist_A = plt.hist(torch.flatten(cumulative_forgetting_A), alpha=0.5, label = \"Model A\", weights = np.ones(forgetlen_A)/forgetlen_A)\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B), alpha=0.5, label = \"Model B\", weights = np.ones(forgetlen_B)/forgetlen_B)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "WiSRtUGcyP4i",
        "outputId": "bf578495-1a22-4a1f-c46d-6a76fe13e941"
      },
      "source": [
        "forget_fcn_epoch_A = list()\n",
        "forget_fcn_epoch_B = list()\n",
        "forget_fcn_epoch_B_on_A=list()\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    forget_fcn_epoch_A.append(torch.sum(torch.flatten(forget_matrix_A[i]),0))\n",
        "    forget_fcn_epoch_B.append(torch.sum(torch.flatten(forget_matrix_B[i]),0))\n",
        "    forget_fcn_epoch_B_on_A.append(torch.sum(torch.flatten(forget_matrix_B_on_A[i]),0))\n",
        "\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGpCAYAAAA9Rhr4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZhddXno/e+dEIlREJX4cojJRMQLiYnBDikUsAiHiNUWqBxP6KjRlsb26NE+eqpy0qeAh6Dto9Vq7UtarajDiwIKRdCDFAS0FSYaghB4jJBAONTE8BIxiia5zx97De6EycyembX2XrP393Nd69p7/dbaa997sbO55/camYkkSZLqZ1qnA5AkSdLITNQkSZJqykRNkiSppkzUJEmSaspETZIkqab263QAVTj44IOzr6+v02FIkiSNac2aNT/OzNkjHevKRK2vr4+hoaFOhyFJkjSmiNi0r2M2fUqSJNWUiZokSVJNmahJkiTVVFf2UZMkSdX75S9/yebNm/n5z3/e6VCmhJkzZzJnzhxmzJjR8mtM1CRJ0oRs3ryZAw44gL6+PiKi0+HUWmaybds2Nm/ezPz581t+nU2fkiRpQn7+85/z3Oc+1yStBRHBc5/73HHXPpqoSZKkCTNJa91E7pWJmiRJUk2ZqEmSJNGYMP/HP/7xhM9Zu3YtEcHXvva10mIyUZMkSSrBxRdfzHHHHcfFF19c2jVN1CRJUlsMDkJfH0yb1ngcHJzc9TZu3Mjhhx/OW9/6Vl760pcyMDDAN77xDY499lgOO+wwbr31VgAefvhhTjvtNBYtWsTRRx/NunXrANi2bRtLly5lwYIFnHXWWWTmk9f+whe+wJIlS1i8eDFvf/vb2bVr16ixZCZf+tKX+OxnP8t1111X2pQlJmqSJKlyg4OwYgVs2gSZjccVKyafrG3YsIH3vve93H333dx9991cdNFF3HLLLXzkIx/hggsuAOCcc87hyCOPZN26dVxwwQW85S1vAeC8887juOOO48477+T000/n/vvvB2D9+vVceumlfOtb32Lt2rVMnz6dwTEC/fa3v838+fM59NBDOeGEE/jqV786uQ9WMFGTJEmVW7kSduzYs2zHjkb5ZMyfP5+FCxcybdo0FixYwEknnUREsHDhQjZu3AjALbfcwpvf/GYATjzxRLZt28b27du56aabeNOb3gTA6173Op797GcDcP3117NmzRqOOuooFi9ezPXXX8+99947ahwXX3wxy5YtA2DZsmWlNX864e0EDA42vlj33w9z58KqVTAw0OmoJEmqr6KyquXyVu2///5PPp82bdqT+9OmTWPnzp0TumZmsnz5cj70oQ+1dP6uXbu4/PLLufLKK1m1atWTk9v+5Cc/4YADDphQDMOsURunqqpuJUnqZnPnjq+8TMcff/yTTZc33ngjBx98MAceeCCvetWruOiiiwC49tpreeSRRwA46aSTuOyyy9iyZQvQ6OO2adOmfV7/+uuvZ9GiRTzwwANs3LiRTZs28YY3vIEvf/nLk47dRG2cqqq6lSSpm61aBbNm7Vk2a1ajvGrnnnsua9asYdGiRXzgAx/gwgsvBBp912666SYWLFjAFVdcwdwiazziiCM4//zzWbp0KYsWLeLkk0/moYce2uf1L774Yk4//fQ9yt7whjeU0vwZzSMcukV/f38ODQ1Vcu1p0xo1aXuLgN27K3lLSZJqaf369bzsZS9r+Xy7Do18zyJiTWb2j3S+fdTGae7cRnPnSOWSJGnfBgZ6LzGbLJs+x6mTVbeSJKm3mKiN08AArF4N8+Y1mjvnzWvs+xeCJEkqm02fE2DVrSRJagdr1CRJkmrKRE2SJKmmTNQkSZKAvr4+fvzjH0/onL6+PhYuXMjixYtZuHAhV155ZSkx2UdNkiSpBDfccAMHH3ww99xzD0uXLuXUU0+d9DWtUZMkSe0xOAh9fY3Z4/v6Jr3+4saNGzn88MN561vfyktf+lIGBgb4xje+wbHHHsthhx3GrbfeCjSWgDrttNNYtGgRRx99NOvWrQNg27ZtLF26lAULFnDWWWfRvAjAF77wBZYsWcLixYt5+9vfzq5du1qOa/v27U8u8D5ZlSdqETE9Ir4XEVcX+zdHxNpi+z8R8ZWi/ISIeKzp2J83XeOUiLgnIjZExAeqjlmSJJWsosWyN2zYwHvf+17uvvtu7r77bi666CJuueUWPvKRj3DBBRcAjaWijjzySNatW8cFF1zAW97yFgDOO+88jjvuOO68805OP/107i9WiF+/fj2XXnop3/rWt1i7di3Tp09/cq3Q0bz61a/m5S9/Ob/5m7/J+eefP6nPNawdTZ/vBtYDBwJk5vHDByLicqC5EffmzHx984sjYjrwKeBkYDNwW0RclZl3VR24JEkqyWiLZU9izqv58+ezcOFCABYsWMBJJ51ERLBw4UI2btwIwC233MLll18OwIknnsi2bdvYvn07N910E1dccQUAr3vd656sBbv++utZs2YNRx11FAA/+9nPeN7znjdmLMNNnz/84Q856aSTOOGEE3jmM5854c8GFSdqETEHeB2wCnjPXscOBE4E3jbGZZYAGzLz3uJ1lwCnAiZqkiRNFUVtVcvlLdp///2ffD5t2rQn96dNm8bOnTsndM3MZPny5XzoQx+a0OsPPfRQnv/853PXXXexZMmSCV1jWNVNnx8H3geMtFz5acD1mbm9qeyYiLg9Iq6NiAVF2SHAA03nbC7K9hARKyJiKCKGtm7dWlL4kiSpFPtaFLsNi2Uff/zxTzZd3njjjRx88MEceOCBvOpVr+Kiiy4C4Nprr+WRRx4B4KSTTuKyyy5jy5YtQKOP26aRFvrehy1btnDfffcxb968ScdeWY1aRLwe2JKZayLihBFOORP4p6b97wLzMvPxiPgt4CvAYa2+X2auBlYD9Pf35xinS5Kkdlq1qtEnrbn5s02LZZ977rn8/u//PosWLWLWrFlceOGFQKPv2plnnsmCBQv4jd/4DeYWSeMRRxzB+eefz9KlS9m9ezczZszgU5/61JiJ16tf/WqmT5/OL3/5Sz784Q/z/Oc/f9KxR/MIhzJFxIeANwM7gZk0+qhdkZlvioiDgXuAQzLz5/t4/Uagn0aydm5mvqYoPxsgM/dZH9nf359DQ0MlfhpJkrS39evX87KXvaz1FwwONvqk3X9/oyZt1aqeW5NxpHsWEWsys3+k8yurUcvMs4GziwBOAP5HZr6pOHwGcHVzkhYRLwB+lJkZEUtoNMtuAx4FDouI+cCDwDLg96qKW5IkVcTFssetUxPeLgM+vFfZGcAfR8RO4GfAsmxU9+2MiHcCXwemA5/JzDvbGq0kSVIHtCVRy8wbgRub9k8Y4Zy/Af5mH6+/BrimmugkSdJEZSYR0ekwpoSJdDdzZQJJkjQhM2fOZNu2bRNKQHpNZrJt2zZmzpw5rte51qckSZqQOXPmsHnzZpwWqzUzZ85kzpw543qNiZokSZqQGTNmMH/+/E6H0dVs+pQkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqqnKE7WImB4R34uIq4v9z0bEfRGxttgWF+UREZ+IiA0RsS4iXtl0jeUR8YNiW151zJIkSXWwXxve493AeuDAprI/zczL9jrvtcBhxfbrwN8Bvx4RzwHOAfqBBNZExFWZ+UjlkUuSJHVQpTVqETEHeB3wTy2cfirwuWz4d+CgiHgh8Brgusx8uEjOrgNOqSxoSZKkmqi66fPjwPuA3XuVryqaNz8WEfsXZYcADzSds7ko21f5HiJiRUQMRcTQ1q1bS/sAkiRJnVJZohYRrwe2ZOaavQ6dDRwOHAU8B3h/Ge+Xmaszsz8z+2fPnl3GJSVJkjqqyhq1Y4HfiYiNwCXAiRHxhcx8qGjefAL4Z2BJcf6DwIuaXj+nKNtXuSRJUlerLFHLzLMzc05m9gHLgH/NzDcV/c6IiABOA75fvOQq4C3F6M+jgccy8yHg68DSiHh2RDwbWFqUSZIkdbV2jPrc22BEzAYCWAv8UVF+DfBbwAZgB/A2gMx8OCL+F3Bbcd4HM/Ph9oYsSZLUfpGZnY6hdP39/Tk0NNTpMCRJksYUEWsys3+kY65MIEmSVFMmapIkSTVloiZJklRTJmqSJEk1ZaImSZJUUyZqkiRJNWWiJkmSVFMmapIkSTVloiZJklRTJmqSJEk1ZaImSZJUUyZqkiRJNWWiJkmSVFMmapIkSTVloiZJklRTJmqSJEk1ZaLWQYOD0NcH06Y1HgcHOx2RJEmqk/06HUCvGhyEFStgx47G/qZNjX2AgYHOxSVJkurDGrUOWbnyV0nasB07GuWSJElgotYx998/vnJJktR7TNQmooTOZXPnjq9ckiT1HhO18RruXLZpE2T+qnPZOJO1Vatg1qw9y2bNapRLkiSBidr4ldS5bGAAVq+GefMgovG4erUDCSRJ0q9EZnY6htL19/fn0NBQNRefNq1Rk7a3CNi9u5r3lCRJXSsi1mRm/0jHrFEbLzuXSZKkNjFRGy87l0mSpDYxURsvO5dJkqQ2cWWCiRgYMDGTJEmVs0ZNkiSppkzUJEmSaspETZIkqaZM1CRJkmrKRE2SJKmmTNQkSZJqykRNkiSppkzUJEmSaspETZIkqaZM1CRJkmpqzEQtIg6NiP2L5ydExLsi4qDqQ5MkSeptrdSoXQ7sioiXAKuBFwEXtfoGETE9Ir4XEVcX+4MRcU9EfD8iPhMRM4ryEyLisYhYW2x/3nSNU4rXbIiID4zrE0qSJE1RrSRquzNzJ3A68MnM/FPgheN4j3cD65v2B4HDgYXA04Gzmo7dnJmLi+2D0Ej0gE8BrwWOAM6MiCPG8f6SJElTUiuJ2i8j4kxgOXB1UTajlYtHxBzgdcA/DZdl5jVZAG4F5oxxmSXAhsy8NzN/AVwCnNrK+0uSJE1lrSRqbwOOAVZl5n0RMR/4fIvX/zjwPmD33geKJs83A19rKj4mIm6PiGsjYkFRdgjwQNM5m4uyva+3IiKGImJo69atLYYnSZJUX60kaidn5rsy82KAzLwP+PlYL4qI1wNbMnPNPk75W+CmzLy52P8uMC8zXwF8EvhKC7E9KTNXZ2Z/ZvbPnj17PC+VJEmqpVYSteUjlL21hdcdC/xORGyk0Vx5YkR8ASAizgFmA+8ZPjkzt2fm48Xza4AZEXEw8CCNAQzD5hRlkiRJXW2/fR0o+qX9HjA/Iq5qOnQA8PBYF87Ms4Gzi2udAPyPzHxTRJwFvAY4KTOfbBKNiBcAP8rMjIglNJLIbcCjwGFFk+uDwLIiLkmSpK62z0QN+DbwEHAw8NGm8p8A6ybxnn8PbAL+LSIArihGeJ4B/HFE7AR+BiwrBhzsjIh3Al8HpgOfycw7J/H+kiRJU0I0cqHu0t/fn0NDQ50OQ5IkaUwRsSYz+0c61srKBL8bET8oJqPdHhE/iYjt5YcpSZKkZqM1fQ77S+C3M3P9mGdKkiSpNK2M+vyRSZokSVL7tVKjNhQRl9KY1+yJ4cLMvKKyqCRJktRSonYgsANY2lSWgImaJElShcZM1DLzbe0IRJIkSXtqZdTnSyPi+oj4frG/KCL+rPrQJEmSelsrgwn+kcYKA78EyMx1NFYHkCRJUoVaSdRmZeate5XtrCIYSZIk/UoridqPI+JQGgMIiIgzaCwtJUmSpAq1MurzHcBq4PCIeBC4DxioNCpJkiS1lKhtysz/HBHPAKZl5k+qDkqSJEmtNX3eFxGrgaOBxyuOR5IkSYVWErXDgW/QaAK9LyL+JiKOqzYsSVPR4CD09cG0aY3HwcFORyRJU9uYiVpm7sjML2bm7wJH0lip4JuVRyZpShkchBUrYNMmyGw8rlhhsiZJk9FKjRoR8ZsR8bfAGmAm8MZKo5JKZk1P9VauhB079izbsaNRLkmamFZWJtgI/AlwM7AwM9+YmZdXHZhUlm6u6alTAnr//eMrlySNLTJz9BMiDszM7W2KpxT9/f05NDTU6TBUE319jeRsb/PmwcaN7Y6mPMMJaHMt1qxZsHo1DHRgAp1uvc+SVLWIWJOZ/SMda6Xp8wWu9amprFtreurW1LhqVSNRbDZrVqN8vOpUUyhJneRan+p6c+eOr3yqqFsCOjDQqM2bNw8iGo8Tqd3r5qZqSRov1/pU1yuzpqdO6piADgw0mjl37248TqQJtm41hZLUSa71qa5XVk1P3XRrAlq3mkJJ6iTX+lRPGBiY+onZ3oY/z8qVjSRm7txGkjbVP+fcuSMPSpjqTdWSNBFjJmqZeS/gWp9SDXVjArpq1cijWad6TaEkTURLE94CZOZPTdIkVa1bm6olaSJaTtQkqV3KGJQATvMhaeozUZM6oG4JRN3iKYPTfEjqBq0sIfW7I2wnRcTz2hGgVCdlJDR1SyDqFk9ZypzmoxsTWUlTQytLSH0VOAa4oSg6gcbi7POBD2bm56sMcCJcQkpVKGvJprottVS3eMoybVoj8dxbRKNJtVV1W6pLUvcZbQmpVhK1rwNvycwfFfvPBz4HnAnclJkvLzneSTNRUxXKSmjKSiDKUrd4ylLWf69uTWQl1cdk1/p80XCSVthSlD1MsayU1AvKmoi1bisK1C2espQ1IbAT8ErqpFYStRsj4uqIWB4Ry4Eri7JnAI9WG55UH2UlNHVbUaBu8ZSlrGk+ujWRlTQ1tJKovQP4LLC42D4HvKOYV+3VFcYm1UpZCU3d5gmrWzxlKmOaj7ITWQcmSBqPMfuoTUX2UVNVBge7b8kmja2s/+51HJjgd1rqvMkOJvhd4C+A5wFRbJmZB5YdaFlM1CTVUd0GJtQxcZR60WQTtQ3Ab2fm+iqCq4KJmqQ6qtsI27oljlKvmuyozx9NpSRNkuqqbgMTHNEq1V8ridpQRFwaEWc2r05QeWSS1GXqNsK2bomjpKdqJVE7ENgBLAV+u9he3+obRMT0iPheRFxd7M+PiO9ExIYiAXxaUb5/sb+hON7XdI2zi/J7IuI1rX88SaqPuo2wrVviKOmpxkzUMvNtI2y/P473eDfQ3HT6F8DHMvMlwCPAHxTlfwA8UpR/rDiPiDgCWAYsAE4B/jYipo/j/SWpNsqYMgTKmeajbomjpKfa52CCiHhfZv5lRHwSeMpJmfmuMS8eMQe4EFgFvIdGbdxW4AWZuTMijgHOzczXFEtVnZuZ/xYR+wH/AcwGPlC834eKaz553r7e18EEkrqZozWl7jLaYIL9RnndcC3YZDKejwPvAw4o9p8LPJqZO4v9zcAhxfNDgAcAiiTuseL8Q4B/b7pm82skqeesXLlnkgaN/ZUrTdSkbrPPRC0z/6V4uiMzv9R8LCL+y1gXjojXA1syc01EnDCpKFsQESuAFQBz7QkrqYs5WlPqHa0MJji7xbK9HQv8TkRsBC4BTgT+GjioaNoEmAM8WDx/EHgRQHH8WcC25vIRXvOkzFydmf2Z2T979uwWwpOkqcnRmlLv2GeiFhGvLfqnHRIRn2jaPgvs3NfrhmXm2Zk5JzP7aAwG+NfMHABuAM4oThte5B3gqmKf4vi/ZqMD3VXAsmJU6HzgMODW8X5QSeoWjtaUesdofdT+D43+ab8DrGkq/wnw/0ziPd8PXBIR5wPfAz5dlH8a+HyxEsLDNJI7MvPOiPgicBeNBPEdmblrEu8vSVPacD801+iUul8rS0i9LzP/cq+yd2fmX1ca2SQ46lOS2svF3aWJm+wSUstGKHvrpCKSJHWN4elCNm1qrGW6aVNjfyJzu0na0z6bPiPiTOD3gPkRcVXToQNoNE1KkuR0IVKFRqtR+zbwUeDu4nF4ey/gMk41UsYM5ZI0UU4XIlVnn4laZm7KzBsz8xhgIzAjM79JYyLcp7cpPo3BJgdJneZ0IVJ1xuyjFhF/CFwG/ENRNAf4SpVBqXWjNTlIUjs4XYhUnVYGE7yDxuS12wEy8wfA86oMSq2zyUFSp7m4u1Sd0eZRG/ZEZv4iIoAnVw0YfU4Ptc3cuY3mzpHKJaldBgZMzKQqtFKj9s2I+J/A0yPiZOBLwL+M8Rq1iU0OkiR1r1YStQ8AW4E7gLcD1wB/VmVQal2ZTQ6OHpUkqV7GXJlgKnJlgvEbHj3aPDBh1qzO9jNxpnNJUi+Y1MoEEXFHRKzba7s5Ij4WEc8tP1x1Qt1GjzrtiKTJsIVA3aKVps9rga8CA8X2LzQWa/8P4LOVRaa2KnP0aBk/kHVLHCVNHf6hp27SSqL2nzPz7My8o9hWAr+ZmX8B9FUbntqlrAkry/qBdNoRSRNV5h961syp01pJ1KZHxJLhnYg4Cphe7O6sJCq1XVmjR8v6gXSmc0kTVdYfetbMqQ5aSdT+APh0RNwXEfcBnwbOiohnAB+qNDq1TVmjR8v6gXTaEUkTVdYfenbBUB2MmqhFxHTg+MxcCCwGFmfmosy8LTN/mplfbEuUaouBAdi4EXbvbjxOZIRlWT+QznQuaaLK+kPPLhiqg1ETtczcBZxZPH8sMx9rS1SassqsCSsjcZTUe8r6Q88uGKqDVpo+vxURfxMRx0fEK4e3yiPTlGRNmKQ6KOMPPbtgqA7GnPA2Im4YoTgz88RqQpo8J7yVJJXBibfVDqNNeOvKBJIkSR002ZUJnhURfxURQ8X20Yh4VvlhSpIkqVkrfdQ+A/wEeGOxbQf+ucqgJEm9yQlmpT3t18I5h2bmG5r2z4uItVUFJEnqTcMTzA7PXTY8wSzYL0y9q5UatZ9FxHHDOxFxLPCz6kKSJPUiJ5iVnqqVGrU/Aj7X1C/tEWB5dSFJknqRE8xKT7XPGrWIeHfx9JmZ+QpgEbAoM4/MzHVtiU6S1DOcYFZ6qtGaPt9WPH4SIDO3Z+b26kOSJPUiJ5iVnmq0ps/1EfED4D9FRHMNWtCY8HZRtaFJknrJ8IABJ5iVfmWfiVpmnhkRLwC+DvxO+0KSJPWqgQETM6nZWIuy/0dmviIzN+29tStASZLU/ZxDb2StjPqUJEmqjHPo7Vsr86hJkiRVxjn09m206Tk+Xzy+e1/nSJKk9qpbE2EZ8TiH3r6N1vT5axHxn4Dfj4jP0Rjt+aTMfLjSyCRJ0h7q1kRYVjxz5zZeO1J5r4vMHPlAxLuAPwZeDDzInolaZuaLqw9vYvr7+3NoaKjTYUiSVKq+vpETmnnzYOPGdkdTXjx7J3zQmENv9ere6KMWEWsys3+kY/ts+szMT2Tmy4DPZOaLM3N+01bbJE2SpG5VZhNhnZosBwYaSdm8eRDReOyVJG0sYw4myMw/johXRMQ7i82JbiVJGoey+pWVtczWcA3Wpk2Q+asmy/HGVeayXwMDjVq43bsbjyZpDWMmakUT6CDwvGIbjIj/XnVgkiR1g7KSIihvma2yRlm67Ff19tlH7ckTGstHHZOZPy32nwH8W52XkLKPmiSpLsruVzY4OPlltqZNaySNe4to1Gi1O55eN1oftVYStTuAozLz58X+TOC2zFw4xutmAjcB+9MYXXpZZp4TETcDBxSnPQ+4NTNPi4gTgCuB+4pjV2TmB4trnQL8NTAd+KfM/PBo722iJkmqizKTorLUbVBCrxstUWtlZYJ/Br4TEV8u9k8DPt3C654ATszMxyNiBnBLRFybmcc3BXY5jeRs2M2Z+fq9gp8OfAo4GdgM3BYRV2XmXS3EIElSR9Vx6olVq0YeZWmTZf20Mpjgr4C3AQ8X29sy8+MtvC4z8/Fid0axPfk3RUQcCJwIfGWMSy0BNmTmvZn5C+AS4NSx3l+SpDqoYz8uR1lOHS0tIZWZ3y2m6/hEZn6v1YtHxPSIWAtsAa7LzO80HT4NuD4ztzeVHRMRt0fEtRGxoCg7BHig6ZzNRdne77UiIoYiYmjr1q2thihJUqXqmhQ5ynJ0dVkBotJF2TNzF7A4Ig4CvhwRL8/M7xeHzwT+qen07wLziqbS36JR03bYON5rNbAaGn3USvkAkiSVYGDARGgqqdMKEG1ZlD0zHwVuAE4BiIiDaTRpfrXpnO3DTaWZeQ0wozjvQeBFTZebU5RJkiSVrk6LxI+aqBVNlzdM5MIRMbuoSSMink5jMMDdxeEzgKuHR5IW57wgIqJ4vqSIbRtwG3BYRMyPiKcBy4CrJhKTJEnSWOq0SPyoiVrRdLk7Ip41gWu/ELihmIftNhp91K4uji0DLt7r/DOA70fE7cAngGXFgISdwDuBrwPrgS9m5p0TiEeSJHW5MvqWlbniwmS1Mo/alcCRwHXAT4fLM/Nd1YY2cc6jJklS7ylrcfd2LxI/2Qlvl49UnpkXlhBbJUzUJEnqPWVO5NvOFRcmlagVF3g6MDcz7yk7uCqYqEmS1HvquApEK0ZL1FpZlP23gbXA14r9xRFhZ35JklQrdepbVpZWpuc4l8ZUGo8CZOZa4MUVxiRJ3asus2hKXaiOq0BMViuJ2i8z87G9ympcgShJNTXcQ3nTpkb7zPAsmiZrUinqugrEZLQymODTwPXAB4A3AO8CZmTmH1Uf3sTYR01SLZXZ01lS15hUHzXgvwMLgCdozH22HfiT8sKTpB5Rp1k0JU0JYyZqmbkjM1cCJwGvzsyVzSsKSFJt1a0/WDf2dJZUqVZGfR4VEXcA64A7IuL2iPi16kOTpEmoY3+wbuzpLKlSrTR9fhr4b5nZl5l9wDuAf640KkmarDqtqjysG3s6S6rUfi2csyszbx7eycxbImJnhTFJ0uTVtT/YwICJmaSW7TNRi4hXFk+/GRH/QGMgQQL/Fbix+tAkaRLmzh15hKX9wSRNIaPVqH10r/1zmp6Pve6UJHXSqlUjr6psfzBJU8g+E7XMfHU7A5GkUg03L7ZrVWVJqsCYfdQi4iDgLUBf8/mZ+a7qwpKkEtgfTMV1P68AABMXSURBVNIU18pggmuAfwfuwKWjJEmS2qaVRG1mZr6n8kgkSZK0h1bmUft8RPxhRLwwIp4zvFUemSRJUo9rpUbtF8D/B6zkV6M9E3hxVUFJkiSptUTtvcBLMvPHVQcjSZKkX2ml6XMDsGPMsySpbougS9IU10qN2k+BtRFxA/DEcKHTc0jaw/Ai6MMTzA4vgg5OkSFJExSZoy8yEBHLRyrPzAsriagE/f39OTQ01OkwpN7S1zfykk3z5sHGje2ORpKmjIhYk5n9Ix0bs0atzgmZpBqp6yLo3Wpw0FUXpB4wZh+1iLgvIu7de2tHcJKmkH0tdt4Ni6DXre/dcDPzpk2Q+atm5k7HJal0rQwm6AeOKrbjgU8AX6gyKElT0KpVjUXPm3XDIuh1TIpWrtxzsXlo7K9c2Zl4JFVmzEQtM7c1bQ9m5seB17UhNklTycAArF7d6JMW0XhcvXpizXF1qsGqY1JkM7PUM1pZlP2VTbvTaNSwtTJaVFKvKWMR9LqNHq1jUjR37sgDN7qhmVnSHlpp+vxo0/Yh4NeAN1YZlKQeVrcarDr2vevWZmZJT9FK0+erm7aTM/MPM/OedgQnqU3q1NRYtxqsOiZFZTYzS6q1Vpo+9wfeAPQ1n5+ZH6wuLEltU7emxro16w3fg7pNhVFGM7Ok2mtlwtuvAY8Ba4Bdw+WZ+dFqQ5s4J7yVxqFuE9XunThCowbLGiNJXWpSE94CczLzlJJjklQXdWtqrGsNliR1QCuJ2rcjYmFm3lF5NJLar25NjWCzniQVWhn1eRywJiLuiYh1EXFHRKyrOjBJbVLHzvKSJKC1RO21wGHAUuC3gdcXj5K6gSMI1Y3qNJJZmoRWpufYNNLWjuC6nj8kqouBgcbAgd27G48maZrK6rjslzRBrdSoqQr+kEhSNeo2abI0CSZqneIPiSRVo24jmaVJqCxRi4iZEXFrRNweEXdGxHlF+Wcj4r6IWFtsi4vyiIhPRMSGYtDCK5uutTwiflBsy6uKua38IZGkatRx2S9pgqqsUXsCODEzXwEsBk6JiKOLY3+amYuLbW1RNjxo4TBgBfB3ABHxHOAc4NeBJcA5EfHsCuNuD39IJHWTOvW5dSSzukhliVo2PF7szii20ZZBOBX4XPG6fwcOiogXAq8BrsvMhzPzEeA6YOpPwOsPiaRuUbc+t45kVheptI9aREyPiLXAFhrJ1neKQ6uK5s2PFWuJAhwCPND08s1F2b7K936vFRExFBFDW7duLf2zlM4fEkndoo59bh3JrC5RaaKWmbsyczEwB1gSES8HzgYOB44CngO8v6T3Wp2Z/ZnZP3v27DIuWT1/SCR1A/vcSpVpy6jPzHwUuAE4JTMfKpo3nwD+mUa/M4AHgRc1vWxOUbavcklSHdjnVqpMlaM+Z0fEQcXzpwMnA3cX/c6IiABOA75fvOQq4C3F6M+jgccy8yHg68DSiHh2MYhgaVEmSaqDMvvc1mlQglQDVdaovRC4oVgX9DYafdSuBgYj4g7gDuBg4Pzi/GuAe4ENwD8C/w0gMx8G/ldxjduADxZlkqTJKiMxKqvPbd0GJUg1EJmjDcScmvr7+3NoaKjTYUhSvQ0nRs0DAWbN6tzApr6+RnK2t3nzGv14pS4VEWsys3+kY65MIEm9qm6jNR2UID2FiZok9aq6JUYOSpCewkRNknpV3RIjJwKXnsJETZJ6Vd0SIycCb40jY3vKfp0OQJLUIcMJ0MqVjebOuXMbSVonE6OBAROz0ew9AGR4ZCx437qUoz4lSZoqHBnblRz1KXUrm0Ck3lK3ASCqnImaNFU5OajUe+o2AESVM1GTpqq6zYElqXp1GwCiypmoSVOVTSBS73FkbM9x1Kc0Vc2dO3KnYptApO7myNieYo2aNFXZBCJVr5sH7HTzZ+siJmrqDWX9INXph80mEKla3Txgp5s/W5dxHjV1v70niIRGzdN4k5qyriNpaujmOcu6+bNNQaPNo2aipu5X1g+SP2xSb5k2rVHbtLcI2L27/fGUqZs/2xTkhLfqbWWNjnSUpdRbunnOsm7+bF3GRE3dr6wfJH/YpN7SzQN2uvmzdRkTNXW/sn6Q/GGTeks3D9gp87PVaZBVFzJRU/cr6wfJHzap9wwMNPqg7t7deJxoklbHf/NlfDZHj1bOwQRSuzl6VOot3fxv3kFWpXDUp1Qn/rBJvaWb/807erQUjvqU6sTRo1Jv6eZ/8w6yqpyJmtRu/rBJvaWb/82XOciqjv34asBETWo3R49KvaWb/82XNciqjoMSapI42kdN6oTBQVi5stH0MXdu4wd7qncqlrRv/psfXd368bV5AIiDCSRJUn3VbVBCmxNHBxNIkqT6qls/vhoNADFRkyRJnVW3fnw1ShxN1CRJUmfVbbmuGiWO+7X9HSVJkvY2MFCfARbDcdRgAIg1apIkqXuUNa1GWeu8TpI1apIkqTvsPa3G8HxsUJ/aunGyRk2SJHWHlSv3nPsMGvsrV3YmnhKYqEmSpO5Qo2k1ymKiJkmSukONptUoi4maJEnqDjWaVqMsJmqSJKk71G0+thI46lOSJHWPOs3HVoLKatQiYmZE3BoRt0fEnRFxXlE+GBH3RMT3I+IzETGjKD8hIh6LiLXF9udN1zqleM2GiPhAVTFLkiTVSZU1ak8AJ2bm40UydktEXAsMAm8qzrkIOAv4u2L/5sx8ffNFImI68CngZGAzcFtEXJWZd1UYuyRJUsdVVqOWDY8XuzOKLTPzmuJYArcCc8a41BJgQ2bem5m/AC4BTq0qbkmSpLqodDBBREyPiLXAFuC6zPxO07EZwJuBrzW95JiiqfTaiFhQlB0CPNB0zuaibO/3WhERQxExtHXr1tI/iyRJUrtVmqhl5q7MXEyj1mxJRLy86fDfAjdl5s3F/neBeZn5CuCTwFfG+V6rM7M/M/tnz55dRviSJEkd1ZbpOTLzUeAG4BSAiDgHmA28p+mc7cNNpZl5DTAjIg4GHgRe1HS5OUWZJElSV6ty1OfsiDioeP50GoMB7o6Is4DXAGdm5u6m818QEVE8X1LEtg24DTgsIuZHxNOAZcBVVcUtSZJUF1WO+nwhcGExanMa8MXMvDoidgKbgH8r8rIrMvODwBnAHxfHfwYsKwYc7IyIdwJfB6YDn8nMOyuMW5IkqRaikQt1l/7+/hwaGup0GJIkSWOKiDWZ2T/SMZeQkiRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJR6waDg9DXB9OmNR4HBzsdUTm69XNJktSiKie8VTsMDsKKFbBjR2N/06bGPsDAQOfimqxu/VySJI2DE95OdX19jSRmb/PmwcaN7Y6mPN36uSRJ2osT3naz++8fX/lU0a2fS5KkcTBRm+rmzh1f+VTRrZ9LkqRxMFGb6latglmz9iybNatRPpV16+eSJGkcTNSmuoEBWL260XcrovG4evXU73DfrZ9LkqRxcDCBJElSBzmYQJIkaQoyUZMkSaopEzVJkqSaMlFT+Vz6SZKkUriElMrl0k+SJJXGGjWVa+XKXyVpw3bsaJRLkqRxMVFTuVz6SZKk0pioqVwu/SRJUmlM1FQul36SJKk0Jmoql0s/SZJUGkd9qnwDAyZmkiSVwBo1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqKRM1SZKkmjJRkyRJqqnKErWImBkRt0bE7RFxZ0ScV5TPj4jvRMSGiLg0Ip5WlO9f7G8ojvc1XevsovyeiHhNVTH3vMFB6OuDadMaj4ODnY5IkqSeVmWN2hPAiZn5CmAxcEpEHA38BfCxzHwJ8AjwB8X5fwA8UpR/rDiPiDgCWAYsAE4B/jYiplcYd28aHIQVK2DTJshsPK5YYbImSVIHVZaoZcPjxe6MYkvgROCyovxC4LTi+anFPsXxkyIiivJLMvOJzLwP2AAsqSrunrVyJezYsWfZjh2NckmS1BGV9lGLiOkRsRbYAlwH/BB4NDN3FqdsBg4pnh8CPABQHH8MeG5z+QivaX6vFRExFBFDW7dureLjdLf77x9fuSRJqlyliVpm7srMxcAcGrVgh1f4Xqszsz8z+2fPnl3V23SvuXPHVy5JkirXllGfmfkocANwDHBQROxXHJoDPFg8fxB4EUBx/FnAtubyEV6jsqxaBbNm7Vk2a1ajXJIkdUSVoz5nR8RBxfOnAycD62kkbGcUpy0HriyeX1XsUxz/18zMonxZMSp0PnAYcGtVcfesgQFYvRrmzYOIxuPq1Y1ySZLUEfuNfcqEvRC4sBihOQ34YmZeHRF3AZdExPnA94BPF+d/Gvh8RGwAHqYx0pPMvDMivgjcBewE3pGZuyqMu3cNDJiYSZJUI9GotOou/f39OTQ01OkwJEmSxhQRazKzf6RjrkwgSZJUUyZqkiRJNWWiJkmSVFMmapIkSTVloiZJklRTJmqSJEk1ZaImSZJUUyZqkiRJNWWiJkmSVFMmapIkSTVloiZJklRTXbnWZ0RsBTaNcsrBwI/bFE4v8z63j/e6PbzP7eO9bh/vdXuMdp/nZebskQ50ZaI2logY2tfipyqP97l9vNft4X1uH+91+3iv22Oi99mmT0mSpJoyUZMkSaqpXk3UVnc6gB7hfW4f73V7eJ/bx3vdPt7r9pjQfe7JPmqSJElTQa/WqEmSJNWeiZokSVJN9VSiFhGnRMQ9EbEhIj7Q6Xi6WURsjIg7ImJtRAx1Op5uEhGfiYgtEfH9prLnRMR1EfGD4vHZnYyxG+zjPp8bEQ8W3+u1EfFbnYyxG0TEiyLihoi4KyLujIh3F+V+p0s2yr32e12yiJgZEbdGxO3FvT6vKJ8fEd8p8pBLI+JpY16rV/qoRcR04P8HTgY2A7cBZ2bmXR0NrEtFxEagPzOdRLFkEfEq4HHgc5n58qLsL4GHM/PDxR8hz87M93cyzqluH/f5XODxzPxIJ2PrJhHxQuCFmfndiDgAWAOcBrwVv9OlGuVevxG/16WKiACekZmPR8QM4Bbg3cB7gCsy85KI+Hvg9sz8u9Gu1Us1akuADZl5b2b+ArgEOLXDMUnjlpk3AQ/vVXwqcGHx/EIaP76ahH3cZ5UsMx/KzO8Wz38CrAcOwe906Ua51ypZNjxe7M4otgROBC4rylv6XvdSonYI8EDT/mb8glYpgf8dEWsiYkWng+kBz8/Mh4rn/wE8v5PBdLl3RsS6omnU5rgSRUQfcCTwHfxOV2qvew1+r0sXEdMjYi2wBbgO+CHwaGbuLE5pKQ/ppURN7XVcZr4SeC3wjqIZSW2Qjf4MvdGnof3+DjgUWAw8BHy0s+F0j4h4JnA58CeZub35mN/pco1wr/1eVyAzd2XmYmAOjVa9wydynV5K1B4EXtS0P6coUwUy88HicQvwZRpfUlXnR0X/k+F+KFs6HE9XyswfFT++u4F/xO91KYo+PJcDg5l5RVHsd7oCI91rv9fVysxHgRuAY4CDImK/4lBLeUgvJWq3AYcVIy6eBiwDrupwTF0pIp5RdFQlIp4BLAW+P/qrNElXAcuL58uBKzsYS9caThwKp+P3etKKTtefBtZn5l81HfI7XbJ93Wu/1+WLiNkRcVDx/Ok0BjKup5GwnVGc1tL3umdGfQIUQ44/DkwHPpOZqzocUleKiBfTqEUD2A+4yHtdnoi4GDgBOBj4EXAO8BXgi8BcYBPwxsy0I/wk7OM+n0CjeSiBjcDbm/pRaQIi4jjgZuAOYHdR/D9p9J3yO12iUe71mfi9LlVELKIxWGA6jUqxL2bmB4v/P14CPAf4HvCmzHxi1Gv1UqImSZI0lfRS06ckSdKUYqImSZJUUyZqkiRJNWWiJkmSVFMmapIkSTVloiZJJYqIEyLi6k7HIak7mKhJkiTVlImapJ4UEW+KiFsjYm1E/EOxgPLjEfGxiLgzIq6PiNnFuYsj4t+LRau/PLxodUS8JCK+ERG3R8R3I+LQ4vLPjIjLIuLuiBgsZoSXpHEzUZPUcyLiZcB/BY4tFk3eBQwAzwCGMnMB8E0aqxEAfA54f2YuojGr+3D5IPCpzHwF8Bs0FrQGOBL4E+AI4MXAsZV/KEldab+xT5GkrnMS8GvAbUVl19NpLPq9G7i0OOcLwBUR8SzgoMz8ZlF+IfClYj3bQzLzywCZ+XOA4nq3ZubmYn8t0AfcUv3HktRtTNQk9aIALszMs/cojPh/9zpvomvsNa/dtwt/ayVNkE2fknrR9cAZEfE8gIh4TkTMo/GbeEZxzu8Bt2TmY8AjEXF8Uf5m4JuZ+RNgc0ScVlxj/4iY1dZPIanr+VeepJ6TmXdFxJ8B/zsipgG/BN4B/BRYUhzbQqMfG8By4O+LROxe4G1F+ZuBf4iIDxbX+C9t/BiSekBkTrRmX5K6S0Q8npnP7HQckjTMpk9JkqSaskZNkiSppqxRkyRJqikTNUmSpJoyUZMkSaopEzVJkqSaMlGTJEmqqf8LnHiVCNEmguwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPLbRXCh2uD5"
      },
      "source": [
        "# Devin's flashcard idea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLmSL9HbPzQc"
      },
      "source": [
        "## 20% of batch replaced with forgettable examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luUvk2xHD_Kc"
      },
      "source": [
        "Devin proposed the following class of experiments, motivated by the way humans learn not to forget difficult examples using flashcards. During training, identify the more forgettable examples (e.g. forgotten more than 3 times) and put them into a stack. Take some subset from this stack and reintroduce them stochastically into the current batch (kind of like shuffling in difficult flashcards into ones you've already learned)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miSqnuNN2zaW",
        "outputId": "105eeb7d-019d-42ea-a5e5-f54375a9672a"
      },
      "source": [
        "#First load models, dataset\n",
        "#make sure to restart runtime\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.CrossEntropyLoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'open_lth' already exists and is not an empty directory.\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TranTF0VmuS9",
        "outputId": "abcacfc2-3e16-4a27-cc84-60bee2cdc362"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "nb_epochs = 60\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "\n",
        "batch_tracker = 0\n",
        "forget_thres = 4 #if an example is forgotten more than 5 times, we will add it to the pile\n",
        "percent_keep = 20 #replace 20%\n",
        "\n",
        "forget_deck_x = torch.empty((0, 3,32,32)).cuda()\n",
        "forget_deck_y = torch.empty((0)).cuda()\n",
        "\n",
        "acc_A_global = list()\n",
        "acc_B_global = list()\n",
        "acc_A_unmodified_global = list()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "    accuracies_A_unmodified = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        #y = y.cuda()\n",
        "\n",
        "        \n",
        "        y_prime = y.clone().detach() #save original targets\n",
        "        x_prime = x.clone().detach() #and the original images\n",
        "\n",
        "        x_mod = x.clone().detach()\n",
        "        y_mod = y.clone().detach()\n",
        "\n",
        "        #replace some of x and y with those from forgotten pile\n",
        "        #only do this after training for a bit\n",
        "\n",
        "        if epoch > 10 and forget_deck_x.size()[0] >= x.size()[0]:\n",
        "            #deck_range = (forget_deck_x.size()[0]) * percent_keep // 100\n",
        "            x_range = (x.size()[0]) * percent_keep // 100\n",
        "            rand_idcs = random.sample(range((forget_deck_x.size()[0])), x_range)\n",
        "            rand_idcs_x = random.sample(range((x.size()[0])), x_range)\n",
        "            x_mod[rand_idcs_x] = forget_deck_x[rand_idcs]\n",
        "            y_mod[rand_idcs_x] = forget_deck_y[rand_idcs].type(torch.LongTensor)\n",
        "            #print(f\"Shuffling in forgotten examples... at positions {rand_idcs_x} from {rand_idcs} \\n\")\n",
        "\n",
        "        l_A = model_A(x_mod)\n",
        "        l_B = model_B(x_prime)\n",
        "        with torch.no_grad():\n",
        "            l_A_unmod = model_A(x_prime)\n",
        "\n",
        "        \n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y_mod[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            if torch.sum(forget_matrix_A, 0)[batch_tracker, k] > forget_thres and forget_deck_x.size()[0] < 2000:\n",
        "                forget_deck_x = torch.cat((forget_deck_x, x_prime[k].unsqueeze(0)), 0)\n",
        "                y_prime = y_prime.cuda()\n",
        "                forget_deck_y = torch.cat((forget_deck_y, y_prime[k].unsqueeze(0)), 0)\n",
        "                y_prime = y_prime.cpu()\n",
        "                #print(f\"Added to forget deck! Size: {forget_deck_x.size()} \\n\")\n",
        "                #and add this example to forgotten list\n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "\n",
        "        #check if model B's prediction is correctly classified\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y_prime[k]: #if it is, mark as '1'\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y_mod.cuda())\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_B = loss_B(l_B, y_prime.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "        J_B.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        accuracies_A.append(y_mod.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_B.append(y_prime.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        accuracies_A_unmodified.append(y_prime.eq(l_A_unmod.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B, A on unmod set: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f}, {torch.tensor(accuracies_A_unmodified).mean():.2f} \\n\")\n",
        "    #print(f\"Training accuracy for A on unmodified set: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_A_unmodified).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "    acc_B_global.append(torch.tensor(accuracies_B).mean())\n",
        "    acc_A_unmodified_global.append(torch.tensor(accuracies_A_unmodified).mean())\n",
        "\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A, B: 2.11 , 2.07 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.22 , 0.23, 0.22 \n",
            "\n",
            "Epoch 2, train loss A, B: 1.82 , 1.80 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.32 , 0.32, 0.32 \n",
            "\n",
            "Epoch 3, train loss A, B: 1.69 , 1.69 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.37 , 0.37, 0.37 \n",
            "\n",
            "Epoch 4, train loss A, B: 1.61 , 1.61 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.40 , 0.40, 0.40 \n",
            "\n",
            "Epoch 5, train loss A, B: 1.55 , 1.55 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.42 , 0.43, 0.42 \n",
            "\n",
            "Epoch 6, train loss A, B: 1.49 , 1.49 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.45 , 0.45, 0.45 \n",
            "\n",
            "Epoch 7, train loss A, B: 1.44 , 1.45 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.47 , 0.47, 0.47 \n",
            "\n",
            "Epoch 8, train loss A, B: 1.39 , 1.40 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.49 , 0.49, 0.49 \n",
            "\n",
            "Epoch 9, train loss A, B: 1.35 , 1.36 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.51 , 0.51, 0.51 \n",
            "\n",
            "Epoch 10, train loss A, B: 1.31 , 1.32 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.52 , 0.52, 0.52 \n",
            "\n",
            "Epoch 11, train loss A, B: 1.28 , 1.29 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.53 , 0.53, 0.53 \n",
            "\n",
            "Epoch 12, train loss A, B: 1.24 , 1.26 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.55 , 0.55, 0.55 \n",
            "\n",
            "Epoch 13, train loss A, B: 1.21 , 1.23 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.56 , 0.56, 0.56 \n",
            "\n",
            "Epoch 14, train loss A, B: 1.14 , 1.21 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.60 , 0.56, 0.56 \n",
            "\n",
            "Epoch 15, train loss A, B: 1.10 , 1.18 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.61 , 0.58, 0.57 \n",
            "\n",
            "Epoch 16, train loss A, B: 1.09 , 1.15 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.61 , 0.59, 0.58 \n",
            "\n",
            "Epoch 17, train loss A, B: 1.08 , 1.13 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.61 , 0.60, 0.59 \n",
            "\n",
            "Epoch 18, train loss A, B: 1.06 , 1.11 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.62 , 0.60, 0.60 \n",
            "\n",
            "Epoch 19, train loss A, B: 1.02 , 1.09 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.64 , 0.61, 0.61 \n",
            "\n",
            "Epoch 20, train loss A, B: 1.00 , 1.07 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.65 , 0.62, 0.62 \n",
            "\n",
            "Epoch 21, train loss A, B: 0.98 , 1.05 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.66 , 0.62, 0.62 \n",
            "\n",
            "Epoch 22, train loss A, B: 0.95 , 1.03 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.67 , 0.63, 0.63 \n",
            "\n",
            "Epoch 23, train loss A, B: 0.94 , 1.01 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.68 , 0.64, 0.63 \n",
            "\n",
            "Epoch 24, train loss A, B: 0.92 , 1.00 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.68 , 0.64, 0.64 \n",
            "\n",
            "Epoch 25, train loss A, B: 0.90 , 0.98 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.69 , 0.65, 0.64 \n",
            "\n",
            "Epoch 26, train loss A, B: 0.89 , 0.97 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.70 , 0.66, 0.65 \n",
            "\n",
            "Epoch 27, train loss A, B: 0.87 , 0.95 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.70 , 0.66, 0.66 \n",
            "\n",
            "Epoch 28, train loss A, B: 0.86 , 0.93 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.71 , 0.67, 0.66 \n",
            "\n",
            "Epoch 29, train loss A, B: 0.85 , 0.92 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.71 , 0.67, 0.66 \n",
            "\n",
            "Epoch 30, train loss A, B: 0.83 , 0.91 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.72 , 0.68, 0.66 \n",
            "\n",
            "Epoch 31, train loss A, B: 0.82 , 0.90 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.72 , 0.68, 0.67 \n",
            "\n",
            "Epoch 32, train loss A, B: 0.81 , 0.89 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.73 , 0.68, 0.67 \n",
            "\n",
            "Epoch 33, train loss A, B: 0.80 , 0.88 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.73 , 0.69, 0.68 \n",
            "\n",
            "Epoch 34, train loss A, B: 0.79 , 0.87 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.73 , 0.69, 0.68 \n",
            "\n",
            "Epoch 35, train loss A, B: 0.78 , 0.86 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.74 , 0.70, 0.68 \n",
            "\n",
            "Epoch 36, train loss A, B: 0.77 , 0.85 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.74 , 0.70, 0.69 \n",
            "\n",
            "Epoch 37, train loss A, B: 0.76 , 0.84 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.75 , 0.70, 0.69 \n",
            "\n",
            "Epoch 38, train loss A, B: 0.75 , 0.82 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.75 , 0.71, 0.70 \n",
            "\n",
            "Epoch 39, train loss A, B: 0.75 , 0.82 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.75 , 0.71, 0.70 \n",
            "\n",
            "Epoch 40, train loss A, B: 0.74 , 0.81 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.75 , 0.71, 0.70 \n",
            "\n",
            "Epoch 41, train loss A, B: 0.73 , 0.80 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.75 , 0.72, 0.70 \n",
            "\n",
            "Epoch 42, train loss A, B: 0.72 , 0.79 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.76 , 0.72, 0.70 \n",
            "\n",
            "Epoch 43, train loss A, B: 0.71 , 0.78 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.76 , 0.72, 0.71 \n",
            "\n",
            "Epoch 44, train loss A, B: 0.71 , 0.78 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.76 , 0.73, 0.71 \n",
            "\n",
            "Epoch 45, train loss A, B: 0.70 , 0.77 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.77 , 0.73, 0.71 \n",
            "\n",
            "Epoch 46, train loss A, B: 0.70 , 0.76 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.76 , 0.73, 0.71 \n",
            "\n",
            "Epoch 47, train loss A, B: 0.69 , 0.75 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.77 , 0.74, 0.71 \n",
            "\n",
            "Epoch 48, train loss A, B: 0.68 , 0.74 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.77 , 0.74, 0.72 \n",
            "\n",
            "Epoch 49, train loss A, B: 0.67 , 0.73 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.74, 0.72 \n",
            "\n",
            "Epoch 50, train loss A, B: 0.67 , 0.73 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.74, 0.72 \n",
            "\n",
            "Epoch 51, train loss A, B: 0.66 , 0.72 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.75, 0.72 \n",
            "\n",
            "Epoch 52, train loss A, B: 0.66 , 0.71 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.75, 0.73 \n",
            "\n",
            "Epoch 53, train loss A, B: 0.65 , 0.70 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.75, 0.73 \n",
            "\n",
            "Epoch 54, train loss A, B: 0.65 , 0.70 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.76, 0.73 \n",
            "\n",
            "Epoch 55, train loss A, B: 0.64 , 0.69 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.78 , 0.76, 0.73 \n",
            "\n",
            "Epoch 56, train loss A, B: 0.63 , 0.68 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.79 , 0.76, 0.74 \n",
            "\n",
            "Epoch 57, train loss A, B: 0.63 , 0.68 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.79 , 0.76, 0.74 \n",
            "\n",
            "Epoch 58, train loss A, B: 0.63 , 0.67 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.79 , 0.77, 0.74 \n",
            "\n",
            "Epoch 59, train loss A, B: 0.63 , 0.66 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.79 , 0.77, 0.74 \n",
            "\n",
            "Epoch 60, train loss A, B: 0.62 , 0.66 \n",
            "\n",
            "Training accuracy for A, B, A on unmod set: 0.79 , 0.77, 0.74 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIY1nUvVm1Ve"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3P3ljVlANlt"
      },
      "source": [
        "xprime, yprime = next(iter(train_set))\n",
        "xprime=xprime.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzkcwuzUBLnM"
      },
      "source": [
        "x, y= next(iter(train_set))\n",
        "x=x.cuda()\n",
        "y = y.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9sj7ICxasgq",
        "outputId": "72df6e66-5462-409c-a335-8f52ab770880"
      },
      "source": [
        "x[2].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd5usn_B1sUA"
      },
      "source": [
        "rand_idcs_x = random.sample(range((x.size()[0])), x_range)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iocnAc6B1trs",
        "outputId": "2ed284d7-76f0-423b-dce1-816315644733"
      },
      "source": [
        "rand_idcs_x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[85,\n",
              " 100,\n",
              " 96,\n",
              " 46,\n",
              " 97,\n",
              " 71,\n",
              " 27,\n",
              " 91,\n",
              " 76,\n",
              " 28,\n",
              " 8,\n",
              " 37,\n",
              " 64,\n",
              " 4,\n",
              " 26,\n",
              " 14,\n",
              " 79,\n",
              " 112,\n",
              " 81,\n",
              " 66,\n",
              " 126,\n",
              " 2,\n",
              " 22,\n",
              " 7,\n",
              " 63]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNDPGgJg2YUi",
        "outputId": "b128cb6f-c270-4fe5-dd84-41de4b738418"
      },
      "source": [
        "x.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "oTNM8jr42Nmm",
        "outputId": "a34f7dce-4698-4285-bf01-cbab211db3e2"
      },
      "source": [
        "x_range = (x.size()[0]) * percent_keep // 100\n",
        "rand_idcs = random.sample(range((forget_deck_x.size()[0])), x_range)\n",
        "rand_idcs_x = random.sample(range((x.size()[0])), x_range)\n",
        "x[rand_idcs_x] = forget_deck_x[rand_idcs]\n",
        "y[rand_idcs_x] = forget_deck_y[rand_idcs]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2fc91883cb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpercent_keep\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrand_idcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_deck_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrand_idcs_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idcs_x\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforget_deck_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idcs_x\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforget_deck_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN87U5Ri1k15"
      },
      "source": [
        "x_range = (x.size()[0]) * percent_keep // 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ9PUGlK1mUu",
        "outputId": "168e486b-cffa-4903-f590-9a97146fbfbe"
      },
      "source": [
        "x_range"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwm8g4dIZwlO"
      },
      "source": [
        "deck = torch.empty((0,3,32,32)).cuda()\n",
        "deck_y = torch.empty(0).cuda()\n",
        "#deck = torch.stack((x[0], x[2]),0)\n",
        "#deck = torch.stack((deck, x[3]),0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uP8IyhenhrX"
      },
      "source": [
        "deck = torch.cat((deck, x[2].unsqueeze(0)),0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKc8g3sMokhV",
        "outputId": "9fad8ed5-3b3e-4e2b-fdcc-913972413069"
      },
      "source": [
        "y[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfLAs1mfocSy"
      },
      "source": [
        "deck_y = torch.cat((deck_y, y[2].unsqueeze(0)), 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tcdOulPo7Z0"
      },
      "source": [
        "deck_y = torch.cat((deck_y, y[4].unsqueeze(0)), 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61jSPhgMo3Y9",
        "outputId": "6c048e2b-4286-4b58-d563-f3d961bdf279"
      },
      "source": [
        "deck_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9., 1.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGiG1jOanxEK"
      },
      "source": [
        "for i in range(10):\n",
        "    deck = torch.cat((deck, x[i].unsqueeze(0)),0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is3DY0s_num9",
        "outputId": "ffb5a264-4b47-4bb9-b738-5ec9fe5e5a48"
      },
      "source": [
        "deck.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV-9eRwbmVHp",
        "outputId": "696ad9cb-e1f7-4144-d428-7869ce56826a"
      },
      "source": [
        "ids=[0,2,5,12]\n",
        "torch.stack(x[ids].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNwt0WQWa4q_",
        "outputId": "3c622cb8-98fe-44af-950b-c11cb32d4d4b"
      },
      "source": [
        "deck.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "fsJ8rAddmJaf",
        "outputId": "baf80a08-344f-4e35-9dc9-c518fecdc970"
      },
      "source": [
        "torch.stack((deck, x[3]),0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-33f0dc629f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 3, 32, 32] at entry 0 and [3, 32, 32] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3_LuepWkusM",
        "outputId": "74faa2d2-357a-4ca8-cdb8-502e44f0340e"
      },
      "source": [
        "x.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvmUcrploFys",
        "outputId": "c773f237-87e9-4b06-a780-e369acc06940"
      },
      "source": [
        "y.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXCe44w3ZfMC"
      },
      "source": [
        "xprime=torch.cat((xprime, x),0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZBR6VYSZmPL",
        "outputId": "57a6b19c-fb21-4b15-8c16-863daed94462"
      },
      "source": [
        "x[2].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE_jSXahZoMS",
        "outputId": "2e005ee2-a012-420d-a159-9aae9443db59"
      },
      "source": [
        "deck.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgdt5y07dcyK",
        "outputId": "c0be3733-bba1-4722-9e8e-5f5b7aaa5d66"
      },
      "source": [
        "(deck.size())[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx0GirHhppqJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Qvxl4Dq5sD",
        "outputId": "f497b609-540b-4acb-e032-8cfba37144f0"
      },
      "source": [
        "k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHvDHjJppnce"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "percentage = 20\n",
        "k = (deck.size()[0]) * percentage // 100\n",
        "indicies = random.sample(range((deck.size()[0])), k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDqjoDg_q-xq",
        "outputId": "39765588-3602-4d15-da12-8a0b8b0343bc"
      },
      "source": [
        "indicies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 9, 15]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwfxaJkZrgSa"
      },
      "source": [
        "x[indicies] = deck[indicies]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZCbtGsHAUfV"
      },
      "source": [
        "y[[0,1,2]] = forget_deck_y[[0,1,2]].type(torch.LongTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0fhBqA0Aus7",
        "outputId": "fd6bd9d2-ddc9-4461-ecdc-028bc18fcf01"
      },
      "source": [
        "forget_deck_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 9., 4., 6., 8., 7., 4., 5., 4., 6., 3., 3., 2., 9., 3., 6., 1., 4.,\n",
              "        1., 6., 4., 1., 9., 3., 8., 8., 8., 7., 4., 8., 8., 9., 4., 6., 5., 9.,\n",
              "        9., 5., 9., 7., 0., 8., 9., 4., 4., 8., 4., 7., 9., 4., 1., 5., 6., 3.,\n",
              "        6., 3., 5., 4., 8., 1., 1., 4., 3., 4., 3., 3., 5., 3., 7., 2., 3., 4.,\n",
              "        4., 2., 9., 9., 3., 0., 4., 6., 1., 6., 3., 8., 4., 6., 1., 4., 4., 3.,\n",
              "        1., 8., 4., 6., 4., 4., 7., 1., 9., 4., 3., 8., 8., 8., 7., 4., 9., 1.,\n",
              "        1., 4., 3., 9., 8., 8., 8., 8., 9., 1., 8., 6., 7., 1., 5., 7., 0., 3.,\n",
              "        8., 1.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "Hd8_gFPDRym8",
        "outputId": "a7572e6e-502d-4f13-d208-f267025f49c7"
      },
      "source": [
        "model_A(x)\n",
        "model_B()\n",
        "J_B.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-127361310b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mJ_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr0Q249Daa3S"
      },
      "source": [
        "x, y = next(iter(train_set))\n",
        "x= x.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vYSWKGvmirxH",
        "outputId": "0a6f151a-b2cc-47f3-8116-44af7e8d8cbf"
      },
      "source": [
        "x.type()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'torch.cuda.FloatTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6OG4KI_RyK7"
      },
      "source": [
        "x, y = next(iter(train_set))\n",
        "x= x.cuda()\n",
        "#y = y.cuda()\n",
        "yprime = y\n",
        "blah = torch.empty((0,3,32,32)).cuda()\n",
        "blah = torch.cat((blah, x[3].unsqueeze(0)),0)\n",
        "blah = torch.cat((blah, x[54].unsqueeze(0)),0)\n",
        "blah = torch.cat((blah, x[12].unsqueeze(0)),0)\n",
        "blah = torch.cat((blah, x[33].unsqueeze(0)),0)\n",
        "\n",
        "blahy = torch.empty(0).cuda()\n",
        "y=y.cuda()\n",
        "blahy = torch.cat((blahy, y[3].unsqueeze(0)),0)\n",
        "blahy = torch.cat((blahy, y[54].unsqueeze(0)),0)\n",
        "blahy = torch.cat((blahy, y[12].unsqueeze(0)),0)\n",
        "blahy = torch.cat((blahy, y[33].unsqueeze(0)),0)\n",
        "#blahy = blahy.cuda()\n",
        "\n",
        "listidc=[1, 3, 19, 2]\n",
        "blahidc=[0,1,2,3]\n",
        "x[listidc] = blah[blahidc]\n",
        "blahy=blahy.cpu()\n",
        "y=y.cpu()\n",
        "y[listidc] = blahy[blahidc].type(torch.LongTensor)\n",
        "la=model_A(x)\n",
        "lb = model_B(x)\n",
        "\n",
        "loss_a = loss_A(la, y.cuda())\n",
        "loss_a.backward()\n",
        "loss_b = loss_B(lb, yprime.cuda())\n",
        "loss_b.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "nFPO2u8ta-aK",
        "outputId": "f9e5cea8-4576-4a65-c18d-d88fa1b50a09"
      },
      "source": [
        "x,y=next(iter(train_set))\n",
        "x=x.cuda()\n",
        "lb = model_B(x)\n",
        "\n",
        "yprime =y\n",
        "import random\n",
        "\n",
        "percent_keep=20\n",
        "x_range = (x.size()[0]) * percent_keep // 100\n",
        "rand_idcs = random.sample(range((forget_deck_x.size()[0])), x_range)\n",
        "rand_idcs_x = random.sample(range((x.size()[0])), x_range)\n",
        "x[rand_idcs_x] = forget_deck_x[rand_idcs]\n",
        "y[rand_idcs_x] = forget_deck_y[rand_idcs].type(torch.LongTensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eb17c09ef0b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpercent_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpercent_keep\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrand_idcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_deck_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrand_idcs_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idcs_x\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforget_deck_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'forget_deck_x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXmpBouCa4lT"
      },
      "source": [
        "la=model_A(x)\n",
        "\n",
        "\n",
        "J_A = loss_A(la, y.cuda())\n",
        "\n",
        "J_B = loss_B(lb, yprime.cuda())\n",
        "\n",
        "model_A.zero_grad()\n",
        "model_B.zero_grad()\n",
        "\n",
        "J_A.backward()\n",
        "J_B.backward()\n",
        "\n",
        "optimizer_A.step()\n",
        "optimizer_B.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "so5HwFUwb12v",
        "outputId": "782fb5da-9392-4a98-e134-2dc02c7f8703"
      },
      "source": [
        "torch.tensor(blah).size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1bc59e966bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblah\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSa2fveN6_Q",
        "outputId": "b055c90f-087c-4bdb-9d5b-7b485800df50"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "nb_epochs = 60\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "\n",
        "batch_tracker = 0\n",
        "forget_thres = 4 #if an example is forgotten more than 5 times, we will add it to the pile\n",
        "percent_keep = 20 #replace 20%\n",
        "\n",
        "forget_deck_x = torch.empty((0, 3,32,32)).cuda()\n",
        "forget_deck_y = torch.empty((0)).cuda()\n",
        "\n",
        "acc_A_global = list()\n",
        "acc_B_global = list()\n",
        "acc_A_unmodified_global = list()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "    accuracies_A_unmodified = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        #y = y.cuda()\n",
        "\n",
        "        \n",
        "        y_prime = y.clone().detach() #save original targets\n",
        "        x_prime = x.clone().detach() #and the original images\n",
        "\n",
        "        x_mod = x.clone().detach()\n",
        "        y_mod = y.clone().detach()\n",
        "\n",
        "        #replace some of x and y with those from forgotten pile\n",
        "        #only do this after training for a bit\n",
        "\n",
        "        if epoch > 10 and forget_deck_x.size()[0] >= x.size()[0]:\n",
        "            #deck_range = (forget_deck_x.size()[0]) * percent_keep // 100\n",
        "            x_range = (x.size()[0]) * percent_keep // 100\n",
        "            rand_idcs = random.sample(range((forget_deck_x.size()[0])), x_range)\n",
        "            rand_idcs_x = random.sample(range((x.size()[0])), x_range)\n",
        "            x_mod[rand_idcs_x] = forget_deck_x[rand_idcs]\n",
        "            y_mod[rand_idcs_x] = forget_deck_y[rand_idcs].type(torch.LongTensor)\n",
        "            #print(f\"Shuffling in forgotten examples... at positions {rand_idcs_x} from {rand_idcs} \\n\")\n",
        "\n",
        "        l_A = model_A(x_mod)\n",
        "        l_B = model_B(x_prime)\n",
        "        with torch.no_grad():\n",
        "            l_A_unmod = model_A(x_prime)\n",
        "\n",
        "        \n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y_mod[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            if torch.sum(forget_matrix_A, 0)[batch_tracker, k] > forget_thres and forget_deck_x.size()[0] < 2000:\n",
        "                forget_deck_x = torch.cat((forget_deck_x, x_prime[k].unsqueeze(0)), 0)\n",
        "                y_prime = y_prime.cuda()\n",
        "                forget_deck_y = torch.cat((forget_deck_y, y_prime[k].unsqueeze(0)), 0)\n",
        "                y_prime = y_prime.cpu()\n",
        "                #print(f\"Added to forget deck! Size: {forget_deck_x.size()} \\n\")\n",
        "                #and add this example to forgotten list\n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "\n",
        "        #check if model B's prediction is correctly classified\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y_prime[k]: #if it is, mark as '1'\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y_mod.cuda())\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_B = loss_B(l_B, y_prime.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "        J_B.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        accuracies_A.append(y_mod.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_B.append(y_prime.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        accuracies_A_unmodified.append(y_prime.eq(l_A_unmod.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A on unmodified set: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_A_unmodified).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "    acc_B_global.append(torch.tensor(accuracies_B).mean())\n",
        "    acc_A_unmodified_global.append(torch.tensor(accuracies_A_unmodified).mean())\n",
        "\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A, B: 2.18 , 2.04 \n",
            "\n",
            "Training accuracy for A, B: 0.20 , 0.24 \n",
            "\n",
            "Epoch 2, train loss A, B: 1.87 , 1.78 \n",
            "\n",
            "Training accuracy for A, B: 0.31 , 0.33 \n",
            "\n",
            "Epoch 3, train loss A, B: 1.71 , 1.65 \n",
            "\n",
            "Training accuracy for A, B: 0.36 , 0.38 \n",
            "\n",
            "Epoch 4, train loss A, B: 1.61 , 1.56 \n",
            "\n",
            "Training accuracy for A, B: 0.40 , 0.42 \n",
            "\n",
            "Epoch 5, train loss A, B: 1.54 , 1.50 \n",
            "\n",
            "Training accuracy for A, B: 0.43 , 0.44 \n",
            "\n",
            "Epoch 6, train loss A, B: 1.48 , 1.45 \n",
            "\n",
            "Training accuracy for A, B: 0.45 , 0.47 \n",
            "\n",
            "Epoch 7, train loss A, B: 1.43 , 1.40 \n",
            "\n",
            "Training accuracy for A, B: 0.47 , 0.49 \n",
            "\n",
            "Epoch 8, train loss A, B: 1.39 , 1.35 \n",
            "\n",
            "Training accuracy for A, B: 0.49 , 0.51 \n",
            "\n",
            "Epoch 9, train loss A, B: 1.35 , 1.30 \n",
            "\n",
            "Training accuracy for A, B: 0.51 , 0.53 \n",
            "\n",
            "Epoch 10, train loss A, B: 1.30 , 1.27 \n",
            "\n",
            "Training accuracy for A, B: 0.53 , 0.54 \n",
            "\n",
            "Epoch 11, train loss A, B: 1.26 , 1.23 \n",
            "\n",
            "Training accuracy for A, B: 0.54 , 0.55 \n",
            "\n",
            "Epoch 12, train loss A, B: 1.23 , 1.19 \n",
            "\n",
            "Training accuracy for A, B: 0.55 , 0.57 \n",
            "\n",
            "Epoch 13, train loss A, B: 1.20 , 1.16 \n",
            "\n",
            "Training accuracy for A, B: 0.57 , 0.58 \n",
            "\n",
            "Epoch 14, train loss A, B: 1.11 , 1.13 \n",
            "\n",
            "Training accuracy for A, B: 0.60 , 0.59 \n",
            "\n",
            "Epoch 15, train loss A, B: 1.08 , 1.10 \n",
            "\n",
            "Training accuracy for A, B: 0.62 , 0.60 \n",
            "\n",
            "Epoch 16, train loss A, B: 1.07 , 1.08 \n",
            "\n",
            "Training accuracy for A, B: 0.62 , 0.61 \n",
            "\n",
            "Epoch 17, train loss A, B: 1.07 , 1.05 \n",
            "\n",
            "Training accuracy for A, B: 0.61 , 0.62 \n",
            "\n",
            "Epoch 18, train loss A, B: 1.04 , 1.03 \n",
            "\n",
            "Training accuracy for A, B: 0.63 , 0.63 \n",
            "\n",
            "Epoch 19, train loss A, B: 1.02 , 1.01 \n",
            "\n",
            "Training accuracy for A, B: 0.64 , 0.64 \n",
            "\n",
            "Epoch 20, train loss A, B: 1.00 , 0.98 \n",
            "\n",
            "Training accuracy for A, B: 0.65 , 0.65 \n",
            "\n",
            "Epoch 21, train loss A, B: 0.97 , 0.97 \n",
            "\n",
            "Training accuracy for A, B: 0.66 , 0.65 \n",
            "\n",
            "Epoch 22, train loss A, B: 0.96 , 0.95 \n",
            "\n",
            "Training accuracy for A, B: 0.67 , 0.66 \n",
            "\n",
            "Epoch 23, train loss A, B: 0.94 , 0.94 \n",
            "\n",
            "Training accuracy for A, B: 0.67 , 0.66 \n",
            "\n",
            "Epoch 24, train loss A, B: 0.92 , 0.92 \n",
            "\n",
            "Training accuracy for A, B: 0.68 , 0.67 \n",
            "\n",
            "Epoch 25, train loss A, B: 0.91 , 0.91 \n",
            "\n",
            "Training accuracy for A, B: 0.69 , 0.67 \n",
            "\n",
            "Epoch 26, train loss A, B: 0.89 , 0.90 \n",
            "\n",
            "Training accuracy for A, B: 0.69 , 0.68 \n",
            "\n",
            "Epoch 27, train loss A, B: 0.88 , 0.88 \n",
            "\n",
            "Training accuracy for A, B: 0.70 , 0.68 \n",
            "\n",
            "Epoch 28, train loss A, B: 0.86 , 0.87 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.69 \n",
            "\n",
            "Epoch 29, train loss A, B: 0.85 , 0.86 \n",
            "\n",
            "Training accuracy for A, B: 0.71 , 0.69 \n",
            "\n",
            "Epoch 30, train loss A, B: 0.84 , 0.85 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.70 \n",
            "\n",
            "Epoch 31, train loss A, B: 0.82 , 0.84 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.70 \n",
            "\n",
            "Epoch 32, train loss A, B: 0.82 , 0.83 \n",
            "\n",
            "Training accuracy for A, B: 0.72 , 0.70 \n",
            "\n",
            "Epoch 33, train loss A, B: 0.80 , 0.82 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.71 \n",
            "\n",
            "Epoch 34, train loss A, B: 0.80 , 0.81 \n",
            "\n",
            "Training accuracy for A, B: 0.73 , 0.71 \n",
            "\n",
            "Epoch 35, train loss A, B: 0.79 , 0.80 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.72 \n",
            "\n",
            "Epoch 36, train loss A, B: 0.78 , 0.79 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.72 \n",
            "\n",
            "Epoch 37, train loss A, B: 0.76 , 0.78 \n",
            "\n",
            "Training accuracy for A, B: 0.74 , 0.72 \n",
            "\n",
            "Epoch 38, train loss A, B: 0.76 , 0.77 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.73 \n",
            "\n",
            "Epoch 39, train loss A, B: 0.74 , 0.77 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.73 \n",
            "\n",
            "Epoch 40, train loss A, B: 0.74 , 0.76 \n",
            "\n",
            "Training accuracy for A, B: 0.75 , 0.73 \n",
            "\n",
            "Epoch 41, train loss A, B: 0.73 , 0.75 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.73 \n",
            "\n",
            "Epoch 42, train loss A, B: 0.72 , 0.74 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.74 \n",
            "\n",
            "Epoch 43, train loss A, B: 0.71 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.76 , 0.74 \n",
            "\n",
            "Epoch 44, train loss A, B: 0.70 , 0.73 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.74 \n",
            "\n",
            "Epoch 45, train loss A, B: 0.70 , 0.72 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.75 \n",
            "\n",
            "Epoch 46, train loss A, B: 0.69 , 0.71 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.75 \n",
            "\n",
            "Epoch 47, train loss A, B: 0.68 , 0.70 \n",
            "\n",
            "Training accuracy for A, B: 0.77 , 0.75 \n",
            "\n",
            "Epoch 48, train loss A, B: 0.68 , 0.69 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.75 \n",
            "\n",
            "Epoch 49, train loss A, B: 0.67 , 0.69 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.76 \n",
            "\n",
            "Epoch 50, train loss A, B: 0.66 , 0.68 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.76 \n",
            "\n",
            "Epoch 51, train loss A, B: 0.65 , 0.68 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.76 \n",
            "\n",
            "Epoch 52, train loss A, B: 0.65 , 0.67 \n",
            "\n",
            "Training accuracy for A, B: 0.78 , 0.77 \n",
            "\n",
            "Epoch 53, train loss A, B: 0.64 , 0.66 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.77 \n",
            "\n",
            "Epoch 54, train loss A, B: 0.64 , 0.66 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.77 \n",
            "\n",
            "Epoch 55, train loss A, B: 0.63 , 0.65 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.77 \n",
            "\n",
            "Epoch 56, train loss A, B: 0.62 , 0.65 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.77 \n",
            "\n",
            "Epoch 57, train loss A, B: 0.62 , 0.64 \n",
            "\n",
            "Training accuracy for A, B: 0.79 , 0.78 \n",
            "\n",
            "Epoch 58, train loss A, B: 0.61 , 0.63 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.78 \n",
            "\n",
            "Epoch 59, train loss A, B: 0.61 , 0.63 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.78 \n",
            "\n",
            "Epoch 60, train loss A, B: 0.60 , 0.62 \n",
            "\n",
            "Training accuracy for A, B: 0.80 , 0.78 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s34DXhO0m5XT"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUn9xlUI34Y6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "de239775-fe4f-4bc7-b5c7-396c5a9427fc"
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-12083fc9ba22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cxHCSZR6OQvF",
        "outputId": "5fa1d053-8aad-4d80-b5a4-c666ff355df0"
      },
      "source": [
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], acc_A_global[1:], c =\"blue\", label = \"model A on modified set\")\n",
        "plt.scatter(eplist[1:], acc_A_unmodified_global[1:], c =\"green\", label = \"model A on original set\")\n",
        "plt.scatter(eplist[1:], acc_B_global[1:], c =\"red\", label = \"model B\")\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"train accuracy\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0030cd5d26f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meplist\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meplist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_A_global\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model A on modified set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meplist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_A_unmodified_global\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model A on original set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meplist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_B_global\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nb_epochs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcqU4CiXNZmY"
      },
      "source": [
        "Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCelFCwUNa4r"
      },
      "source": [
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = False\n",
        ")\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "test_accuracy_A = list()\n",
        "test_accuracy_B = list()\n",
        "\n",
        "batch_counter = 0\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l_A = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "        \n",
        "    test_accuracy_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    test_accuracy_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    if batch_counter%10 == 0:\n",
        "        print(f\"Test accuracy A: {torch.tensor(test_accuracy_A).mean():.2f} \\n\")\n",
        "        print(f\"Test accuracy B: {torch.tensor(test_accuracy_B).mean():.2f} \\n\")\n",
        "    \n",
        "    batch_counter+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs_NMvX85kZM"
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RoPkR6ogSjT"
      },
      "source": [
        "import numpy as np\n",
        "cumulative_forgetting_A = torch.sum(forget_matrix_A, 0)\n",
        "cumulative_forgetting_B = torch.sum(forget_matrix_B, 0)\n",
        "\n",
        "forgetlen_A = len(torch.flatten(cumulative_forgetting_A))\n",
        "forgetlen_B = len(torch.flatten(cumulative_forgetting_B))\n",
        "\n",
        "hist_A = plt.hist(torch.flatten(cumulative_forgetting_A), alpha=0.5, label = \"Model A\", weights = np.ones(forgetlen_A)/forgetlen_A)\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B), alpha=0.5, label = \"Model B\", weights = np.ones(forgetlen_B)/forgetlen_B)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ6SjTDk6G6Z"
      },
      "source": [
        "forget_fcn_epoch_A = list()\n",
        "forget_fcn_epoch_B = list()\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    forget_fcn_epoch_A.append(torch.sum(torch.flatten(forget_matrix_A[i]),0))\n",
        "    forget_fcn_epoch_B.append(torch.sum(torch.flatten(forget_matrix_B[i]),0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mei1voSz6Jr1"
      },
      "source": [
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbPm7DoT5qnY"
      },
      "source": [
        "torch.sum(torch.flatten(cumulative_forgetting_A))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-VB_o3r5t5o"
      },
      "source": [
        "torch.sum(torch.flatten(cumulative_forgetting_B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO-oq4RrP5C5"
      },
      "source": [
        "## 15% with higher threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cq-SWohP8uN"
      },
      "source": [
        "#First load models, dataset\n",
        "#make sure to restart runtime\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.CrossEntropyLoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FjO0AM84r0N"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "nb_epochs = 60\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "\n",
        "batch_tracker = 0\n",
        "forget_thres = 6 #if an example is forgotten more than 6 times, we will add it to the pile\n",
        "percent_keep = 15 #replace 15%\n",
        "\n",
        "forget_deck_x = torch.empty((0, 3,32,32)).cuda()\n",
        "forget_deck_y = torch.empty((0)).cuda()\n",
        "\n",
        "acc_A_global = list()\n",
        "acc_B_global = list()\n",
        "acc_A_unmodified_global = list()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "    accuracies_A_unmodified = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        #y = y.cuda()\n",
        "\n",
        "        \n",
        "        y_prime = y.clone().detach() #save original targets\n",
        "        x_prime = x.clone().detach() #and the original images\n",
        "\n",
        "        x_mod = x.clone().detach()\n",
        "        y_mod = y.clone().detach()\n",
        "\n",
        "        #replace some of x and y with those from forgotten pile\n",
        "        #only do this after training for a bit\n",
        "\n",
        "        if epoch > 10 and forget_deck_x.size()[0] >= x.size()[0]:\n",
        "            #deck_range = (forget_deck_x.size()[0]) * percent_keep // 100\n",
        "            x_range = (x.size()[0]) * percent_keep // 100\n",
        "            rand_idcs = random.sample(range((forget_deck_x.size()[0])), x_range)\n",
        "            rand_idcs_x = random.sample(range((x.size()[0])), x_range)\n",
        "            x_mod[rand_idcs_x] = forget_deck_x[rand_idcs]\n",
        "            y_mod[rand_idcs_x] = forget_deck_y[rand_idcs].type(torch.LongTensor)\n",
        "            #print(f\"Shuffling in forgotten examples... at positions {rand_idcs_x} from {rand_idcs} \\n\")\n",
        "\n",
        "        l_A = model_A(x_mod)\n",
        "        l_B = model_B(x_prime)\n",
        "        with torch.no_grad():\n",
        "            l_A_unmod = model_A(x_prime)\n",
        "\n",
        "        \n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y_mod[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            if torch.sum(forget_matrix_A, 0)[batch_tracker, k] > forget_thres and forget_deck_x.size()[0] < 3000:\n",
        "                forget_deck_x = torch.cat((forget_deck_x, x_prime[k].unsqueeze(0)), 0)\n",
        "                y_prime = y_prime.cuda()\n",
        "                forget_deck_y = torch.cat((forget_deck_y, y_prime[k].unsqueeze(0)), 0)\n",
        "                y_prime = y_prime.cpu()\n",
        "                #print(f\"Added to forget deck! Size: {forget_deck_x.size()} \\n\")\n",
        "                #and add this example to forgotten list\n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "\n",
        "        #check if model B's prediction is correctly classified\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y_prime[k]: #if it is, mark as '1'\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y_mod.cuda())\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_B = loss_B(l_B, y_prime.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "        J_B.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        accuracies_A.append(y_mod.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_B.append(y_prime.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        accuracies_A_unmodified.append(y_prime.eq(l_A_unmod.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B, A on unmod set: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f}, {torch.tensor(accuracies_A_unmodified).mean():.2f} \\n\")\n",
        "    #print(f\"Training accuracy for A on unmodified set: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_A_unmodified).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "    acc_B_global.append(torch.tensor(accuracies_B).mean())\n",
        "    acc_A_unmodified_global.append(torch.tensor(accuracies_A_unmodified).mean())\n",
        "\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XirypQdzIt_B"
      },
      "source": [
        "Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvt6RNYZItf4"
      },
      "source": [
        "test_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = False\n",
        ")\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "test_accuracy_A = list()\n",
        "test_accuracy_B = list()\n",
        "\n",
        "batch_counter = 0\n",
        "for batch in test_set:\n",
        "    x, y = batch\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l_A = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "        \n",
        "    test_accuracy_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    test_accuracy_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "    \n",
        "    if batch_counter%10 == 0:\n",
        "        print(f\"Test accuracy A: {torch.tensor(test_accuracy_A).mean():.2f} \\n\")\n",
        "        print(f\"Test accuracy B: {torch.tensor(test_accuracy_B).mean():.2f} \\n\")\n",
        "    \n",
        "    batch_counter+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSsOxYO_I4Yo"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL6iOl8DI5vo"
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "hM_jzt9qI-F0",
        "outputId": "6adf3f1f-d51f-4d74-88f1-5f4b5ca68c21"
      },
      "source": [
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], acc_A_global[1:], c =\"blue\", label = \"model A on modified set\")\n",
        "plt.scatter(eplist[1:], acc_A_unmodified_global[1:], c =\"green\", label = \"model A on original set\")\n",
        "plt.scatter(eplist[1:], acc_B_global[1:], c =\"red\", label = \"model B\")\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"train accuracy\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "EodeZF13JLHO",
        "outputId": "9330a35d-419c-4707-9766-bf4985026552"
      },
      "source": [
        "import numpy as np\n",
        "cumulative_forgetting_A = torch.sum(forget_matrix_A, 0)\n",
        "cumulative_forgetting_B = torch.sum(forget_matrix_B, 0)\n",
        "\n",
        "forgetlen_A = len(torch.flatten(cumulative_forgetting_A))\n",
        "forgetlen_B = len(torch.flatten(cumulative_forgetting_B))\n",
        "\n",
        "hist_A = plt.hist(torch.flatten(cumulative_forgetting_A), alpha=0.5, label = \"Model A\", weights = np.ones(forgetlen_A)/forgetlen_A)\n",
        "hist_B = plt.hist(torch.flatten(cumulative_forgetting_B), alpha=0.5, label = \"Model B\", weights = np.ones(forgetlen_B)/forgetlen_B)\n",
        "plt.ylabel('Fraction of total events')\n",
        "plt.xlabel('Number of forgetting events')\n",
        "plt.text(20, .14, r'CIFAR10, n_epochs = 75')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "TQVoNzu3JQ5e",
        "outputId": "05e0ed0a-615e-46f8-dab1-9c6132e484ff"
      },
      "source": [
        "forget_fcn_epoch_A = list()\n",
        "forget_fcn_epoch_B = list()\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    forget_fcn_epoch_A.append(torch.sum(torch.flatten(forget_matrix_A[i]),0))\n",
        "    forget_fcn_epoch_B.append(torch.sum(torch.flatten(forget_matrix_B[i]),0))\n",
        "\n",
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4mPZRXAMP85"
      },
      "source": [
        "## Measure default "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw5qS8XYMl__",
        "outputId": "36b0fda6-7c67-4e08-e6ad-72390fbe2314"
      },
      "source": [
        "#First load models, dataset\n",
        "#make sure to restart runtime\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.CrossEntropyLoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'open_lth' already exists and is not an empty directory.\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "keHgguWrMVG9",
        "outputId": "bbc0206f-ae4b-4e44-db9a-8789d8065e50"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "nb_epochs = 30\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "\n",
        "batch_tracker = 0\n",
        "forget_thres = 4 #if an example is forgotten more than 5 times, we will add it to the pile\n",
        "percent_keep = 20 #replace 20%\n",
        "\n",
        "forget_deck_x = torch.empty((0, 3,32,32)).cuda()\n",
        "forget_deck_y = torch.empty((0)).cuda()\n",
        "\n",
        "acc_A_global = list()\n",
        "acc_B_global = list()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        #y = y.cuda()\n",
        "\n",
        "        \n",
        "        y_prime = y #save original targets\n",
        "        x_prime = x #and the original images\n",
        "\n",
        "        x_mod = x\n",
        "        y_mod = y\n",
        "\n",
        "        #replace some of x and y with those from forgotten pile\n",
        "        #only do this after training for a bit\n",
        "\n",
        "        l_A = model_A(x_mod)\n",
        "        l_B = model_B(x_prime)\n",
        "\n",
        "        \n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y_mod[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "\n",
        "        #check if model B's prediction is correctly classified\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y_prime[k]: #if it is, mark as '1'\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y_mod.cuda())\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_B = loss_B(l_B, y_prime.cuda())\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward()\n",
        "        J_B.backward()\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        accuracies_A.append(y_mod.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_B.append(y_prime.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "    acc_B_global.append(torch.tensor(accuracies_B).mean())\n",
        "\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A, B: 2.14 , 2.11 \n",
            "\n",
            "Training accuracy for A, B: 0.21 , 0.23 \n",
            "\n",
            "Epoch 2, train loss A, B: 1.82 , 1.82 \n",
            "\n",
            "Training accuracy for A, B: 0.32 , 0.32 \n",
            "\n",
            "Epoch 3, train loss A, B: 1.70 , 1.73 \n",
            "\n",
            "Training accuracy for A, B: 0.36 , 0.35 \n",
            "\n",
            "Epoch 4, train loss A, B: 1.62 , 1.65 \n",
            "\n",
            "Training accuracy for A, B: 0.39 , 0.38 \n",
            "\n",
            "Epoch 5, train loss A, B: 1.55 , 1.58 \n",
            "\n",
            "Training accuracy for A, B: 0.42 , 0.41 \n",
            "\n",
            "Epoch 6, train loss A, B: 1.49 , 1.52 \n",
            "\n",
            "Training accuracy for A, B: 0.45 , 0.44 \n",
            "\n",
            "Epoch 7, train loss A, B: 1.43 , 1.46 \n",
            "\n",
            "Training accuracy for A, B: 0.47 , 0.46 \n",
            "\n",
            "Epoch 8, train loss A, B: 1.39 , 1.41 \n",
            "\n",
            "Training accuracy for A, B: 0.49 , 0.48 \n",
            "\n",
            "Epoch 9, train loss A, B: 1.34 , 1.37 \n",
            "\n",
            "Training accuracy for A, B: 0.51 , 0.50 \n",
            "\n",
            "Epoch 10, train loss A, B: 1.31 , 1.33 \n",
            "\n",
            "Training accuracy for A, B: 0.52 , 0.51 \n",
            "\n",
            "Epoch 11, train loss A, B: 1.27 , 1.29 \n",
            "\n",
            "Training accuracy for A, B: 0.54 , 0.53 \n",
            "\n",
            "Epoch 12, train loss A, B: 1.24 , 1.26 \n",
            "\n",
            "Training accuracy for A, B: 0.55 , 0.55 \n",
            "\n",
            "Epoch 13, train loss A, B: 1.21 , 1.23 \n",
            "\n",
            "Training accuracy for A, B: 0.56 , 0.56 \n",
            "\n",
            "Epoch 14, train loss A, B: 1.18 , 1.20 \n",
            "\n",
            "Training accuracy for A, B: 0.57 , 0.57 \n",
            "\n",
            "Epoch 15, train loss A, B: 1.15 , 1.17 \n",
            "\n",
            "Training accuracy for A, B: 0.58 , 0.58 \n",
            "\n",
            "Epoch 16, train loss A, B: 1.13 , 1.15 \n",
            "\n",
            "Training accuracy for A, B: 0.59 , 0.59 \n",
            "\n",
            "Epoch 17, train loss A, B: 1.11 , 1.12 \n",
            "\n",
            "Training accuracy for A, B: 0.60 , 0.59 \n",
            "\n",
            "Epoch 18, train loss A, B: 1.09 , 1.10 \n",
            "\n",
            "Training accuracy for A, B: 0.61 , 0.60 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3fbf611630a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m#6. monitor loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mlosses_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0maccuracies_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzOzfwp3Mu9G"
      },
      "source": [
        "forget_fcn_epoch_A = list()\n",
        "forget_fcn_epoch_B = list()\n",
        "\n",
        "for i in range(nb_epochs):\n",
        "    forget_fcn_epoch_A.append(torch.sum(torch.flatten(forget_matrix_A[i]),0))\n",
        "    forget_fcn_epoch_B.append(torch.sum(torch.flatten(forget_matrix_B[i]),0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "9KhhoUrGMxkF",
        "outputId": "2381cb57-8e64-4031-8597-2f11510f50a5"
      },
      "source": [
        "import numpy as np\n",
        "eplist= np.arange(nb_epochs)\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_A[1:], c =\"blue\", label = \"model A\")\n",
        "plt.scatter(eplist[1:], forget_fcn_epoch_B[1:], c =\"red\", label = \"model B\")\n",
        "#plt.scatter(eplist[1:], forget_fcn_epoch_B_on_A[1:], c =\"green\", label = \"model B on A\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"number of forgetting events\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evNg3ZqdCWEh"
      },
      "source": [
        "# Should we regard forgettable examples as a separate distribution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPw2uBoqDA_X"
      },
      "source": [
        "Is there a sense in which forgotten examples are learned \"correctly\" via generalization and stay learned? We could construct a dataset D' consisting only of the most forgotten examples. We can then, over the course of training, measure how well a model trained on the original dataset D (of which D' is a subset) does in evaluating examples in D' at different stages in training (early, middle, late). If we believe that there's a \"correct\" way to learn these forgotten examples via generalization, then maybe at a later stage in training, the model will basically not fluctuate very much on the classification of these examples. Note that we wouldn't be measuring if these examples are classified correctly, just that if there is a lot of fluctuation in how the model classifies them i.e. is the model mostly settled on what it thinks these examples are, even if it's wrong, or does it basically still oscillate wildly at late times?\n",
        "\n",
        "Similarly, I wonder if the set of forgettable examples D' really should be thought of an entirely separate distribution and the reason the model has such a hard time learning them is that it's somehow trying to reconcile that all of the elements in this distribution should fit into one of the labels in D. What if we relabeled all examples in D' as a new label or set of labels? If we think our flashcard experiments are hinting at the fact that the model basically memorizes forgettable examples, does this mean that we would wind up with 0 forgetting events for a model trained this way? Moreover, it would be interesting to see what such a model would classify as a forgettable example during testing. Are these the examples that a model trained on the original dataset D would misclassify?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnSZHXCdCbBP"
      },
      "source": [
        "#Instead of copying and modifying the same general type of code, let's try and create classes that will\n",
        "#make life a little easier for us. We'll call the first class 'measure' since we want to measure a bunch of different\n",
        "#things.\n",
        "#It should have a few different functions. Some of the things we might want to do are:\n",
        "# -Measure overall forgetting events, given a model, training set, and training time\n",
        "# -Measure train and test accuracies across iterations\n",
        "\n",
        "class measure:\n",
        "\n",
        "\n",
        "# We can make another class to deal with data modifications during training, such as:\n",
        "# -Modify k% examples in a batch based on criteria like whether they have been forgotten X times\n",
        "# -Rewind logits based on how much they have been forgotten\n",
        "\n",
        "class trainingMod:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOPxRBpLciZL"
      },
      "source": [
        "# Adding noise to the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdFmu7b4w0bK"
      },
      "source": [
        "Here we train a model and then damage it by adding noise (= 1+ \\epsilon) to the weights. Dialing epsilon, we can study the order in which examples are forgotten.\n",
        "\n",
        "There are many ways to about doing this. Here's one of them:\n",
        "\n",
        "-- The first step is to determine the examples that are forgotten during training, ranked by how often they are forgotten. We should restrict to the examples that are classified *correctly* at the end of training because we want to see how easily they become forgotten as we damage the network.\n",
        "\n",
        "-- Then, we add noise to the network and determine which examples are then misclassified. For example suppose the noise is drawn from a gaussian of std dev 0.1 and mean 0. We record which examples are misclassified. Then we clone the original model, and set the noise to 0.2. We again record which examples are misclassified. We go on doing this as the noise is dialed to be O(1).\n",
        "\n",
        "-- Next, we can construct two ranked lists: the first list is just the rank ordering of how many times examples are forgotten. The next list is the epsilon (noise level) at which that example was incorrectly classified.\n",
        "\n",
        "-- Finally, we compute the rank correlation between these two. We should average over initializations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDf0Ns8vkCiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "39438d4c06e5441e91b4d8a5e33a8dae",
            "d51a33872b3d49819d32e51627942bf1",
            "0f1dab00d9874020b18fc00639955a20",
            "6076b94f793c4501b6554ba4f326493d",
            "0a7127ab99884f3195c33b1537a4fd78",
            "31ea81610fa64df1bd15b66bd35091c2",
            "d55a2e9ebdd84104b31128fd7b20aefa",
            "9936469fdaa747949307583e96a59fd2"
          ]
        },
        "outputId": "fa1c554a-2d10-4ed0-a894-d9c66e57cd35"
      },
      "source": [
        "#First load models, dataset\n",
        "#make sure to restart runtime\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "#And load the CIFAR10 dataset\n",
        "train_dataset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "train_set = DataLoader(train_dataset, batch_size=128, num_workers = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 4.64 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39438d4c06e5441e91b4d8a5e33a8dae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /cifar-10-python.tar.gz to /\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k7TpEngkLMW"
      },
      "source": [
        "class measureForget:\n",
        "    def __init__(self, nb_epochs, num_batches, batch_size ):\n",
        "        self.forgetStatistics = torch.zeros(nb_epochs, num_batches, batch_size)\n",
        "        self.correctStatistics = torch.zeros(nb_epochs, num_batches, batch_size)\n",
        "        self.a_i = torch.zeros(nb_epochs, num_batches, batch_size) #measures if correctly classified or not\n",
        "        self.softmaxfunc = nn.Softmax(dim=1)\n",
        "        self.train_batch_tracker = 0\n",
        "        self.classify_batch_tracker = 0\n",
        "        self.train_iteration = 0\n",
        "        self.num_ep = nb_epochs\n",
        "        self.num_btchs = num_batches\n",
        "        self.btc_size = batch_size\n",
        "\n",
        "    #def trackForgettableExamples(self, batch_model_output, labels): #track examples check if examples in a batch were correctly classified\n",
        "    #before and aren't classified correctly now (where before and now refer to subsequent training iterations)\n",
        "    def trackForgettableExamples(self, batch_model_output, labels):\n",
        "        if self.train_iteration < 1:\n",
        "            pass\n",
        "        else:\n",
        "            counter = 0\n",
        "            for logit in batch_model_output:\n",
        "                if torch.argmax(logit) == labels[counter]:\n",
        "                    self.a_i[self.train_iteration, self.train_batch_tracker, counter] = 1\n",
        "                else:\n",
        "                    self.a_i[self.train_iteration, self.train_batch_tracker, counter] = 0\n",
        "            \n",
        "                if self.a_i[self.train_iteration, self.train_batch_tracker, counter] < self.a_i[self.train_iteration-1, self.train_batch_tracker, counter]:\n",
        "                    self.forgetStatistics[self.train_iteration, self.train_batch_tracker, counter]+=1\n",
        "            \n",
        "                counter+=1\n",
        "    \n",
        "    def trackCorrectExamples(self, batch_model_output, labels):\n",
        "        for k in range(len(batch_model_output)):\n",
        "            if torch.argmax(batch_model_output[k]) == labels[k]:\n",
        "                self.correctStatistics[self.train_iteration, self.classify_batch_tracker, k] = 1\n",
        "    \n",
        "    def resetCorrectStatistics(self):\n",
        "        self.correctStatistics = torch.zeros(self.num_ep, self.num_btchs, self.btc_size)\n",
        "\n",
        "    def incrementTrainBatch(self):\n",
        "        self.train_batch_tracker+=1\n",
        "    \n",
        "    def incrementClassifyBatch(self):\n",
        "        self.classify_batch_tracker+=1\n",
        "    \n",
        "    def getTrainBatchNumber(self):\n",
        "        return self.train_batch_tracker\n",
        "    \n",
        "    def getClassifyBatchNumber(self):\n",
        "        return self.classify_batch_tracker\n",
        "\n",
        "    def getTrainIteration(self):\n",
        "        return self.train_iteration\n",
        "\n",
        "    def incrementTrainIter(self):\n",
        "        self.train_iteration+=1\n",
        "    \n",
        "    def decrementTrainIter(self):\n",
        "        self.train_iteration-=1\n",
        "\n",
        "    def resetTrainBatchTracker(self):\n",
        "        self.train_batch_tracker=0\n",
        "    \n",
        "    def resetClassifyBatchTracker(self):\n",
        "        self.classify_batch_tracker=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rORvmyjMtVJt"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib.pyplot import figure\n",
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "\n",
        "class processMeasurements:\n",
        "    def __init__(self, forget_msrmt):\n",
        "        self.forget_stats = forget_msrmt.forgetStatistics\n",
        "        self.sum_over_ep_Forget = torch.sum(self.forget_stats, 0)\n",
        "    \n",
        "    def plotForgetHist(self):\n",
        "        length = len(torch.flatten(self.sum_over_ep_Forget))\n",
        "        hist = plt.hist(torch.flatten(self.sum_over_ep_Forget), alpha=0.5, weights = np.ones(length)/length)\n",
        "        plt.ylabel('Fraction of total events')\n",
        "        plt.xlabel('Number of forgetting events')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoURDtU_tlIa"
      },
      "source": [
        "class manageForgetDataset:\n",
        "    def __init__(self, forget_msrmt, forget_thres = 3):\n",
        "        self.forget_stats = forget_msrmt.forgetStatistics\n",
        "        self.correct_stats = forget_msrmt.correctStatistics\n",
        "        self.sum_over_ep_flatten_forget = torch.flatten(torch.sum(self.forget_stats, 0))\n",
        "        self.forget_thres = forget_thres\n",
        "        self.trainset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "        self.forget_mask = list()\n",
        "        self.forget_mask_correct = list()\n",
        "        self.batch_size = self.forget_stats[0,0].size()[0] #infer batch size from input\n",
        "        self.forgotten_correct_stats = list()\n",
        "\n",
        "    #get a mask of most forgotten examples\n",
        "    def getForgetMask(self):\n",
        "        for k in range(len(self.sum_over_ep_flatten_forget)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres:\n",
        "                self.forget_mask.append(k)\n",
        "        return self.forget_mask\n",
        "\n",
        "    #get a mask of most forgotten examples that were classified *correctly* at the end of training\n",
        "    def getForgetMaskCorrect(self, which_epoch = None):\n",
        "        if which_epoch == None:\n",
        "            at_epoch = len(self.correct_stats)\n",
        "        else:\n",
        "            at_epoch = which_epoch\n",
        "    \n",
        "        correct_flat = torch.flatten(self.correct_stats[at_epoch-1])\n",
        "\n",
        "        #note that to add the which_epoch functionality correctly\n",
        "        #I'll have to make sure sum_over_ep_flatten is only summed\n",
        "        #up to which_epoch... can add that in later **TODO!!**\n",
        "        #right now it just assumes we mean the last epoch of training\n",
        "\n",
        "        for k in range(len(correct_flat)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres and correct_flat[k]==1:\n",
        "                self.forget_mask_correct.append(k)\n",
        "                self.forgotten_correct_stats.append(torch.IntTensor.item(self.sum_over_ep_flatten_forget[k]))\n",
        "\n",
        "        return self.forget_mask_correct\n",
        "\n",
        "    def get_forgotten_dataset_correct(self): #return a mask of those examples that were forgotten AND classified correctly\n",
        "        #requires having run getForgetMask() first\n",
        "        train_subset_correct = torch.utils.data.Subset(self.trainset, self.getForgetMaskCorrect())\n",
        "        return torch.utils.data.DataLoader(train_subset_correct, batch_size=self.batch_size, num_workers = 0)\n",
        "\n",
        "    #outputs statistics of correct, forgettable examples\n",
        "    # def get_forgotten_dataset_correct_stats(self):\n",
        "    #     if self.get_num_forgotten_correct()==0:\n",
        "    #         raise ValueError(\"Obtain the mask of forgettable examples first.\")\n",
        "        \n",
        "    #     correct_flat = torch.flatten(self.correct_stats[len(self.correct_stats)-1])\n",
        "\n",
        "    #     _forgotten_correct_stats = list()\n",
        "    #     for k in range(len(correct_flat)):\n",
        "    #         if correct_flat[k] == 1:\n",
        "    #             _forgotten_correct_stats.append(self.sum_over_ep_flatten_forget[k])\n",
        "\n",
        "    #     return _forgotten_correct_stats\n",
        "\n",
        "    def get_num_forgotten(self):\n",
        "        if len(self.forget_mask)==0:\n",
        "            raise ValueError(\"Obtain the mask of forgettable examples first.\")\n",
        "        return len(self.forget_mask)\n",
        "\n",
        "    def get_num_forgotten_correct(self, which_epoch = None):\n",
        "        if which_epoch == None:\n",
        "            return len(self.forget_mask_correct)\n",
        "        else:\n",
        "            return len(self.getForgetMaskCorrect(which_epoch))\n",
        "\n",
        "    def get_forgotten_dataset(self):\n",
        "        train_subset = torch.utils.data.Subset(self.trainset, self.getForgetMask())\n",
        "        return torch.utils.data.DataLoader(train_subset, batch_size=self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d6bYNZRpMmN"
      },
      "source": [
        "class damageModel:\n",
        "    def __init__(self, model):\n",
        "        from foundations import hparams\n",
        "        from models import registry\n",
        "        \n",
        "        model.eval()\n",
        "        self.model_clones = []\n",
        "        self.model_state_dict = model.state_dict()\n",
        "        self.model_hparams = hparams.ModelHparams('cifar_resnet_20', 'kaiming_uniform', 'uniform')\n",
        "\n",
        "    def addNoise(self, num_points = 200, min_noise = 0., max_noise = 0.1): #returns an array of length num_points, consisting of models increasingly damaged from Gaussian noise with stdev min_noise to max_noise\n",
        "        from foundations import hparams\n",
        "        from models import registry\n",
        "\n",
        "        epsilons = np.linspace(min_noise, max_noise, num_points)\n",
        "        for i in range(len(epsilons)):\n",
        "            sys.stdout.write(\"\\r{0}Cloning models...\".format(\"|\"*i))\n",
        "            sys.stdout.flush()\n",
        "            self.model_clones.append(registry.get(self.model_hparams).cuda())\n",
        "            self.model_clones[i].load_state_dict(self.model_state_dict)\n",
        "            \n",
        "\n",
        "        with torch.no_grad():\n",
        "            k = 0\n",
        "            for model in self.model_clones:\n",
        "                for param in model.parameters():\n",
        "                    param.multiply_(1+torch.empty(param.size()).cuda().normal_(mean=0,std=epsilons[k]))\n",
        "                k+=1\n",
        "            \n",
        "        for models in self.model_clones:\n",
        "            models.eval()\n",
        "        \n",
        "        return self.model_clones\n",
        "    \n",
        "    def getEpsilons(self, num_points = 200, min_noise = 0., max_noise = 0.1):\n",
        "        return np.linspace(min_noise, max_noise, num_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6MtPum2qUR7"
      },
      "source": [
        "class postProcess:\n",
        "    def __init__(self, num_examples = None):\n",
        "        #self.catalog = torch.zeros(num_examples)\n",
        "        self.num_examples = num_examples\n",
        "\n",
        "    #measure at which noise level an example that's classified correctly becomes misclassifed\n",
        "    #this function just classifies a dataset given a model\n",
        "\n",
        "    def classifyDataset(self, data_loader, models):\n",
        "        if self.num_examples==None:\n",
        "            raise ValueError(\"Specify the size of the dataset please.\")\n",
        "\n",
        "        num_models = len(models)\n",
        "        __catalog = torch.zeros(num_models, self.num_examples)\n",
        "\n",
        "        num_ex_per_batch = list()\n",
        "        for batch in data_loader:\n",
        "            num_ex_per_batch.append(len(batch[0]))\n",
        "        #print(num_ex_per_batch)\n",
        "        \n",
        "        modeltrcker = 0\n",
        "        \n",
        "        for model in models:\n",
        "            model.eval()\n",
        "\n",
        "            btrkcer = 0\n",
        "            for batch in data_loader:\n",
        "                x,y = batch\n",
        "                x=x.cuda()\n",
        "                with torch.no_grad():\n",
        "                    l_A = model(x)\n",
        "                for k in range(len(l_A)):\n",
        "                    if torch.argmax(l_A[k]) == y.cuda()[k]:\n",
        "                        #print(f\"{modeltrcker}, {k+sum(num_ex_per_batch[0:btrkcer])}\")\n",
        "                        __catalog[modeltrcker, k+sum(num_ex_per_batch[0:btrkcer])] = 1\n",
        "                btrkcer+=1        \n",
        "                \n",
        "            modeltrcker+=1\n",
        "\n",
        "        return __catalog\n",
        "    \n",
        "    #returns a table consisting of {epsilon at which example was forgotten, times it was forgotten}\n",
        "    def tabulateNoiseForget(self, catalog, epsilonList, forgetStats):\n",
        "        epsilonForgotten = list()\n",
        "        timesForgotten = list()\n",
        "\n",
        "        for k in range(len(catalog[0])): #go through each example\n",
        "            idx = next((i for i in range(len(catalog[0:,k])) if catalog[0:,k][i] == 0), None)\n",
        "            if idx != None:\n",
        "                epsilonForgotten.append(epsilonList[idx])\n",
        "                timesForgotten.append(forgetStats[k])\n",
        "\n",
        "\n",
        "        # for k in range(len(catalog[0])): #go through each example\n",
        "        #     for j in range(len(catalog[0:,k])): #look at how that example was classified amongst the models\n",
        "        #         if catalog[0:,k][j]==0:\n",
        "        #             epsilonForgotten.append(epsilonList[j])\n",
        "        #             timesForgotten.append(forgetStats[k])\n",
        "        #             continue\n",
        "        \n",
        "        return epsilonForgotten, timesForgotten\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M6mRWLi0JMk",
        "outputId": "73ae2f2e-e2bc-4901-96ed-8da21f0c8d61"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "nb_epochs = 70\n",
        "\n",
        "model_A_msrments = measureForget(nb_epochs, num_batches = len(train_set), batch_size=128)\n",
        "\n",
        "acc_A_global = list()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "\n",
        "    model_A.train()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        l_A = model_A(x)\n",
        "\n",
        "\n",
        "        model_A_msrments.trackForgettableExamples(l_A.detach(), y.detach())\n",
        "\n",
        "        J_A = loss_A(l_A, y.cuda())\n",
        "\n",
        "        model_A.zero_grad()\n",
        "\n",
        "        J_A.backward()\n",
        "\n",
        "        optimizer_A.step()\n",
        "\n",
        "        losses_A.append(J_A.item())\n",
        "        accuracies_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        model_A_msrments.incrementTrainBatch()\n",
        "    \n",
        "    model_A_msrments.resetTrainBatchTracker()\n",
        "\n",
        "    model_A.eval()\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        with torch.no_grad():\n",
        "            l_Ap = model_A(x.detach())\n",
        "        \n",
        "        model_A_msrments.trackCorrectExamples(l_Ap.detach(), y.detach())\n",
        "        model_A_msrments.incrementClassifyBatch()\n",
        "    \n",
        "    model_A_msrments.resetClassifyBatchTracker()\n",
        "\n",
        "    model_A_msrments.incrementTrainIter()\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Epoch {model_A_msrments.getTrainIteration()}, train loss A: {torch.tensor(losses_A).mean():.2f} accuracy A: {torch.tensor(accuracies_A).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A: 2.04 accuracy A: 0.25 \n",
            "\n",
            "Epoch 2, train loss A: 1.72 accuracy A: 0.36 \n",
            "\n",
            "Epoch 3, train loss A: 1.58 accuracy A: 0.42 \n",
            "\n",
            "Epoch 4, train loss A: 1.49 accuracy A: 0.45 \n",
            "\n",
            "Epoch 5, train loss A: 1.42 accuracy A: 0.48 \n",
            "\n",
            "Epoch 6, train loss A: 1.35 accuracy A: 0.51 \n",
            "\n",
            "Epoch 7, train loss A: 1.30 accuracy A: 0.53 \n",
            "\n",
            "Epoch 8, train loss A: 1.25 accuracy A: 0.55 \n",
            "\n",
            "Epoch 9, train loss A: 1.20 accuracy A: 0.57 \n",
            "\n",
            "Epoch 10, train loss A: 1.16 accuracy A: 0.59 \n",
            "\n",
            "Epoch 11, train loss A: 1.12 accuracy A: 0.60 \n",
            "\n",
            "Epoch 12, train loss A: 1.08 accuracy A: 0.61 \n",
            "\n",
            "Epoch 13, train loss A: 1.04 accuracy A: 0.63 \n",
            "\n",
            "Epoch 14, train loss A: 1.01 accuracy A: 0.64 \n",
            "\n",
            "Epoch 15, train loss A: 0.99 accuracy A: 0.65 \n",
            "\n",
            "Epoch 16, train loss A: 0.96 accuracy A: 0.66 \n",
            "\n",
            "Epoch 17, train loss A: 0.93 accuracy A: 0.67 \n",
            "\n",
            "Epoch 18, train loss A: 0.91 accuracy A: 0.67 \n",
            "\n",
            "Epoch 19, train loss A: 0.89 accuracy A: 0.68 \n",
            "\n",
            "Epoch 20, train loss A: 0.87 accuracy A: 0.69 \n",
            "\n",
            "Epoch 21, train loss A: 0.85 accuracy A: 0.70 \n",
            "\n",
            "Epoch 22, train loss A: 0.83 accuracy A: 0.71 \n",
            "\n",
            "Epoch 23, train loss A: 0.81 accuracy A: 0.71 \n",
            "\n",
            "Epoch 24, train loss A: 0.79 accuracy A: 0.72 \n",
            "\n",
            "Epoch 25, train loss A: 0.78 accuracy A: 0.72 \n",
            "\n",
            "Epoch 26, train loss A: 0.76 accuracy A: 0.73 \n",
            "\n",
            "Epoch 27, train loss A: 0.74 accuracy A: 0.74 \n",
            "\n",
            "Epoch 28, train loss A: 0.73 accuracy A: 0.74 \n",
            "\n",
            "Epoch 29, train loss A: 0.71 accuracy A: 0.75 \n",
            "\n",
            "Epoch 30, train loss A: 0.69 accuracy A: 0.76 \n",
            "\n",
            "Epoch 31, train loss A: 0.68 accuracy A: 0.76 \n",
            "\n",
            "Epoch 32, train loss A: 0.66 accuracy A: 0.77 \n",
            "\n",
            "Epoch 33, train loss A: 0.65 accuracy A: 0.77 \n",
            "\n",
            "Epoch 34, train loss A: 0.63 accuracy A: 0.78 \n",
            "\n",
            "Epoch 35, train loss A: 0.62 accuracy A: 0.79 \n",
            "\n",
            "Epoch 36, train loss A: 0.60 accuracy A: 0.79 \n",
            "\n",
            "Epoch 37, train loss A: 0.58 accuracy A: 0.80 \n",
            "\n",
            "Epoch 38, train loss A: 0.57 accuracy A: 0.80 \n",
            "\n",
            "Epoch 39, train loss A: 0.55 accuracy A: 0.81 \n",
            "\n",
            "Epoch 40, train loss A: 0.53 accuracy A: 0.82 \n",
            "\n",
            "Epoch 41, train loss A: 0.52 accuracy A: 0.82 \n",
            "\n",
            "Epoch 42, train loss A: 0.50 accuracy A: 0.83 \n",
            "\n",
            "Epoch 43, train loss A: 0.49 accuracy A: 0.83 \n",
            "\n",
            "Epoch 44, train loss A: 0.48 accuracy A: 0.84 \n",
            "\n",
            "Epoch 45, train loss A: 0.46 accuracy A: 0.84 \n",
            "\n",
            "Epoch 46, train loss A: 0.45 accuracy A: 0.85 \n",
            "\n",
            "Epoch 47, train loss A: 0.43 accuracy A: 0.86 \n",
            "\n",
            "Epoch 48, train loss A: 0.42 accuracy A: 0.86 \n",
            "\n",
            "Epoch 49, train loss A: 0.41 accuracy A: 0.86 \n",
            "\n",
            "Epoch 50, train loss A: 0.40 accuracy A: 0.86 \n",
            "\n",
            "Epoch 51, train loss A: 0.40 accuracy A: 0.86 \n",
            "\n",
            "Epoch 52, train loss A: 0.39 accuracy A: 0.87 \n",
            "\n",
            "Epoch 53, train loss A: 0.38 accuracy A: 0.87 \n",
            "\n",
            "Epoch 54, train loss A: 0.37 accuracy A: 0.87 \n",
            "\n",
            "Epoch 55, train loss A: 0.36 accuracy A: 0.87 \n",
            "\n",
            "Epoch 56, train loss A: 0.35 accuracy A: 0.88 \n",
            "\n",
            "Epoch 57, train loss A: 0.33 accuracy A: 0.89 \n",
            "\n",
            "Epoch 58, train loss A: 0.32 accuracy A: 0.89 \n",
            "\n",
            "Epoch 59, train loss A: 0.30 accuracy A: 0.90 \n",
            "\n",
            "Epoch 60, train loss A: 0.29 accuracy A: 0.91 \n",
            "\n",
            "Epoch 61, train loss A: 0.28 accuracy A: 0.91 \n",
            "\n",
            "Epoch 62, train loss A: 0.27 accuracy A: 0.91 \n",
            "\n",
            "Epoch 63, train loss A: 0.26 accuracy A: 0.91 \n",
            "\n",
            "Epoch 64, train loss A: 0.25 accuracy A: 0.92 \n",
            "\n",
            "Epoch 65, train loss A: 0.24 accuracy A: 0.92 \n",
            "\n",
            "Epoch 66, train loss A: 0.23 accuracy A: 0.93 \n",
            "\n",
            "Epoch 67, train loss A: 0.22 accuracy A: 0.93 \n",
            "\n",
            "Epoch 68, train loss A: 0.21 accuracy A: 0.93 \n",
            "\n",
            "Epoch 69, train loss A: 0.20 accuracy A: 0.93 \n",
            "\n",
            "Epoch 70, train loss A: 0.20 accuracy A: 0.93 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO0NBkq3AniQ"
      },
      "source": [
        "Let's look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "dZwnxNcuAj17",
        "outputId": "703279fa-16fb-4559-c247-b9c06b04bb54"
      },
      "source": [
        "process_msrments = processMeasurements(model_A_msrments)\n",
        "process_msrments.plotForgetHist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtpBl8EnAuCL"
      },
      "source": [
        "Let's check that our subset dataset of correct forgettable examples is, in fact, classified as correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_Wl4IL4Azjr",
        "outputId": "f34388a2-6ff0-43fd-ff64-40813846f690"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "forget_data = mgdataset.get_forgotten_dataset()\n",
        "forget_data_crt = mgdataset.get_forgotten_dataset_correct()\n",
        "forget_stats_crt = mgdataset.forgotten_correct_stats\n",
        "\n",
        "numcorrect = 0\n",
        "numincorrect = 0\n",
        "\n",
        "model_A.eval()\n",
        "for batch in forget_data_crt:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.detach().clone())\n",
        "    for k in range(len(out)):\n",
        "        if torch.argmax(out[k])==y[k]:\n",
        "            numcorrect+=1\n",
        "        else:\n",
        "            numincorrect+=1\n",
        "        #print(f\"{torch.argmax(out[k])}, {y[k]}, {torch.argmax(out[k])==y[k]}\")\n",
        "print(numcorrect/(numcorrect+numincorrect))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpZTxPxxRSW2",
        "outputId": "002cb0fc-a02f-4385-a243-2bf657cae545"
      },
      "source": [
        "len(forget_stats_crt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gyPuxj-BNOK"
      },
      "source": [
        "Now add noise to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luxkj2aABOgL",
        "outputId": "8455f6ac-f3bc-4c6f-a332-286ffa4dc164"
      },
      "source": [
        "model_damage = damageModel(model_A)\n",
        "noisy_clones = model_damage.addNoise()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Cloning models..."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhVQYGaiBiSG",
        "outputId": "97ab779e-a42a-4417-8fbd-325b6dae6ca7"
      },
      "source": [
        "print(model_damage.getEpsilons())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.00050251 0.00100503 0.00150754 0.00201005 0.00251256\n",
            " 0.00301508 0.00351759 0.0040201  0.00452261 0.00502513 0.00552764\n",
            " 0.00603015 0.00653266 0.00703518 0.00753769 0.0080402  0.00854271\n",
            " 0.00904523 0.00954774 0.01005025 0.01055276 0.01105528 0.01155779\n",
            " 0.0120603  0.01256281 0.01306533 0.01356784 0.01407035 0.01457286\n",
            " 0.01507538 0.01557789 0.0160804  0.01658291 0.01708543 0.01758794\n",
            " 0.01809045 0.01859296 0.01909548 0.01959799 0.0201005  0.02060302\n",
            " 0.02110553 0.02160804 0.02211055 0.02261307 0.02311558 0.02361809\n",
            " 0.0241206  0.02462312 0.02512563 0.02562814 0.02613065 0.02663317\n",
            " 0.02713568 0.02763819 0.0281407  0.02864322 0.02914573 0.02964824\n",
            " 0.03015075 0.03065327 0.03115578 0.03165829 0.0321608  0.03266332\n",
            " 0.03316583 0.03366834 0.03417085 0.03467337 0.03517588 0.03567839\n",
            " 0.0361809  0.03668342 0.03718593 0.03768844 0.03819095 0.03869347\n",
            " 0.03919598 0.03969849 0.04020101 0.04070352 0.04120603 0.04170854\n",
            " 0.04221106 0.04271357 0.04321608 0.04371859 0.04422111 0.04472362\n",
            " 0.04522613 0.04572864 0.04623116 0.04673367 0.04723618 0.04773869\n",
            " 0.04824121 0.04874372 0.04924623 0.04974874 0.05025126 0.05075377\n",
            " 0.05125628 0.05175879 0.05226131 0.05276382 0.05326633 0.05376884\n",
            " 0.05427136 0.05477387 0.05527638 0.05577889 0.05628141 0.05678392\n",
            " 0.05728643 0.05778894 0.05829146 0.05879397 0.05929648 0.05979899\n",
            " 0.06030151 0.06080402 0.06130653 0.06180905 0.06231156 0.06281407\n",
            " 0.06331658 0.0638191  0.06432161 0.06482412 0.06532663 0.06582915\n",
            " 0.06633166 0.06683417 0.06733668 0.0678392  0.06834171 0.06884422\n",
            " 0.06934673 0.06984925 0.07035176 0.07085427 0.07135678 0.0718593\n",
            " 0.07236181 0.07286432 0.07336683 0.07386935 0.07437186 0.07487437\n",
            " 0.07537688 0.0758794  0.07638191 0.07688442 0.07738693 0.07788945\n",
            " 0.07839196 0.07889447 0.07939698 0.0798995  0.08040201 0.08090452\n",
            " 0.08140704 0.08190955 0.08241206 0.08291457 0.08341709 0.0839196\n",
            " 0.08442211 0.08492462 0.08542714 0.08592965 0.08643216 0.08693467\n",
            " 0.08743719 0.0879397  0.08844221 0.08894472 0.08944724 0.08994975\n",
            " 0.09045226 0.09095477 0.09145729 0.0919598  0.09246231 0.09296482\n",
            " 0.09346734 0.09396985 0.09447236 0.09497487 0.09547739 0.0959799\n",
            " 0.09648241 0.09698492 0.09748744 0.09798995 0.09849246 0.09899497\n",
            " 0.09949749 0.1       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSz-T9SpBmSW"
      },
      "source": [
        "post_process = postProcess(mgdataset.get_num_forgotten_correct())\n",
        "classifications = post_process.classifyDataset(forget_data_crt, noisy_clones)\n",
        "#epsilonForgotten, timesForgotten = post_process.tabulateNoiseForget(classifications, model_damage.getEpsilons(), forget_stats_crt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwY4AmSRI8v_",
        "outputId": "29b06bae-759c-4cae-db63-baa0242e17b0"
      },
      "source": [
        "mgdataset.get_num_forgotten_correct()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlji53C7FlIj"
      },
      "source": [
        "epsilonForgotten, timesForgotten = post_process.tabulateNoiseForget(classifications, model_damage.getEpsilons(), forget_stats_crt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "9Zs7uZYpVlAP",
        "outputId": "14094b6f-fc5c-4182-e83c-963383c6023a"
      },
      "source": [
        "plt.scatter(epsilonForgotten, timesForgotten)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmub8AAsfVJ0"
      },
      "source": [
        "#For a given set of epsilon, forget stats, it scans through and determines the\n",
        "#largest N epsilons for a given # of forgotten events before the example was\n",
        "#misclassifed\n",
        "\n",
        "import heapq\n",
        "def findLargestEpsilon(epsilonForgotten, timesForgotten, largestN):\n",
        "    largest_value = int(max(timesForgotten))\n",
        "    smallest_value = int(min(timesForgotten))\n",
        "\n",
        "    largest_epsilon = torch.zeros(largest_value-smallest_value+1, largestN)\n",
        "    largest_forgotten = torch.zeros(largest_value-smallest_value+1, largestN)\n",
        "\n",
        "    for j in range(smallest_value, largest_value+1):\n",
        "        #largest_forgotten[j-smallest_value] = j\n",
        "        idx = [i for i in range(len(timesForgotten)) if timesForgotten[i]==j]\n",
        "        for k in range(len(timesForgotten)):\n",
        "            largest_forgotten[j-smallest_value, 0:] = torch.tensor([j]*largestN)\n",
        "            largest_epsilon[j-smallest_value, 0:] = torch.tensor(heapq.nlargest(largestN,[epsilonForgotten[i] for i in idx]))\n",
        "\n",
        "    return torch.flatten(largest_epsilon), torch.flatten(largest_forgotten)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuzNfGcln7EJ"
      },
      "source": [
        "maxeps, maxforget = findLargestEpsilon(epsilonForgotten, timesForgotten, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "fYOwetgLoLQx",
        "outputId": "b10c071a-0519-49ee-8723-c0a344a4a483"
      },
      "source": [
        "plt.scatter(maxforget, maxeps)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufGZLoc7hokL"
      },
      "source": [
        "# Developing experiment + trainer classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut6kKkIkwuh_"
      },
      "source": [
        "#default argument: default config file\n",
        "#it contains the following:\n",
        "#name of experiment, directory to store files\n",
        "#no. of models to train num_models\n",
        "#split into number of jobs num_jobs\n",
        "#for each job:\n",
        "#model parameters\n",
        "#train loop section\n",
        "#contains which measurements/actions to take (flags) during training\n",
        "#number of epochs\n",
        "#dataset to use, dataset params\n",
        "#default storage directory names for each job, model\n",
        "#store parameters\n",
        "\n",
        "#runner will go through config file and create a slurm script, \n",
        "#for each job it will generate a command e.g. python forget.py -name = \" \", -no models = \" \", etc.\n",
        "#so forget.py should be able to parse all of those parameters\n",
        "\n",
        "#decorator in training loop @config looks for flags to determine structure of training loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmrAIGZ0T7Nj",
        "outputId": "47247efb-c6ff-43aa-f6c2-0d4f1484e9d3"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 6.30 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLdbhUQ3QoTG"
      },
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import configparser\n",
        "import parser\n",
        "import numpy\n",
        "import trainer\n",
        "\n",
        "\n",
        "class experiment:\n",
        "    \"\"\"\n",
        "    the experiment should call on config.py to get info and create the appropriate directories\n",
        "    based on the contents of config.ini file. It should then be divided into two steps:\n",
        "    1. Pretraining (e.g. load model from OpenLTH)\n",
        "    2. Training (for each job, pass models onto trainer.py which trains it and stores the data)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config_file = \"default_config.ini\"):\n",
        "        #pretraining step:\n",
        "\n",
        "        #get config files from parser\n",
        "        self.reader = parser.readConfig(config_file)\n",
        "\n",
        "        #get the number of models to train\n",
        "        self.num_models = int(self.reader.exp_info[\"number of models\"])\n",
        "        self.num_jobs = int(self.reader.exp_info[\"number of jobs\"])\n",
        "\n",
        "        #number of models to train per job\n",
        "        if self.num_models % self.num_jobs == 0:\n",
        "            self.num_train_per_job = numpy.full(self.num_jobs, self.num_models/self.num_jobs).astype(int)\n",
        "        else:\n",
        "            self.num_train_per_job = numpy.full(self.num_jobs - 1, int(self.num_models/self.num_jobs)).astype(int)\n",
        "            self.num_train_per_job = numpy.append(self.num_train_per_job, int(self.num_models % self.num_jobs)) #check this\n",
        "\n",
        "        #make output directories\n",
        "        self.reader.mk_directories(self.num_train_per_job)\n",
        "        \n",
        "        print(f\"Division of jobs (models/job): {self.num_train_per_job}\")\n",
        "        #training step:\n",
        "        #and for each job, pass models onto trainer\n",
        "        job_idx = 0\n",
        "        model_idx = 0\n",
        "        for job in self.reader.jobs:\n",
        "            print(f\"{job}: {self.reader.jobs[job]}\")\n",
        "            for model_no in range(self.num_train_per_job[job_idx]):\n",
        "                model = self.reader.get_model(job)\n",
        "                model_trainer = trainer.train(model, self.reader.exp_info, self.reader.jobs[job], job_idx, model_idx)\n",
        "                model_trainer.trainLoop(model)\n",
        "                model_idx+=1\n",
        "            model_idx=0\n",
        "            job_idx+=1\n",
        "\n",
        "\n",
        "class post_experiment:\n",
        "    pass\n",
        "    #3. Post-training (do stuff with the data - e.g., from postprocess.py and plotter.py plot statistics of forgetting events)\n",
        "\n",
        "#     !git clone https://github.com/facebookresearch/open_lth.git\n",
        "# import sys\n",
        "# sys.path.append('/content/open_lth/')\n",
        "\n",
        "# from foundations import hparams\n",
        "# from models import registry\n",
        "\n",
        "# model_hparams = hparams.ModelHparams(\n",
        "#     'cifar_resnet_20',\n",
        "#     'kaiming_uniform',\n",
        "#     'uniform'\n",
        "# )\n",
        "\n",
        "# model_A = registry.get(model_hparams).cuda()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6FGJWpTN9Ds",
        "outputId": "4ee83c5b-ebd3-4246-a284-e887468e754b"
      },
      "source": [
        "exp = experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment info: {'name': 'AddingNoise3', 'storage directory': 'default', 'number of models': 8, 'number of jobs': 2}\n",
            "Division of jobs (models/job): [4 4]\n",
            "Job 1: {'model parameters': 'default', 'save models': 'true', 'num epochs': '10', 'save every': '2', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Job 2: {'model parameters': 'default', 'save models': 'true', 'num epochs': '10', 'save every': '2', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A1BzRJn14il2",
        "outputId": "03a1cedd-62a7-48b9-f006-787cb0779bb6"
      },
      "source": [
        "str(Path(Path().absolute()).parent) + 'open_lth/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/open_lth/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYa_b3KuozDm"
      },
      "source": [
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(\"/content/default_config.ini\")\n",
        "sections = config.sections()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To1hxtgSyPRI"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import configparser\n",
        "\n",
        "@dataclass\n",
        "class readConfig:\n",
        "    config_file: str = \"default_config.ini\"\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        parent_dir_path = Path(Path().absolute()).parent\n",
        "        sys.path.append(str(parent_dir_path) + 'open_lth/')\n",
        "\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read(self.config_file)\n",
        "        self.sections = config.sections()\n",
        "        self.exp_info = {}\n",
        "        self.jobs = {}\n",
        "\n",
        "\n",
        "        for section in self.sections:\n",
        "            if section == \"Experiment info\":\n",
        "                options = config.options(section)\n",
        "                self.exp_info[options[0]] = config.get(section, options[0])\n",
        "                self.exp_info[options[1]] = config.get(section, options[1])\n",
        "                self.exp_info[options[2]] = int(config.get(section, options[2]))\n",
        "                self.exp_info[options[3]] = int(config.get(section, options[3]))\n",
        "            elif str.split(section)[0] == \"Job\" and str.split(section)[1].isdigit():\n",
        "                self.jobs[section] = {}\n",
        "                options = config.options(section)\n",
        "                #change this to a loop\n",
        "                for i in range(len(options)):\n",
        "                    self.jobs[section][str(options[i])] = config.get(section, options[i])\n",
        "            else:\n",
        "                raise ValueError(\"Unknown section command in config file!\")\n",
        "    \n",
        "        #make directories for experiment, each job and model\n",
        "        parent_dir_path = Path(Path().absolute()).parent\n",
        "\n",
        "        #make experiment path\n",
        "        print(f\"Experiment info: {self.exp_info}\")\n",
        "        self.exp_path = str(parent_dir_path) + \"/\" + self.exp_info[\"name\"]\n",
        "        Path(self.exp_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        #for each job and for each model in the job, make the corresponding directory\n",
        "    \n",
        "    def mk_directories(self, model_numbers_per_job):\n",
        "        for job in self.jobs:\n",
        "            self.job_path = self.exp_path + \"/\" + job\n",
        "            Path(self.job_path).mkdir(parents=True, exist_ok=True)\n",
        "            for model_idx in model_numbers_per_job:#range(int(self.exp_info[\"number of models\"])):\n",
        "                self.model_path = self.job_path + \"/model\" + str(model_idx)\n",
        "                Path(self.model_path).mkdir(parents=True, exist_ok=True)\n",
        "            #and if track flags are on, create directories for those\n",
        "                if self.jobs[job][\"measure forget\"] == \"true\" or self.jobs[job][\"measure forget\"] == \"True\":\n",
        "                    Path(self.model_path + \"/forgetdata\").mkdir(parents=True, exist_ok=True)\n",
        "                if self.jobs[job][\"track correct examples\"] == \"true\" or self.jobs[job][\"track correct examples\"] == \"True\":\n",
        "                    Path(self.model_path + \"/correctdata\").mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        #model params\n",
        "    def get_model(self, job):\n",
        "        from open_lth.foundations import hparams\n",
        "        from open_lth.models import registry\n",
        "\n",
        "        if self.jobs[job][\"model parameters\"] == \"default\":\n",
        "            _model_params = hparams.ModelHparams(\n",
        "                'cifar_resnet_20',\n",
        "                'kaiming_uniform',\n",
        "                'uniform'\n",
        "            )\n",
        "            return registry.get(_model_params).cuda()\n",
        "        else:\n",
        "            pass #to do for case of other model parameters\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btnn-VsjBLmz",
        "outputId": "6a77e43c-ee03-4a55-c753-5a14b57b7e9b"
      },
      "source": [
        "reader = parser.readConfig()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'AddingNoise', 'storage directory': 'default', 'number of models': 2, 'number of jobs': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auYUMfr2QJAy",
        "outputId": "af1bd393-47f9-40fa-faf3-e43fe61c3295"
      },
      "source": [
        "reader.jobs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Job 1': {'dataset': 'cifar10',\n",
              "  'dataset params': 'default',\n",
              "  'measure forget': 'true',\n",
              "  'model noise': 'true',\n",
              "  'model parameters': 'default',\n",
              "  'noise parameters': 'default',\n",
              "  'num epochs': '10',\n",
              "  'save clones': 'true',\n",
              "  'save every': '2',\n",
              "  'save forget_dataset': 'true',\n",
              "  'save models': 'true',\n",
              "  'storage directory': 'default',\n",
              "  'track correct examples': 'true'},\n",
              " 'Job 2': {'dataset': 'cifar10',\n",
              "  'dataset params': 'default',\n",
              "  'measure forget': 'true',\n",
              "  'model noise': 'true',\n",
              "  'model parameters': 'default',\n",
              "  'noise parameters': 'default',\n",
              "  'num epochs': '10',\n",
              "  'save clones': 'true',\n",
              "  'save every': '2',\n",
              "  'save forget_dataset': 'true',\n",
              "  'save models': 'true',\n",
              "  'storage directory': 'default',\n",
              "  'track correct examples': 'true'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "ks5H6q3izRp6",
        "outputId": "92523e58-6e61-4272-eeda-b85ac75077ba"
      },
      "source": [
        "reader.hello()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-98e8bd5ad65e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhello\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'readConfig' object has no attribute 'hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "ezIB_z5izBYG",
        "outputId": "d3acdf7b-d2b8-4e55-c817-d8416d7939be"
      },
      "source": [
        "reader.get_model(reader.jobs[\"Job 1\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bd2cb8c5c41c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Job 1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'readConfig' object has no attribute 'get_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdSO0v64C3le",
        "outputId": "48c4e3bb-5497-4116-ba6e-1bba91a1a77e"
      },
      "source": [
        "for jobs in reader.jobs:\n",
        "    print(jobs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job 1\n",
            "Job 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qotnNSyuHxlE"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from measureforget import measureForget\n",
        "\n",
        "class train:\n",
        "    def __init__(self, model, exp_info, job_info, job_idx, model_idx): #job_idx, model_idx should be a unique modifier that indexes the job, model\n",
        "        #structure of directory is eg ../jobs/job1/model1/\n",
        "        #idx here would be '1'\n",
        "\n",
        "        #list of datasets that trainer knows about\n",
        "        parent_dir_path = Path(Path().absolute()).parent\n",
        "\n",
        "        self.dataset_names = ['cifar10']\n",
        "        self.num_epochs = int(job_info[\"num epochs\"])\n",
        "        self.save_every = int(job_info[\"save every\"])\n",
        "        if exp_info[\"storage directory\"] == \"default\":\n",
        "            self.exp_directory = str(parent_dir_path) + exp_info[\"name\"] + \"/\"\n",
        "        else:\n",
        "            self.exp_directory = exp_info[\"storage directory\"]\n",
        "\n",
        "        if job_info[\"model parameters\"] == \"default\":\n",
        "            self.optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
        "            self.loss = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            pass #to add this functionality, custom loss and optimizer\n",
        "\n",
        "        if job_info[\"save models\"] == \"true\" or job_info[\"save models\"] == \"True\":\n",
        "            self.save_model = True\n",
        "        else:\n",
        "            self.save_model = False\n",
        "\n",
        "        data_idx = [i for i in range(len(self.dataset_names)) if self.dataset_names[i]==job_info[\"dataset\"]][0]\n",
        "        self.data_loader = self.getTrainDataset(data_idx)\n",
        "\n",
        "        if job_info[\"dataset params\"] == \"default\":\n",
        "            self.batch_size = 128 #note that this also gets passed to measureForget\n",
        "        else:\n",
        "            pass #to add, custom dataset batch size, num workers, etc.\n",
        "\n",
        "        if job_info[\"measure forget\"] == \"true\" or job_info[\"measure forget\"] == \"True\":\n",
        "            self.forget_flag = True\n",
        "            self.forget_msrmt = measureForget(self.num_epochs, num_batches = self.batch_size, batch_size=self.batch_size)\n",
        "        else:\n",
        "            self.forget_msrmt = None\n",
        "\n",
        "        if job_info[\"track correct examples\"] == \"true\" or job_info[\"track correct examples\"] == \"True\":\n",
        "            self.track_correct_ex = True\n",
        "        \n",
        "        if job_info[\"storage directory\"] == \"default\":\n",
        "            self.store_directory = self.exp_directory + \"job\" + str(job_idx) + \"/\" + \"model\" + str(model_idx) + \"/\"\n",
        "        else:\n",
        "            pass #to add..\n",
        "        \n",
        "        self.trainLoop(model) #train the model\n",
        "    \n",
        "    def getTrainDataset(self, data_idx): #option to change batch size?\n",
        "        print(f\"Loading train dataset {self.getTrainDataset(data_idx)}... batch size {self.batch_size}\")\n",
        "        if data_idx == 0:\n",
        "            train_dataset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "            return DataLoader(train_dataset, batch_size=self.batch_size, num_workers = 0)\n",
        "\n",
        "    def trainLoop(self, model):\n",
        "        losses = list()\n",
        "        accuracies = list()\n",
        "        epochs = list()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            batch_loss = list()\n",
        "            batch_acc = list()\n",
        "\n",
        "            model.train()\n",
        "            for batch in self.data_loader:\n",
        "                x,y = batch\n",
        "                x=x.cuda()\n",
        "                logits = model(x)\n",
        "\n",
        "                if self.forget_flag: #eventually should change forget class to have wrapper instead of these flags.\n",
        "                    self.forget_msrmt.trackForgettableExamples(logits.detach(), y.detach())\n",
        "\n",
        "                J = self.loss(logits, y.cuda())\n",
        "                model.zero_grad()\n",
        "                J.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                batch_loss.append(J.item())\n",
        "                batch_acc.append(y.eq(logits.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "                if self.forget_flag:\n",
        "                    self.forget_msrmt.incrementTrainBatch()\n",
        "            \n",
        "            if self.forget_flag:\n",
        "                self.forget_msrmt.resetTrainBatchTracker()\n",
        "            \n",
        "            accuracies.append(torch.tensor(batch_acc).mean())\n",
        "            if self.forget_flag:\n",
        "                self.forget_msrmt.incrementTrainIter()\n",
        "\n",
        "            if self.track_correct_ex:\n",
        "                model.eval()\n",
        "                for batch in self.data_loader:\n",
        "                    x, y = batch\n",
        "                    x = x.cuda()\n",
        "                    with torch.no_grad():\n",
        "                        logits_prime = model(x.detach())\n",
        "        \n",
        "                    self.forget_msrmt.trackCorrectExamples(logits_prime.detach(), y.detach())\n",
        "                    self.forget_msrmt.incrementClassifyBatch()\n",
        "    \n",
        "                self.forget_msrmt.resetClassifyBatchTracker()\n",
        "\n",
        "            if (epoch+1) % self.save_every == 0:\n",
        "                self.save_model(model, epoch, torch.tensor(batch_loss).mean())\n",
        "                self.save_data()\n",
        "\n",
        "        if self.forget_flag:\n",
        "            self.forget_msrmt.resetTrainIter()\n",
        "        \n",
        "        model.eval()\n",
        "        self.clean(model)\n",
        "    \n",
        "    def save_model(self, model, epoch, loss):\n",
        "        torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, self.store_directory + \"epoch=\" + str(epoch+1))\n",
        "        \n",
        "    def save_data(self):\n",
        "        if self.forget_flag:\n",
        "            self.forget_msrmt.saveForget(epoch, self.store_directory)\n",
        "        if self.track_correct_ex:\n",
        "            self.forget_msrmt.saveCorrect(epoch, self.store_directory)\n",
        "\n",
        "        #to add: save accuracies\n",
        "\n",
        "    def clean(self):\n",
        "        del model\n",
        "        del self.forget_msrmt\n",
        "\n",
        "        #after training, clean caches,..\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCn89BsAaaIW",
        "outputId": "6ac67167-a6d2-46cb-a795-f68c49ab6fad"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "model2 = registry.get(model_hparams).cuda()\n",
        "optimizer = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'open_lth' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AehJCgl38vi7"
      },
      "source": [
        "torch.save({\n",
        "            'epoch': 1,\n",
        "            'model_state_dict': model_A.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': .34\n",
        "            }, \"/content/model1epoch=\"+str(0+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-JAeTYL91Cx",
        "outputId": "37285639-f4bf-4ebd-dc82-819349f48d56"
      },
      "source": [
        "checkpoint = torch.load(\"/content/model1epoch=1\")\n",
        "model2.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d96dHl-VcElK"
      },
      "source": [
        "trainer = train(model_A, reader.exp_info, reader.jobs['Job 1'], 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k3pz8X1Yg7-D",
        "outputId": "d34d3163-9b53-46b5-9a38-869750fb15e9"
      },
      "source": [
        "trainer.store_directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/AddingNoise/job1/model1/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbsuHrTzdB_t"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "@dataclass\n",
        "\n",
        "class testClass:\n",
        "    num: int\n",
        "    othernum = 2\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.myarray = np.arange(self.num)\n",
        "\n",
        "    def func(self):\n",
        "        return self.myarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVJAQ4vkdOu2",
        "outputId": "638c9696-cae1-4a42-ffcb-e4b7205e5fc8"
      },
      "source": [
        "mytest = testClass(3)\n",
        "mytest.func()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1q9WTFSfjPT"
      },
      "source": [
        "mytest.othernum =5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2shzCpG8fbcB",
        "outputId": "b8d2d788-7e2d-4aa1-db9c-2848a8eb4765"
      },
      "source": [
        "print(mytest.othernum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BewwJS9IFAG"
      },
      "source": [
        "#finally forget.py should just run the config reader and training loop\n",
        "#for each job"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Q03Wzl4vUW"
      },
      "source": [
        "test={}\n",
        "test[\"Job 1\"] = {}\n",
        "test[\"Job 1\"][\"num ep\"] = 3\n",
        "test[\"Job 1\"][\"save model\"] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQqqExtr438K",
        "outputId": "ad401dc0-4af3-40aa-adf9-7545a1f262d0"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Job 1': {'num ep': 3, 'save model': True}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eezOfIVm37PA"
      },
      "source": [
        "def hasNumbers(inputString):\n",
        "    return any(char.isdigit() for char in inputString)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RMZXtCF2sl0",
        "outputId": "c4e1bcde-57b9-412d-9818-9a06ac397a37"
      },
      "source": [
        "str.split(\"Job 2\")[1].isdigit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uqQju4HrrMVI",
        "outputId": "44c13ea6-9867-494f-d087-34d0c241f789"
      },
      "source": [
        "config.options('Experiment info')\n",
        "config.get('Experiment info', config.options('Experiment info')[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4UIfcc2WhtAY",
        "outputId": "5da6fdf2-fd6c-42e9-d89c-766410a463ff"
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "\n",
        "parser = argparse.ArgumentParser(description = \"Calculate\")\n",
        "parser.add_argument(\"-r\", \"--radius\", type=int, metavar='', required=True, help = \"R\")\n",
        "parser.add_argument(\"-H\", \"--height\", type=int, metavar='', required=True, help = \"H\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "def myFunc(radius, height):\n",
        "    vol = (math.pi)*(radius**2)*(height)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(myFunc(args.radius, args.height))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a4738c54ce21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYVGx3VnOkkt"
      },
      "source": [
        "                # self.jobs[section][\"save models\"] = config.get(section, options[1])\n",
        "                # self.jobs[section][\"num epochs\"] = int(config.get(section, options[2]))\n",
        "                # self.jobs[section][\"dataset\"] = config.get(section, options[3])\n",
        "                # self.jobs[section][\"measure forget\"] = config.get(section, options[4])\n",
        "                # self.jobs[section][\"track correct examples\"] = config.get(section, options[5])\n",
        "                # self.jobs[section][\"save forget dataset\"] = config.get(section, options[6])\n",
        "                # self.jobs[section][\"job directory\"] = config.get(section, options[7])\n",
        "                # self.jobs[section][\"add model noise\"] = config.get(section, options[8])\n",
        "                # self.jobs[section][\"noise parameters\"] = config.get(section, options[9])\n",
        "                # self.jobs[section][\"save clones\"] = config.get(section, options[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1yCWFaGrVrr"
      },
      "source": [
        "## More scratch..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrKcHz0yrVlQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "dONyxqQ6STOi",
        "outputId": "2ead055d-fc03-4348-efed-2ce4a7bfc8ad"
      },
      "source": [
        "test1 = [3, 4, 5]\n",
        "test2 = [[.1,.2,.3], [.3,.2,.1],[.4,.7,.1]]\n",
        "plt.scatter(test1,test2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-cbe419d5b07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m         plotnonfinite=plotnonfinite, **({\"data\": data} if data is not\n\u001b[0;32m-> 2816\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "ex-mwmLWBeRU",
        "outputId": "5cad9359-9604-44d7-f226-85d1617f89e2"
      },
      "source": [
        "import heapq\n",
        "heapq.nlargest(3, epsilonForgotten[i for i in range(len(timesForgotten)) if timesForgotten[i]==4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-a139d6245f21>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    heapq.nlargest(3, epsilonForgotten[i for i in range(len(timesForgotten)) if timesForgotten[i]==4])\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItuBoGLQA7cC",
        "outputId": "4fc45c08-770f-432d-c9d9-9a3e215a2f97"
      },
      "source": [
        "import heapq\n",
        "idx=[i for i in range(len(timesForgotten)) if timesForgotten[i]==4]\n",
        "torch.tensor(heapq.nlargest(3,[epsilonForgotten[i] for i in idx]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0749, 0.0749, 0.0749], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "M8cjlVuIT79P",
        "outputId": "53b99bcd-8956-4720-c894-5ea884afbc0b"
      },
      "source": [
        "classifications[0:,2].index(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-08a4d6f0598c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifications\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XuFhgz4UPMF",
        "outputId": "a634fbc4-d227-48ed-87d4-e48a702e258a"
      },
      "source": [
        "len(classifications[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1142"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9azq38K9Iu5"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLWUWr6O5eue",
        "outputId": "2f767b8f-cb45-4a20-e49f-e459c909ab31"
      },
      "source": [
        "print(model_A_msrments.getTrainIteration())\n",
        "print(model_A_msrments.correctStatistics[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
            "        [1., 1., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56BNtuucvPPr",
        "outputId": "399f43f6-43c7-46b7-eb7c-809725a2e9ca"
      },
      "source": [
        "model_A.eval()\n",
        "\n",
        "print(model_A_msrments.getTrainIteration())\n",
        "model_A_msrments.incrementTrainIter()\n",
        "print(model_A_msrments.getTrainIteration())\n",
        "\n",
        "myownstats = torch.zeros(391, 128)\n",
        "\n",
        "trker = 0\n",
        "for batch in train_set:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        l_A2 = model_A(x.detach().clone())\n",
        "    \n",
        "    for l in range(len(l_A2)):\n",
        "        if torch.argmax(l_A2[l]) == y[l]:\n",
        "            myownstats[trker, l] = 1\n",
        "\n",
        "    model_A_msrments.trackCorrectExamples(l_A2.detach().clone(), y.detach().clone())\n",
        "    model_A_msrments.incrementClassifyBatch()\n",
        "\n",
        "model_A_msrments.resetClassifyBatchTracker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl_Nzj4-1fYD",
        "outputId": "6c84a667-bda7-4715-f198-ea5ab0976681"
      },
      "source": [
        "model_A_msrments.correctStatistics[29,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRwEq8cj8jrV"
      },
      "source": [
        "Let's look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "AaJd_pLt8irt",
        "outputId": "049a90d6-f337-440f-9e10-23eda3866c4d"
      },
      "source": [
        "process_msrments = processMeasurements(model_A_msrments)\n",
        "process_msrments.plotForgetHist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_stY7eAnp2PS",
        "outputId": "58ff0c05-55ee-4895-f569-5dff6e1ff69a"
      },
      "source": [
        "model_A_msrments.correctStatistics[23,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
              "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mHhUAJ89zl4",
        "outputId": "1e635249-747a-4932-9534-cf677643d7f4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "crt_stats = model_A_msrments.correctStatistics[29]\n",
        "model_A.eval()\n",
        "tracker=0\n",
        "grid=[]\n",
        "\n",
        "train_set_2 = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "train_set_2_loader = torch.utils.data.DataLoader(train_set_2, batch_size=128, num_workers = 0)\n",
        "\n",
        "for batch in train_set_2_loader:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.detach().clone())\n",
        "        \n",
        "    for k in range(len(out)):\n",
        "        if crt_stats[tracker, k]==1 and torch.argmax(out[k]) != y[k]:\n",
        "            grid.append(x.cpu()[k])\n",
        "\n",
        "    tracker+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "PF3wsCedqXxd",
        "outputId": "2b752128-df01-4328-ab2c-4a98a8496a71"
      },
      "source": [
        "show(grid[2]*.23+ 0.45)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc5IDCUzpbar"
      },
      "source": [
        "model_A_msrments.resetClassifyBatchTracker()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "U1WLKFwZlnf_",
        "outputId": "8bfcd64e-92be-48ba-fda1-1e1790722914"
      },
      "source": [
        "model_A.eval()\n",
        "tracker=0\n",
        "for batch in train_set:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        l_Ap = model_A(x.detach().clone())\n",
        "    if tracker==5:\n",
        "        show(x[4].cpu())\n",
        "\n",
        "    model_A_msrments.trackCorrectExamples(l_Ap.detach().clone(), y.detach().clone())\n",
        "    model_A_msrments.incrementClassifyBatch()\n",
        "    tracker+=1\n",
        "\n",
        "model_A_msrments.resetClassifyBatchTracker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4vuqJYd8bb5",
        "outputId": "1db5478a-b89e-456f-dd16-919ceb26b043"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "forget_data = mgdataset.get_forgotten_dataset()\n",
        "forget_data_crt = mgdataset.get_forgotten_dataset_correct()\n",
        "\n",
        "numcorrect = 0\n",
        "numincorrect = 0\n",
        "\n",
        "model_A.eval()\n",
        "for batch in forget_data_crt:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.detach().clone())\n",
        "    for k in range(len(out)):\n",
        "        if torch.argmax(out[k])==y[k]:\n",
        "            numcorrect+=1\n",
        "        else:\n",
        "            numincorrect+=1\n",
        "        #print(f\"{torch.argmax(out[k])}, {y[k]}, {torch.argmax(out[k])==y[k]}\")\n",
        "print(numcorrect/(numcorrect+numincorrect))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "0.6736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_gscBw2jBam",
        "outputId": "73e6d344-0589-413a-b581-c83c8e6cda3c"
      },
      "source": [
        "len(model_A_msrments.correctStatistics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPbhYtsK5Crf"
      },
      "source": [
        "# Debugging package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aurnVp5k5IH2",
        "outputId": "2be32757-dc95-46da-a71f-c7890f1d9d53"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "!git clone https://github.com/nikhilanand91/Forget.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 8.02 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Cloning into 'Forget'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 168 (delta 72), reused 140 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (168/168), 28.95 KiB | 7.24 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3khcLc406LP",
        "outputId": "73581dc5-b2c3-49f7-9d91-6910e76be49e"
      },
      "source": [
        "!rm -r \"/AddingNoise2/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/AddingNoise2/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI54SXgkhQWF"
      },
      "source": [
        "rm -r \"/content/Forget/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZkVoGlYhTff",
        "outputId": "1c27c745-ea3d-490b-c18e-86514db3dbfb"
      },
      "source": [
        "rm -r \"/content/open_lth/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/open_lth/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVV3X61B1VmQ"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd()+\"/Forget/open_lth/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7rKA3IB5j3i"
      },
      "source": [
        "from Forget.main import experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgxLyYpM6hB2",
        "outputId": "c9785655-ff6b-4fe6-bef0-387edb50bae7"
      },
      "source": [
        "experiment.run_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Appending paths: /content/Forget\n",
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 2, 'number of jobs': 2}\n",
            "Division of jobs (models/job): [1 1]\n",
            "Starting training...\n",
            "Job 1: {'model parameters': 'default', 'save models': 'true', 'num epochs': '10', 'save every': '10', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Job 2: {'model parameters': 'default', 'save models': 'true', 'num epochs': '10', 'save every': '10', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Now processing output...\n",
            "Appending path: /content/Forget\n",
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 2, 'number of jobs': 2}\n",
            "Reading from output files in experiment AddingNoise2...\n",
            "Fetching dataset...\n",
            "Files already downloaded and verified\n",
            "['/AddingNoise2/Job 1/model0/forgetdata/', '/AddingNoise2/Job 2/model0/forgetdata/']\n",
            "Number forgotten: 14\n",
            "Number forgotten + correct: 6\n",
            "Number forgotten: 17\n",
            "Number forgotten + correct: 9\n",
            "Appending /content/Forget/open_lth/\n",
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 2, 'number of jobs': 2}\n",
            "Reading from output files in experiment AddingNoise2...\n",
            "['/AddingNoise2/Job 1/model0', '/AddingNoise2/Job 2/model0']\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Cloning models..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Forget.main.experiment.run_experiment at 0x7fdfd025ce10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_0xYRkbMOAQ"
      },
      "source": [
        "from Forget.postprocess import postprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdgChaHpMXbs",
        "outputId": "c979766c-13c3-4d5f-ce0d-24b5c7ac7d24"
      },
      "source": [
        "procs = postprocess.postProcess()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 2, 'number of jobs': 2}\n",
            "Reading from clones files in experiment AddingNoise2...\n",
            "Model counts: [200, 200]\n",
            "Clone folders: ['/AddingNoise2/Job 1/model0/clones/', '/AddingNoise2/Job 2/model0/clones/']\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [6]\n",
            "[[0.013065326633165831, 0.0030150753768844224, 0.01155778894472362, 0.010552763819095479, 0.006030150753768845, 0.018592964824120605]]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [9]\n",
            "[[0.013065326633165831, 0.0030150753768844224, 0.01155778894472362, 0.010552763819095479, 0.006030150753768845, 0.018592964824120605], [0.017085427135678392, 0.0030150753768844224, 0.005527638190954774, 0.020100502512562818, 0.029145728643216084, 0.008542713567839196, 0.01256281407035176, 0.022110552763819097, 0.008542713567839196]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "fL65kt58NurY",
        "outputId": "5aca1c0b-5be2-45f4-9b39-f3569ae57378"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "\n",
        "plt.scatter(procs.totalEpsilonsTensor, procs.totalForgottenTensor)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7fekV0FNLXm",
        "outputId": "287a8e1f-d8cd-40bc-85d1-ab249b685522"
      },
      "source": [
        "import torch\n",
        "t = [[2,3,4],[3,2,1,2]]\n",
        "[item for sublist in t for item in sublist]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 3, 2, 1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTy0a5BTPZn_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTX7A8KSMPMc"
      },
      "source": [
        "# Debugging package 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjwmaC2UXCmJ",
        "outputId": "b6cc1293-b52c-4edc-be6c-f96186ea714c"
      },
      "source": [
        "!zip -r /content/AddingNoise2.zip /AddingNoise2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: AddingNoise2/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model2/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model2/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model2/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model2/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model2/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model2/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 1/model2/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model2/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model2/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model2/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 1/model2/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 1/model2/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 1/model3/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model3/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model3/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model3/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model3/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model3/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 1/model3/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model3/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model3/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model3/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 1/model3/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 1/model3/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 1/model4/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model4/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model4/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model4/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model4/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model4/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 1/model4/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model4/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model4/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model4/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 1/model4/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 1/model4/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 1/model1/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model1/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model1/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model1/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model1/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model1/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 1/model1/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model1/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model1/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model1/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 1/model1/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 1/model1/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 1/model0/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model0/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model0/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model0/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 1/model0/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model0/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 1/model0/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model0/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 1/model0/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 1/model0/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 1/model0/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 1/model0/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 2/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model2/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model2/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model2/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model2/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model2/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model2/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 2/model2/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model2/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model2/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model2/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 2/model2/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 2/model2/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 2/model3/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model3/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model3/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model3/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model3/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model3/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 2/model3/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model3/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model3/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model3/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 2/model3/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 2/model3/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 2/model4/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model4/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model4/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model4/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model4/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model4/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 2/model4/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model4/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model4/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model4/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 2/model4/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 2/model4/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 2/model1/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model1/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model1/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model1/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model1/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model1/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 2/model1/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model1/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model1/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model1/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 2/model1/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 2/model1/correctdata/correctstatsepoch=20.pt (deflated 96%)\n",
            "  adding: AddingNoise2/Job 2/model0/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model0/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model0/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model0/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise2/Job 2/model0/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model0/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise2/Job 2/model0/forgetdata/forgetstatsepoch=20.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model0/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise2/Job 2/model0/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise2/Job 2/model0/correctdata/correctstatsepoch=30.pt (deflated 94%)\n",
            "  adding: AddingNoise2/Job 2/model0/correctdata/correctstatsepoch=10.pt (deflated 98%)\n",
            "  adding: AddingNoise2/Job 2/model0/correctdata/correctstatsepoch=20.pt (deflated 96%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rsmHHIzPXKp3",
        "outputId": "bfef8a80-356a-466b-d5f3-0b660cd45548"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/AddingNoise2.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_399b1ac4-a993-47ce-a397-8466b1d18737\", \"AddingNoise2.zip\", 69817449)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi_Uu1JiX_m6"
      },
      "source": [
        "# Developing post process files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heiW_o_b624f",
        "outputId": "8b0cff55-0f9a-4054-f159-4c142e903cc3"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "!git clone https://github.com/nikhilanand91/Forget.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 2.94 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Cloning into 'Forget'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 142 (delta 59), reused 118 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (142/142), 26.27 KiB | 6.57 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27Cv0qpSOyLh"
      },
      "source": [
        "!rm -r \"/AddingNoise2\"\n",
        "!rm -r \"/Forget/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMtpnbwXOhgm",
        "outputId": "39baedbf-32fa-4e8a-8431-9e38e3cfab87"
      },
      "source": [
        "!unzip \"/AddingNoise2.zip\" -d \"/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /AddingNoise2.zip\n",
            "   creating: /AddingNoise2/\n",
            "   creating: /AddingNoise2/Job 1/\n",
            "   creating: /AddingNoise2/Job 1/model2/\n",
            "  inflating: /AddingNoise2/Job 1/model2/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model2/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model2/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model2/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 1/model2/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model2/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model2/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model2/correctdata/\n",
            "  inflating: /AddingNoise2/Job 1/model2/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model2/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model2/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 1/model3/\n",
            "  inflating: /AddingNoise2/Job 1/model3/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model3/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model3/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model3/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 1/model3/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model3/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model3/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model3/correctdata/\n",
            "  inflating: /AddingNoise2/Job 1/model3/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model3/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model3/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 1/model4/\n",
            "  inflating: /AddingNoise2/Job 1/model4/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model4/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model4/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model4/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 1/model4/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model4/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model4/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model4/correctdata/\n",
            "  inflating: /AddingNoise2/Job 1/model4/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model4/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model4/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 1/model1/\n",
            "  inflating: /AddingNoise2/Job 1/model1/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model1/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model1/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model1/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 1/model1/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model1/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model1/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model1/correctdata/\n",
            "  inflating: /AddingNoise2/Job 1/model1/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model1/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model1/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 1/model0/\n",
            "  inflating: /AddingNoise2/Job 1/model0/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model0/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model0/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model0/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 1/model0/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model0/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model0/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 1/model0/correctdata/\n",
            "  inflating: /AddingNoise2/Job 1/model0/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model0/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 1/model0/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 2/\n",
            "   creating: /AddingNoise2/Job 2/model2/\n",
            "  inflating: /AddingNoise2/Job 2/model2/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model2/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model2/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model2/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 2/model2/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model2/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model2/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model2/correctdata/\n",
            "  inflating: /AddingNoise2/Job 2/model2/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model2/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model2/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 2/model3/\n",
            "  inflating: /AddingNoise2/Job 2/model3/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model3/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model3/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model3/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 2/model3/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model3/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model3/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model3/correctdata/\n",
            "  inflating: /AddingNoise2/Job 2/model3/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model3/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model3/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 2/model4/\n",
            "  inflating: /AddingNoise2/Job 2/model4/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model4/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model4/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model4/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 2/model4/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model4/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model4/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model4/correctdata/\n",
            "  inflating: /AddingNoise2/Job 2/model4/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model4/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model4/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 2/model1/\n",
            "  inflating: /AddingNoise2/Job 2/model1/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model1/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model1/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model1/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 2/model1/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model1/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model1/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model1/correctdata/\n",
            "  inflating: /AddingNoise2/Job 2/model1/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model1/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model1/correctdata/correctstatsepoch=20.pt  \n",
            "   creating: /AddingNoise2/Job 2/model0/\n",
            "  inflating: /AddingNoise2/Job 2/model0/epoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model0/epoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model0/epoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model0/forgetdata/\n",
            "  inflating: /AddingNoise2/Job 2/model0/forgetdata/forgetstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model0/forgetdata/forgetstatsepoch=20.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model0/forgetdata/forgetstatsepoch=30.pt  \n",
            "   creating: /AddingNoise2/Job 2/model0/correctdata/\n",
            "  inflating: /AddingNoise2/Job 2/model0/correctdata/correctstatsepoch=30.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model0/correctdata/correctstatsepoch=10.pt  \n",
            "  inflating: /AddingNoise2/Job 2/model0/correctdata/correctstatsepoch=20.pt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZifzkWLa7VEK"
      },
      "source": [
        "#First go through models, assume latest epoch, get forget dataset + mask\n",
        "#can we save the dataset + mask?\n",
        "\n",
        "\"\"\"\n",
        "Class for managing forget data. It can go through forget statistics and output\n",
        "a mask corresponding to the forget threshold. This mask can be used e.g. to obtain a\n",
        "dataset of forgettable examples. It can also output subset datasets of correctly\n",
        "classified examples and forgotten + correctly classified examples at the end of\n",
        "training.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(str(\"/\")) #this will need to be fixed later\n",
        "from Forget.config import parser\n",
        "from Forget.training import trainer\n",
        "import os\n",
        "\n",
        "class createForgetDataset:\n",
        "    def __init__(self, forget_thres = 3, config_file = \"/Forget/config/default_config.ini\"):\n",
        "        #assume latest epoch, go through forgetstats and correctstats\n",
        "        #for each model, initialize the below variables, create the forget\n",
        "        #dataset + save into forgetdata/ and also the forget+correct dataset.\n",
        "\n",
        "        #first set the threshold\n",
        "        self.forget_thres = forget_thres\n",
        "\n",
        "        #from the config file, obtain experiment name + number of jobs + number of models\n",
        "        self.reader = parser.readConfig(config_file)\n",
        "\n",
        "        self.exp_name = self.reader.exp_info[\"name\"]\n",
        "        #self.num_jobs = int(self.reader.exp_info[\"number of jobs\"])\n",
        "        \n",
        "        self.list_forget_folders = []\n",
        "        print(f\"Reading from output files in experiment {self.exp_name}...\")\n",
        "        for job in self.reader.jobs: #remember to put everything into forgetdata\n",
        "            job_subdir = [\"/\"+self.exp_name + \"/\" + job + \"/\" + f.name + \"/forgetdata/\" for f in os.scandir(\"/\" + self.exp_name + \"/\" + job + \"/\") if f.is_dir()]\n",
        "            for dir in job_subdir:\n",
        "                self.list_forget_folders.append(dir)\n",
        "            #self.list_correct_folders = [self.exp_name + \"/\" + job + \"/\" + f.name + \"/correctdata/\" for f in os.scandir(\"/\" + self.exp_name + \"/\" + job + \"/\") if f.is_dir()]\n",
        "            self.max_epoch = int(self.reader.jobs[job][\"num epochs\"])\n",
        "\n",
        "        \"\"\"\n",
        "        should move dataset info to experiment probably\n",
        "        so this part needs to be systematized better.\n",
        "        right now we just assume that the dataset is CIFAR10\n",
        "        but soon we'll want to do other datasets and we should\n",
        "        just write a class that can fetch a dataset from exp_info.\n",
        "        Perhaps I should dispense with the \"job\" system entirely?\n",
        "        \"\"\"\n",
        "        print(f\"Fetching dataset...\")\n",
        "        self.trainset = fetch_dataset(data_idx = 0, batch_size = 128)\n",
        "        \n",
        "        \"\"\"\n",
        "        for each model go through forgetstats and correctstats, upload the masks to the folder.\n",
        "        This part is undoubtedly clunky... ideally we'd have a directory just associated with the model\n",
        "        and scan over models, rather than scan over directories.\n",
        "        Maybe something to improve in the next iteration of this code.\n",
        "        \"\"\"\n",
        "        print(self.list_forget_folders)\n",
        "\n",
        "        self.getForgetMask_has_run = False\n",
        "        self.getForgetMaskCorrect_has_run = False\n",
        "        \n",
        "        for folder_idx, subfolder in enumerate(self.list_forget_folders):\n",
        "            self.forget_stats = torch.load(subfolder + \"forgetstatsepoch=\"+str(self.max_epoch)+\".pt\")\n",
        "            self.correct_stats = torch.load(subfolder + \"correctstatsepoch=\"+str(self.max_epoch)+\".pt\")\n",
        "            self.sum_over_ep_flatten_forget = torch.flatten(torch.sum(self.forget_stats, 0))\n",
        "            self.forget_mask = list()\n",
        "            self.forget_mask_correct = list()\n",
        "            self.batch_size = self.forget_stats[0,0].size()[0] #infer batch size from input\n",
        "            self.forgotten_correct_stats = list()\n",
        "\n",
        "            #now compute the masks and save them\n",
        "            self.getForgetMask(save_directory = subfolder)\n",
        "            self.getForgetMaskCorrect(save_directory = subfolder)\n",
        "\n",
        "            #obtain the forgotten dataset that's classified correctly and save it\n",
        "            self.getFgtDatasetCorrect(save_directory = subfolder)\n",
        "\n",
        "            #as well as the full forgotten dataset\n",
        "            self.getFullForgottenDataset(save_directory = subfolder)\n",
        "\n",
        "            print(f\"Number forgotten: {self.get_num_forgotten()}\")\n",
        "            print(f\"Number forgotten + correct: {self.get_num_forgotten_correct()}\")\n",
        "            torch.save(torch.tensor([self.get_num_forgotten(), self.get_num_forgotten_correct()]), subfolder + \"num_forgotten.pt\")\n",
        "\n",
        "\n",
        "    def getForgetMask(self, save_directory = \"/\", save = True):\n",
        "        self.getForgetMask_has_run = True\n",
        "        for k in range(len(self.sum_over_ep_flatten_forget)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres:\n",
        "                self.forget_mask.append(k)\n",
        "        if save:\n",
        "            torch.save(torch.tensor(self.forget_mask), save_directory + \"forgetmask_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "\n",
        "    def getForgetMaskCorrect(self, save_directory = \"/\", save = True):\n",
        "        self.getForgetMaskCorrect_has_run = True\n",
        "        self.correct_flat = torch.flatten(self.correct_stats[self.max_epoch-1])\n",
        "\n",
        "        for k in range(len(self.correct_flat)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres and self.correct_flat[k]==1:\n",
        "                self.forget_mask_correct.append(k)\n",
        "                self.forgotten_correct_stats.append(torch.IntTensor.item(self.sum_over_ep_flatten_forget[k]))\n",
        "\n",
        "        if save:\n",
        "            torch.save(torch.tensor(self.forget_mask_correct), save_directory + \"forgetmask_correct_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "    \n",
        "    def getFgtDatasetCorrect(self, save_directory = \"/\", save = True): #return a mask of those examples that were forgotten AND classified correctly\n",
        "        \"\"\"\n",
        "        this is now redundant since the dataloader should be built up from mask+Dataset\n",
        "        \"\"\"\n",
        "        #requires having run getForgetMask() first\n",
        "        if not self.getForgetMaskCorrect_has_run:\n",
        "            raise ValueError(\"Run getForgetMaskCorrect() first!\")\n",
        "\n",
        "        #self.train_subset_correct = torch.utils.data.Subset(self.trainset, self.forget_mask_correct)\n",
        "\n",
        "        if save:\n",
        "            torch.save(self.forget_mask_correct, save_directory + \"forgotten_correct_mask_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "            #torch.save(self.train_subset_correct,\n",
        "            #           save_directory + \"forgotten_correct_dataset_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "\n",
        "    def get_num_forgotten(self):\n",
        "        if len(self.forget_mask)==0:\n",
        "            raise ValueError(\"Obtain the mask of forgettable examples first; right now it's empty.\")\n",
        "        else:\n",
        "            return len(self.forget_mask)\n",
        "\n",
        "    def get_num_forgotten_correct(self, which_epoch = None):\n",
        "        if len(self.forget_mask_correct) == 0:\n",
        "            raise ValueError(\"Obtain mask of forgettable + correct examples first; right now it's empty.\")\n",
        "        else:\n",
        "            return len(self.forget_mask_correct)\n",
        "\n",
        "    def getFullForgottenDataset(self, save_directory = \"/\", save = True):\n",
        "        \"\"\"\n",
        "        also redundant -- to remove\n",
        "        \"\"\"\n",
        "        if not self.getForgetMask_has_run:\n",
        "            raise ValueError(\"Run getForgetMask() first!\")\n",
        "        #self.train_subset = torch.utils.data.Subset(self.trainset, self.forget_mask)\n",
        "        #self.full_forgotten_dataset = torch.utils.data.DataLoader(self.train_subset, batch_size=self.batch_size)\n",
        "\n",
        "        if save:\n",
        "            torch.save(self.trainset, save_directory + \"trainset.pt\")\n",
        "            torch.save(self.forget_mask, save_directory + \"forgotten_mask_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "def fetch_dataset(data_idx, batch_size):\n",
        "    if data_idx == 0:\n",
        "        train_dataset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "        return DataLoader(train_dataset, batch_size, num_workers = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "4a1eb70b47be446f9956b6842bb450ef",
            "37f8accd8f584ccea0b009a3d84412b7",
            "404ed0be6f89472ab7c97a0c9695879e",
            "759a62f4a8e7425293fdd823e076ccac",
            "84a7809041fc4dd4887e94377cd82f57",
            "a1df876c8f594c0c8e6822a075cd08b0",
            "2f70b00dc6694726a737e6f58590eaf2",
            "c6660faf481e4f3083a8dc9d0e8e1657",
            "c32ea2be7f724f1f94d0823a7b25723d",
            "7a6713c8b7b84b3e866db02a1a1a3a75",
            "eb00a2996ef44ef9bd490b4236c3dfd2"
          ]
        },
        "id": "KX3Ge--a5_sg",
        "outputId": "d4183981-ec18-4483-fc90-0fb1e89aac65"
      },
      "source": [
        "crtForget = createForgetDataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from output files in experiment AddingNoise2...\n",
            "Fetching dataset...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a1eb70b47be446f9956b6842bb450ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /cifar-10-python.tar.gz to /\n",
            "['/AddingNoise2/Job 1/model2/forgetdata/', '/AddingNoise2/Job 1/model1/forgetdata/', '/AddingNoise2/Job 1/model3/forgetdata/', '/AddingNoise2/Job 1/model0/forgetdata/', '/AddingNoise2/Job 1/model4/forgetdata/', '/AddingNoise2/Job 2/model2/forgetdata/', '/AddingNoise2/Job 2/model1/forgetdata/', '/AddingNoise2/Job 2/model3/forgetdata/', '/AddingNoise2/Job 2/model0/forgetdata/', '/AddingNoise2/Job 2/model4/forgetdata/']\n",
            "Number forgotten: 537\n",
            "Number forgotten + correct: 292\n",
            "Number forgotten: 919\n",
            "Number forgotten + correct: 495\n",
            "Number forgotten: 1017\n",
            "Number forgotten + correct: 588\n",
            "Number forgotten: 704\n",
            "Number forgotten + correct: 418\n",
            "Number forgotten: 582\n",
            "Number forgotten + correct: 366\n",
            "Number forgotten: 1332\n",
            "Number forgotten + correct: 766\n",
            "Number forgotten: 677\n",
            "Number forgotten + correct: 418\n",
            "Number forgotten: 1057\n",
            "Number forgotten + correct: 659\n",
            "Number forgotten: 1005\n",
            "Number forgotten + correct: 517\n",
            "Number forgotten: 917\n",
            "Number forgotten + correct: 553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWU3kgWKK07o"
      },
      "source": [
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "def fetch_dataset(data_idx, batch_size):\n",
        "    if data_idx == 0:\n",
        "        train_dataset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "        return DataLoader(train_dataset, batch_size, num_workers = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mmnx38N7RA3"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"/\")\n",
        "sys.path.append(\"/Forget/open_lth/\")\n",
        "from Forget.open_lth.foundations import hparams\n",
        "from Forget.open_lth.models import registry\n",
        "from pathlib import Path\n",
        "\n",
        "class damageModel:\n",
        "    def __init__(self, config_file = \"/Forget/config/default_config.ini\"):\n",
        "\n",
        "        self.reader = parser.readConfig(config_file)\n",
        "        self.exp_name = self.reader.exp_info[\"name\"]\n",
        "        #self.num_jobs = int(self.reader.exp_info[\"number of jobs\"])\n",
        "        \n",
        "        self.list_model_folders = []\n",
        "        print(f\"Reading from output files in experiment {self.exp_name}...\")\n",
        "        for job in self.reader.jobs: #remember to put everything into forgetdata\n",
        "            job_subdir = [\"/\"+self.exp_name + \"/\" + job + \"/\" + f.name for f in os.scandir(\"/\" + self.exp_name + \"/\" + job + \"/\") if f.is_dir()]\n",
        "            for dir in job_subdir:\n",
        "                self.list_model_folders.append(dir)\n",
        "            self.max_epoch = int(self.reader.jobs[job][\"num epochs\"])\n",
        "\n",
        "            if self.reader.jobs[job][\"model parameters\"] == \"default\":\n",
        "                self.model_hparams = hparams.ModelHparams('cifar_resnet_20', 'kaiming_uniform', 'uniform')\n",
        "            else:\n",
        "                pass #to add in\n",
        "        \n",
        "        print(self.list_model_folders)\n",
        "\n",
        "        for folder in self.list_model_folders:\n",
        "            self.model = registry.get(self.model_hparams).cuda()\n",
        "            self.model.load_state_dict(torch.load(folder+\"/epoch=\"+str(self.max_epoch)+\".pt\")['model_state_dict'])\n",
        "            self.clones = self.addNoise(self.model)\n",
        "\n",
        "            save_clone_path = folder + \"/clones/\"\n",
        "            Path(save_clone_path).mkdir(parents=True, exist_ok=True)\n",
        "            for idx, clone in enumerate(self.clones):\n",
        "                torch.save(clone.state_dict(), save_clone_path + str(idx) + \".pt\")\n",
        "\n",
        "            del self.model\n",
        "            del self.clones\n",
        "\n",
        "        \n",
        "    def addNoise(self, model, num_points = 200, min_noise = 0., max_noise = 0.1):\n",
        "        \"\"\"\n",
        "        returns an array of length num_points, consisting of models increasingly damaged\n",
        "        from Gaussian noise with stdev min_noise to max_noise\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        self.model_clones = []\n",
        "        model_state_dict = model.state_dict()\n",
        "\n",
        "        epsilons = np.linspace(min_noise, max_noise, num_points)\n",
        "        for i in range(len(epsilons)):\n",
        "            sys.stdout.write(\"\\r{0}Cloning models...\".format(\"|\"*i))\n",
        "            sys.stdout.flush()\n",
        "            self.model_clones.append(registry.get(self.model_hparams).cuda())\n",
        "            self.model_clones[i].load_state_dict(model_state_dict)\n",
        "            \n",
        "\n",
        "        with torch.no_grad():\n",
        "            k = 0\n",
        "            for model in self.model_clones:\n",
        "                for param in model.parameters():\n",
        "                    param.multiply_(1+torch.empty(param.size()).cuda().normal_(mean=0,std=epsilons[k]))\n",
        "                k+=1\n",
        "            \n",
        "        for models in self.model_clones:\n",
        "            models.eval()\n",
        "        \n",
        "        return self.model_clones\n",
        "        \n",
        "\n",
        "def getEpsilons(num_points = 200, min_noise = 0., max_noise = 0.1):\n",
        "        return np.linspace(min_noise, max_noise, num_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyF4yhhX2Jk_",
        "outputId": "91d3a7cc-eed8-464c-9893-90eb73817256"
      },
      "source": [
        "dmg = damageModel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from output files in experiment AddingNoise2...\n",
            "['/AddingNoise2/Job 1/model2', '/AddingNoise2/Job 1/model1', '/AddingNoise2/Job 1/model3', '/AddingNoise2/Job 1/model0', '/AddingNoise2/Job 1/model4', '/AddingNoise2/Job 2/model2', '/AddingNoise2/Job 2/model1', '/AddingNoise2/Job 2/model3', '/AddingNoise2/Job 2/model0', '/AddingNoise2/Job 2/model4']\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Cloning models..."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeWvh0hrVN1X",
        "outputId": "518cad5e-c45e-4c64-df4f-dfc919da07b5"
      },
      "source": [
        "list_subfolders_with_paths = [f.path for f in os.scandir(\"/AddingNoise2/Job 1/\") if f.is_dir()]\n",
        "print(list_subfolders_with_paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/AddingNoise2/Job 1/model4', '/AddingNoise2/Job 1/model3', '/AddingNoise2/Job 1/model2', '/AddingNoise2/Job 1/model1', '/AddingNoise2/Job 1/model0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIrCih2b-m6_",
        "outputId": "cfdf642c-e790-4cce-d994-204053fe168b"
      },
      "source": [
        "[ f.name + \"/forgetdata/\" for f in os.scandir(\"/\" + \"AddingNoise2\" + \"/\" + \"Job 1\" + \"/\") if f.is_dir()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model4/forgetdata/',\n",
              " 'model3/forgetdata/',\n",
              " 'model2/forgetdata/',\n",
              " 'model1/forgetdata/',\n",
              " 'model0/forgetdata/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27Txwt5wZOQ5",
        "outputId": "47d434dd-8818-480b-ae5a-e4f03385e043"
      },
      "source": [
        "list_subfolders_with_paths_2 = [f.name + \"/forgetdata/\" for f in os.scandir(\"/AddingNoise2/Job 1/\") if f.is_dir()]\n",
        "print(list_subfolders_with_paths_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['model4/forgetdata/', 'model3/forgetdata/', 'model2/forgetdata/', 'model1/forgetdata/', 'model0/forgetdata/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvzi2S3MJgek",
        "outputId": "f5a6719d-ab9d-4f09-b977-eb18101335fd"
      },
      "source": [
        "import os\n",
        "list_of_files = {}\n",
        "for (dirpath, dirnames, filenames) in os.walk(\"/AddingNoise2/Job 1/model0/forgetdata/\"):\n",
        "    for filename in filenames:\n",
        "        print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forgetstatsepoch=30.pt\n",
            "forgetstatsepoch=10.pt\n",
            "forgetstatsepoch=20.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JcwzhVxx6Fn",
        "outputId": "0c1b21a5-01a9-4064-8ee5-08594039c5e8"
      },
      "source": [
        "len(os.listdir('/AddingNoise2/Job 1/model0/clones/'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7sVSVknqcb-"
      },
      "source": [
        "import torch\n",
        "import os, os.path\n",
        "import sys\n",
        "sys.path.append(\"/\")\n",
        "sys.path.append(\"/Forget/open_lth/\")\n",
        "from Forget.open_lth.foundations import hparams\n",
        "from Forget.open_lth.models import registry\n",
        "\n",
        "class postProcess:\n",
        "    \"\"\"\n",
        "    Go through clones/ and for each clone load the forget_correct dataset and classify it.\n",
        "    \"\"\"\n",
        "    def __init__(self, config_file = \"/Forget/config/default_config.ini\"):\n",
        "        self.reader = parser.readConfig(config_file)\n",
        "        self.exp_name = self.reader.exp_info[\"name\"]\n",
        "        #self.num_jobs = int(self.reader.exp_info[\"number of jobs\"])\n",
        "        \n",
        "        self.list_clone_folders = []\n",
        "        self.list_model_folders = []\n",
        "        self.num_examples = []\n",
        "        self.num_forgotten_correct = []\n",
        "        self.model_counts = []\n",
        "        print(f\"Reading from clones files in experiment {self.exp_name}...\")\n",
        "        for job in self.reader.jobs: #remember to put everything into forgetdata\n",
        "            clone_subdir = [\"/\"+self.exp_name + \"/\" + job + \"/\" + f.name + \"/clones/\" for f in os.scandir(\"/\" + self.exp_name + \"/\" + job + \"/\") if f.is_dir()]\n",
        "            model_subdir = [\"/\"+self.exp_name + \"/\" + job + \"/\" + f.name for f in os.scandir(\"/\" + self.exp_name + \"/\" + job + \"/\") if f.is_dir()]\n",
        "            for dir in model_subdir:\n",
        "                self.num_examples.append(torch.load(dir + \"/forgetdata/num_forgotten.pt\")[1])\n",
        "                self.list_model_folders.append(dir)\n",
        "            \n",
        "            for dir in clone_subdir:\n",
        "                self.list_clone_folders.append(dir)\n",
        "                self.model_counts.append(len(os.listdir(dir)))\n",
        "\n",
        "            if self.reader.jobs[job][\"model parameters\"] == \"default\":\n",
        "                self.model_hparams = hparams.ModelHparams('cifar_resnet_20', 'kaiming_uniform', 'uniform')\n",
        "            else:\n",
        "                pass #to add in\n",
        "            self.max_epoch = int(self.reader.jobs[job][\"num epochs\"])\n",
        "        \n",
        "        print(f\"Model counts: {self.model_counts}\")\n",
        "        print(f\"Clone folders: {self.list_clone_folders}\")\n",
        "        #scan and find models\n",
        "\n",
        "        self.epsilons = getEpsilons()\n",
        "        self.totalEpsilons = list()\n",
        "        self.totalForgotten = list()\n",
        "        \n",
        "        for clone_idx, clone_dir in enumerate(self.list_clone_folders):\n",
        "            self.train_set = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])) # torch.load(self.list_model_folders[clone_idx] + \"/forgetdata/trainset.pt\")\n",
        "            self.forgot_correct_mask = torch.load(self.list_model_folders[clone_idx] + \"/forgetdata/forgetmask_correct_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "            self.forgot_correct_dataset = torch.utils.data.Subset(self.train_set, self.forgot_correct_mask)\n",
        "            self.forget_correct_dataloader = torch.utils.data.DataLoader(self.forgot_correct_dataset, batch_size = 128) #to change: this call to batch_size\n",
        "            self.model_list = []\n",
        "            \n",
        "            for idx in range(self.model_counts[clone_idx]):\n",
        "                self.model = registry.get(self.model_hparams).cuda()\n",
        "                self.model.load_state_dict(torch.load(clone_dir+str(idx)+\".pt\"))\n",
        "                self.model_list.append(self.model)\n",
        "\n",
        "            catalog = self.classifyDataset(self.forget_correct_dataloader, self.model_list, self.num_examples[clone_idx])\n",
        "            torch.save(catalog, self.list_model_folders[clone_idx]+\"/catalog.pt\")\n",
        "\n",
        "            self.forget_correct_stats = torch.load(self.list_model_folders[clone_idx] + \"/forgetdata/forgotten_correct_stats_epoch=\" + str(self.max_epoch) + \".pt\")\n",
        "            self.epsilonForgotten, self.timesForgotten = self.tabulateNoiseForget(catalog, self.epsilons, self.forget_correct_stats)\n",
        "            self.totalEpsilons.append(self.epsilonForgotten)\n",
        "            self.totalForgotten.append(self.timesForgotten)\n",
        "            torch.save(torch.tensor(self.epsilonForgotten),  self.list_model_folders[clone_idx]+\"/epsilonForgotten.pt\")\n",
        "            torch.save(torch.tensor(self.timesForgotten),  self.list_model_folders[clone_idx]+\"/timesForgotten.pt\")\n",
        "\n",
        "            self.totalEpsilonsTensor = torch.flatten(torch.Tensor(self.totalEpsilons))\n",
        "            self.totalForgottenTensor = torch.flatten(torch.Tensor(self.totalForgotten))\n",
        "\n",
        "            torch.save(self.totalEpsilonsTensor, self.list_model_folders[clone_idx]+\"/epsilontotal.pt\")\n",
        "            torch.save(self.totalForgottenTensor, self.list_model_folders[clone_idx]+\"/timesforgottentotal.pt\")\n",
        "\n",
        "    #measure at which noise level an example that's classified correctly becomes misclassifed\n",
        "    #this function just classifies a dataset given a model\n",
        "    def classifyDataset(self, data_loader, models, num_examples):\n",
        "        if self.num_examples==None:\n",
        "            raise ValueError(\"Specify the size of the dataset please.\")\n",
        "\n",
        "        num_models = len(models)\n",
        "        __catalog = torch.zeros(num_models, num_examples)\n",
        "\n",
        "        num_ex_per_batch = list()\n",
        "        for batch in data_loader:\n",
        "            num_ex_per_batch.append(len(batch[0]))\n",
        "        print(f\"Classifying dataset... examples/batch: {num_ex_per_batch}\")\n",
        "        \n",
        "        modeltrcker = 0\n",
        "        for model in models:\n",
        "            model.eval()\n",
        "\n",
        "            btrkcer = 0\n",
        "            for batch in data_loader:\n",
        "                x,y = batch\n",
        "                x=x.cuda()\n",
        "                with torch.no_grad():\n",
        "                    l_A = model(x)\n",
        "                for k in range(len(l_A)):\n",
        "                    if torch.argmax(l_A[k]) == y.cuda()[k]:\n",
        "                        #print(f\"{modeltrcker}, {k+sum(num_ex_per_batch[0:btrkcer])}\")\n",
        "                        __catalog[modeltrcker, k+sum(num_ex_per_batch[0:btrkcer])] = 1\n",
        "                btrkcer+=1        \n",
        "                \n",
        "            modeltrcker+=1\n",
        "\n",
        "        return __catalog\n",
        "    \n",
        "    #returns a table consisting of {epsilon at which example was forgotten, times it was forgotten}\n",
        "    def tabulateNoiseForget(self, catalog, epsilonList, forgetCorrectStats):\n",
        "        epsilonForgotten = list()\n",
        "        timesForgotten = list()\n",
        "\n",
        "        for k in range(len(catalog[0])): #go through each example\n",
        "            idx = next((i for i in range(len(catalog[0:,k])) if catalog[0:,k][i] == 0), None)\n",
        "            if idx != None:\n",
        "                epsilonForgotten.append(epsilonList[idx])\n",
        "                timesForgotten.append(forgetCorrectStats[k])\n",
        "        \n",
        "        return epsilonForgotten, timesForgotten\n",
        "\n",
        "    #For a given set of epsilon, forget stats, it scans through and determines the\n",
        "    #largest N epsilons for a given # of forgotten events before the example was\n",
        "    #misclassifed\n",
        "    def findLargestEpsilon(self, epsilonForgotten, timesForgotten, largestN):\n",
        "        import heapq\n",
        "        largest_value = int(max(timesForgotten))\n",
        "        smallest_value = int(min(timesForgotten))\n",
        "\n",
        "        largest_epsilon = torch.zeros(largest_value-smallest_value+1, largestN)\n",
        "        largest_forgotten = torch.zeros(largest_value-smallest_value+1, largestN)\n",
        "\n",
        "        for j in range(smallest_value, largest_value+1):\n",
        "            idx = [i for i in range(len(timesForgotten)) if timesForgotten[i]==j]\n",
        "            for k in range(len(timesForgotten)):\n",
        "                largest_forgotten[j-smallest_value, 0:] = torch.tensor([j]*largestN)\n",
        "                largest_epsilon[j-smallest_value, 0:] = torch.tensor(heapq.nlargest(largestN,[epsilonForgotten[i] for i in idx]))\n",
        "\n",
        "        return torch.flatten(largest_epsilon), torch.flatten(largest_forgotten)\n",
        "\n",
        "def getEpsilons(num_points = 200, min_noise = 0., max_noise = 0.1):\n",
        "        return np.linspace(min_noise, max_noise, num_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeW2ibgD6kM3",
        "outputId": "479a6af8-233e-4a2e-be38-e3552461765a"
      },
      "source": [
        "#mydataloader = torch.utils.data.DataLoader(torch.load(\"/AddingNoise2/Job 1/model0/forgetdata/forgotten_correct_dataloader_epoch=30.pt\"), 128)\n",
        "train_set = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])) # torch.load(\"/AddingNoise2/Job 1/model0/forgetdata/trainset.pt\")\n",
        "forgot_correct_mask = torch.load(\"/AddingNoise2/Job 1/model0/forgetdata/forgetmask_correct_epoch=30.pt\")\n",
        "forgot_correct_dataset = torch.utils.data.Subset(train_set, forgot_correct_mask)\n",
        "mydataloader = torch.utils.data.DataLoader(forgot_correct_dataset, batch_size = 128) #to change: this call to batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "-NC-V7QuGMkY",
        "outputId": "97330c2c-c4a1-4ea7-8ee8-c281abae2ce0"
      },
      "source": [
        "pprocess = postProcess()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from clones files in experiment AddingNoise2...\n",
            "Model counts: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
            "Clone folders: ['/AddingNoise2/Job 1/model2/clones/', '/AddingNoise2/Job 1/model1/clones/', '/AddingNoise2/Job 1/model3/clones/', '/AddingNoise2/Job 1/model0/clones/', '/AddingNoise2/Job 1/model4/clones/', '/AddingNoise2/Job 2/model2/clones/', '/AddingNoise2/Job 2/model1/clones/', '/AddingNoise2/Job 2/model3/clones/', '/AddingNoise2/Job 2/model0/clones/', '/AddingNoise2/Job 2/model4/clones/']\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 36]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ca9a5b3262f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-d21692927a7b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_file)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_model_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/forgetdata/forgetstatsepoch=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilonForgotten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesForgotten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabulateNoiseForget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotalEpsilons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilonForgotten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotalForgotten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesForgotten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d21692927a7b>\u001b[0m in \u001b[0;36mtabulateNoiseForget\u001b[0;34m(self, catalog, epsilonList, forgetStats)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mepsilonForgotten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilonList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mtimesForgotten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforgetStats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mepsilonForgotten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesForgotten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 30 is out of bounds for dimension 0 with size 30"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9tHhQSU9wkY",
        "outputId": "0598657e-58f9-4d68-c2dc-13a2abf2360b"
      },
      "source": [
        "next(iter(mydataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[[ 1.2557,  1.2728,  1.3242,  ...,  1.5982,  1.5982,  1.5982],\n",
              "           [ 1.1872,  1.2214,  1.2728,  ...,  1.5810,  1.5639,  1.5468],\n",
              "           [ 1.1700,  1.2043,  1.2557,  ...,  1.5982,  1.5810,  1.5639],\n",
              "           ...,\n",
              "           [ 0.5536,  0.5536,  0.5536,  ..., -0.4397, -0.1143,  0.0741],\n",
              "           [ 0.5878,  0.6049,  0.5193,  ...,  0.1254, -0.1143, -0.5596],\n",
              "           [ 0.4851,  0.4679,  0.3652,  ..., -0.5596, -0.5767, -0.7650]],\n",
              " \n",
              "          [[ 1.2381,  1.2556,  1.3081,  ...,  1.4832,  1.4832,  1.4832],\n",
              "           [ 1.1681,  1.2031,  1.2556,  ...,  1.4657,  1.4657,  1.4307],\n",
              "           [ 1.1506,  1.1856,  1.2381,  ...,  1.4832,  1.4657,  1.4482],\n",
              "           ...,\n",
              "           [ 0.5203,  0.5203,  0.5203,  ..., -0.6527, -0.3550, -0.1975],\n",
              "           [ 0.5553,  0.5728,  0.4853,  ..., -0.1099, -0.3025, -0.7227],\n",
              "           [ 0.4503,  0.4328,  0.3277,  ..., -0.7752, -0.7577, -0.8978]],\n",
              " \n",
              "          [[ 1.4722,  1.4897,  1.5420,  ...,  1.7511,  1.7511,  1.7511],\n",
              "           [ 1.4025,  1.4374,  1.4897,  ...,  1.7337,  1.7163,  1.6988],\n",
              "           [ 1.3851,  1.4200,  1.4722,  ...,  1.7511,  1.7337,  1.7163],\n",
              "           ...,\n",
              "           [ 0.7576,  0.7576,  0.7576,  ..., -0.6715, -0.3753, -0.2532],\n",
              "           [ 0.7925,  0.8099,  0.7228,  ..., -0.1312, -0.3055, -0.6890],\n",
              "           [ 0.6879,  0.6705,  0.5659,  ..., -0.7413, -0.7064, -0.8110]]],\n",
              " \n",
              " \n",
              "         [[[-1.7240, -1.3130, -1.2274,  ...,  0.1254,  0.9988,  1.5639],\n",
              "           [-1.5699, -1.2445, -1.1589,  ...,  0.4337,  1.3755,  1.4612],\n",
              "           [-1.4158, -1.1075, -1.0904,  ...,  0.2453,  1.2385,  1.3584],\n",
              "           ...,\n",
              "           [ 0.7419,  0.9132,  0.9988,  ...,  1.2899,  0.8961,  0.3652],\n",
              "           [ 0.7248,  0.7077,  0.7077,  ...,  1.0159,  1.0159,  1.1187],\n",
              "           [ 0.8789,  0.8447,  0.8447,  ...,  1.1358,  1.1015,  1.1700]],\n",
              " \n",
              "          [[-1.5630, -1.1779, -1.2304,  ...,  0.2402,  1.1155,  1.6758],\n",
              "           [-1.4930, -1.1779, -1.1779,  ...,  0.5553,  1.5707,  1.7633],\n",
              "           [-1.3880, -1.0378, -1.0028,  ...,  0.3803,  1.4657,  1.7633],\n",
              "           ...,\n",
              "           [ 0.8880,  1.0630,  1.1506,  ...,  1.3256,  0.9580,  0.4153],\n",
              "           [ 0.8704,  0.8529,  0.8529,  ...,  1.1331,  1.1506,  1.2381],\n",
              "           [ 1.0280,  0.9930,  0.9930,  ...,  1.2381,  1.2206,  1.2731]],\n",
              " \n",
              "          [[-1.4210, -1.0898, -1.2816,  ...,  0.2348,  1.2282,  2.2043],\n",
              "           [-1.3861, -1.1421, -1.2119,  ...,  0.5136,  1.6291,  2.2391],\n",
              "           [-1.3687, -0.9853, -0.9156,  ...,  0.3742,  1.5768,  2.2566],\n",
              "           ...,\n",
              "           [ 1.1237,  1.2805,  1.3677,  ...,  1.5420,  1.1759,  0.6356],\n",
              "           [ 1.0888,  1.0714,  1.0714,  ...,  1.3502,  1.3677,  1.4722],\n",
              "           [ 1.2457,  1.2108,  1.2108,  ...,  1.4025,  1.3851,  1.4374]]],\n",
              " \n",
              " \n",
              "         [[[ 0.9646,  0.9988,  0.9646,  ...,  0.7077,  0.7248,  0.6906],\n",
              "           [ 0.8104,  0.8618,  0.8104,  ...,  0.6734,  0.6563,  0.6049],\n",
              "           [ 0.7248,  0.7933,  0.7933,  ...,  0.5878,  0.6221,  0.6049],\n",
              "           ...,\n",
              "           [-1.2445, -1.2274, -1.2617,  ..., -0.6452, -0.6109, -0.6623],\n",
              "           [-1.2959, -1.2788, -1.2959,  ..., -0.7479, -0.7308, -0.7993],\n",
              "           [-1.2959, -1.2788, -1.2788,  ..., -0.8678, -0.7822, -0.7822]],\n",
              " \n",
              "          [[ 1.2381,  1.2731,  1.2381,  ...,  0.9405,  0.9580,  0.9230],\n",
              "           [ 1.0805,  1.1331,  1.0805,  ...,  0.9055,  0.8880,  0.8354],\n",
              "           [ 0.9930,  1.0630,  1.0630,  ...,  0.8179,  0.8529,  0.8179],\n",
              "           ...,\n",
              "           [-1.0203, -1.0203, -1.0553,  ..., -0.4426, -0.4076, -0.4601],\n",
              "           [-1.0728, -1.0553, -1.0728,  ..., -0.5476, -0.5301, -0.6001],\n",
              "           [-1.0728, -1.0553, -1.0553,  ..., -0.6702, -0.5826, -0.5826]],\n",
              " \n",
              "          [[ 1.3154,  1.3502,  1.3154,  ...,  1.0539,  1.0714,  1.0365],\n",
              "           [ 1.1585,  1.2108,  1.1585,  ...,  1.0191,  1.0017,  0.9494],\n",
              "           [ 1.0714,  1.1411,  1.1411,  ...,  0.9319,  0.9668,  0.9494],\n",
              "           ...,\n",
              "           [-0.9330, -0.9330, -0.9504,  ..., -0.3230, -0.3055, -0.3404],\n",
              "           [-0.9853, -0.9678, -0.9853,  ..., -0.4275, -0.4101, -0.4798],\n",
              "           [-0.9853, -0.9678, -0.9678,  ..., -0.5495, -0.4624, -0.4624]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[-1.5185, -1.6042, -1.6213,  ..., -0.3198, -0.7993, -0.9363],\n",
              "           [-1.2274, -1.3473, -1.3644,  ..., -0.3712, -0.6623, -0.7479],\n",
              "           [-1.1418, -1.2959, -1.2959,  ...,  0.0569, -0.5596, -0.6281],\n",
              "           ...,\n",
              "           [-0.7479, -0.7137, -0.6109,  ..., -0.7479, -0.5767, -0.3712],\n",
              "           [-0.6281, -0.5938, -0.5938,  ..., -0.7993, -0.5767, -0.3541],\n",
              "           [-0.6281, -0.6794, -0.6965,  ..., -0.6109, -0.3541, -0.1143]],\n",
              " \n",
              "          [[-1.9307, -1.8782, -1.8256,  ..., -0.9328, -1.2304, -1.3529],\n",
              "           [-1.7031, -1.6856, -1.6506,  ..., -1.1429, -1.2654, -1.2654],\n",
              "           [-1.6856, -1.7381, -1.7206,  ..., -0.8102, -1.3179, -1.2829],\n",
              "           ...,\n",
              "           [-1.3880, -1.3354, -1.2479,  ..., -1.3004, -1.2479, -1.1779],\n",
              "           [-1.2479, -1.1954, -1.2129,  ..., -1.3529, -1.2479, -1.1253],\n",
              "           [-1.2479, -1.3004, -1.3179,  ..., -1.1954, -1.0728, -0.9328]],\n",
              " \n",
              "          [[-1.6824, -1.7173, -1.6650,  ..., -1.1073, -1.3164, -1.3513],\n",
              "           [-1.4733, -1.5779, -1.5081,  ..., -1.3339, -1.2816, -1.3339],\n",
              "           [-1.5081, -1.6650, -1.5779,  ..., -1.3513, -1.4733, -1.4384],\n",
              "           ...,\n",
              "           [-1.5081, -1.3861, -1.2816,  ..., -1.3861, -1.3861, -1.3513],\n",
              "           [-1.3861, -1.2816, -1.2467,  ..., -1.4384, -1.3861, -1.3164],\n",
              "           [-1.3861, -1.3513, -1.3513,  ..., -1.3164, -1.2467, -1.1596]]],\n",
              " \n",
              " \n",
              "         [[[-0.4054, -0.7137, -0.6281,  ...,  0.0398, -0.1999, -0.3027],\n",
              "           [-0.4397, -0.5596, -0.4054,  ..., -0.0972, -0.3369, -0.3369],\n",
              "           [-0.4054, -0.5767, -0.7993,  ..., -0.1999, -0.5082, -0.4911],\n",
              "           ...,\n",
              "           [-0.5767, -0.7308, -0.8164,  ...,  0.0398, -0.0116,  0.0912],\n",
              "           [-0.4397, -0.3369, -0.6109,  ..., -0.7137, -0.5767, -0.3027],\n",
              "           [-0.3198, -0.5767, -0.5424,  ..., -0.4226, -0.0972, -0.0458]],\n",
              " \n",
              "          [[-0.0224, -0.0924, -0.2325,  ...,  0.3978,  0.2402,  0.0651],\n",
              "           [-0.0574, -0.0749, -0.1099,  ...,  0.3627, -0.0574,  0.1702],\n",
              "           [-0.0399, -0.2675, -0.5301,  ...,  0.2402, -0.0224,  0.0826],\n",
              "           ...,\n",
              "           [-0.0049, -0.1800, -0.4776,  ...,  0.0301,  0.1702,  0.3627],\n",
              "           [-0.1275,  0.2227, -0.1450,  ..., -0.6352, -0.3550, -0.0924],\n",
              "           [ 0.0826, -0.0574, -0.1625,  ..., -0.4076, -0.0399,  0.0301]],\n",
              " \n",
              "          [[-0.9156, -1.3861, -1.1421,  ..., -0.7413, -1.0027, -1.0550],\n",
              "           [-0.8981, -1.2119, -0.9678,  ..., -0.8458, -1.1247, -1.0027],\n",
              "           [-0.8284, -1.0201, -1.1770,  ..., -0.9678, -1.3513, -1.1596],\n",
              "           ...,\n",
              "           [-1.1770, -1.2816, -1.2293,  ..., -0.3753, -0.4101, -0.4798],\n",
              "           [-0.9678, -1.1421, -1.0898,  ..., -0.9330, -0.8458, -0.7761],\n",
              "           [-0.8807, -1.1944, -0.9504,  ..., -0.8284, -0.6367, -0.6890]]],\n",
              " \n",
              " \n",
              "         [[[-0.9020, -0.7993, -0.7479,  ..., -0.9363, -0.6281, -1.1932],\n",
              "           [-0.9192, -1.1247, -0.8849,  ..., -0.7479, -0.6794, -1.2788],\n",
              "           [-0.9877, -1.1932, -0.8335,  ..., -0.7479, -1.1075, -1.5014],\n",
              "           ...,\n",
              "           [-0.0116, -0.1828,  0.0056,  ...,  0.9132,  0.8961,  0.7933],\n",
              "           [-0.2856, -0.2171, -0.1999,  ...,  0.9474,  0.8276,  0.7248],\n",
              "           [-0.3369, -0.4054, -0.3883,  ...,  0.9817,  0.9817,  0.8961]],\n",
              " \n",
              "          [[-0.7402, -0.4251, -0.1275,  ..., -0.4601, -0.1275, -0.5826],\n",
              "           [-0.8803, -0.8803, -0.4076,  ..., -0.3200, -0.2675, -0.7752],\n",
              "           [-0.8452, -0.9853, -0.5651,  ..., -0.3725, -0.7752, -1.0728],\n",
              "           ...,\n",
              "           [ 0.2927,  0.1702,  0.4328,  ...,  0.7479,  0.7304,  0.6429],\n",
              "           [ 0.1352,  0.3627,  0.5028,  ...,  0.8179,  0.6779,  0.5728],\n",
              "           [ 0.2927,  0.3277,  0.4328,  ...,  0.8354,  0.8354,  0.7479]],\n",
              " \n",
              "          [[-0.9504, -0.8458, -0.6541,  ..., -0.9678, -0.6715, -1.0898],\n",
              "           [-0.9330, -1.1596, -0.8458,  ..., -0.7761, -0.7064, -1.1073],\n",
              "           [-1.0201, -1.2641, -0.8284,  ..., -0.8110, -1.1421, -1.2816],\n",
              "           ...,\n",
              "           [-0.0267, -0.1312,  0.0779,  ...,  0.8274,  0.8274,  0.7402],\n",
              "           [-0.2532, -0.3230, -0.2184,  ...,  0.8971,  0.7576,  0.6356],\n",
              "           [-0.2184, -0.3578, -0.2881,  ...,  0.9145,  0.9145,  0.8274]]]]),\n",
              " tensor([2, 6, 8, 5, 6, 4, 9, 5, 3, 3, 9, 6, 0, 0, 5, 0, 8, 8, 7, 0, 8, 3, 9, 5,\n",
              "         3, 0, 1, 1, 1, 2, 3, 3, 1, 0, 1, 8, 0, 1, 5, 7, 4, 8, 8, 3, 4, 4, 4, 4,\n",
              "         6, 2, 2, 1, 3, 6, 6, 3, 5, 3, 8, 3, 9, 8, 6, 8, 3, 0, 6, 6, 3, 7, 7, 6,\n",
              "         3, 0, 2, 5, 7, 0, 8, 5, 8, 3, 7, 1, 4, 2, 6, 5, 0, 7, 1, 1, 3, 1, 6, 3,\n",
              "         7, 2, 6, 1, 6, 1, 8, 2, 5, 4, 1, 1, 3, 3, 1, 3, 1, 6, 1, 6, 0, 9, 3, 3,\n",
              "         0, 0, 1, 3, 4, 1, 6, 2])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S0lmqB77wkR",
        "outputId": "93bc5aaf-0ab5-4d62-e899-130aa6cf5677"
      },
      "source": [
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "cifar10 = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "subset=[0,10,20,30,40,50]\n",
        "dataloader = torch.utils.data.DataLoader(cifar10, batch_size=128, num_workers = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "j761wFs09Ugy",
        "outputId": "1f2939ce-11fc-4a3b-e7f1-25c9b64a5538"
      },
      "source": [
        "torch.save(dataloader, \"mydataloader.pt\")\n",
        "del(dataloader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-baf9191c2989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mydataloader.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mydataloader.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mydataloader.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6HDksq09k4h",
        "outputId": "8b87f627-0605-42a7-e131-3ea232d6e63b"
      },
      "source": [
        "newdataloader = torch.load(\"/content/mydataloader.pt\")\n",
        "next(iter(newdataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[[-1.1075, -1.3815, -1.2617,  ...,  0.5878,  0.4851,  0.4166],\n",
              "           [-1.8439, -2.1179, -1.8097,  ..., -0.0116, -0.0801, -0.0287],\n",
              "           [-1.6898, -1.8439, -1.2788,  ..., -0.0972, -0.0629, -0.2513],\n",
              "           ...,\n",
              "           [ 1.4440,  1.3242,  1.2728,  ...,  0.6221, -1.1589, -1.2103],\n",
              "           [ 0.9646,  0.8447,  1.0673,  ...,  1.0331, -0.4568, -0.6965],\n",
              "           [ 0.9132,  0.7591,  0.9474,  ...,  1.5810,  0.4679, -0.0116]],\n",
              " \n",
              "          [[-0.9503, -1.2304, -1.1954,  ...,  0.2752,  0.1527,  0.1352],\n",
              "           [-1.6856, -2.0357, -1.8957,  ..., -0.4951, -0.5826, -0.5126],\n",
              "           [-1.6155, -1.9132, -1.5630,  ..., -0.5651, -0.5651, -0.7577],\n",
              "           ...,\n",
              "           [ 0.9405,  0.6429,  0.7829,  ...,  0.2927, -1.4930, -1.4405],\n",
              "           [ 0.3978,  0.1176,  0.4853,  ...,  0.5553, -0.9503, -1.1078],\n",
              "           [ 0.4853,  0.2227,  0.4503,  ...,  1.1856,  0.0301, -0.4251]],\n",
              " \n",
              "          [[-0.7064, -1.0201, -1.0550,  ...,  0.0779, -0.0267, -0.0092],\n",
              "           [-1.4559, -1.8044, -1.8044,  ..., -0.8458, -0.9330, -0.8110],\n",
              "           [-1.4384, -1.8044, -1.6650,  ..., -0.9330, -0.9330, -1.0724],\n",
              "           ...,\n",
              "           [-0.1312, -1.2119, -1.3513,  ..., -0.5844, -1.6824, -1.4559],\n",
              "           [-0.1312, -1.0724, -1.2816,  ..., -0.1661, -1.2119, -1.2119],\n",
              "           [ 0.2173, -0.1661, -0.2881,  ...,  0.6356, -0.3404, -0.5495]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5193,  0.0398, -0.3198,  ..., -0.5596, -0.6281, -0.7650],\n",
              "           [ 0.2796,  0.3652,  0.0227,  ..., -0.4739, -0.7993, -0.9020],\n",
              "           [ 0.2796,  0.2624, -0.1486,  ..., -0.7650, -0.9534, -0.9705],\n",
              "           ...,\n",
              "           [ 0.8789,  0.5536,  0.5193,  ..., -1.3987, -1.0733, -0.5253],\n",
              "           [ 0.7077,  0.5536,  0.6049,  ..., -0.3541, -0.0116,  0.1254],\n",
              "           [ 0.6734,  0.5878,  0.6734,  ...,  0.3309,  0.3309,  0.3309]],\n",
              " \n",
              "          [[ 1.0630,  0.3627, -0.2150,  ..., -0.3725, -0.4601, -0.6176],\n",
              "           [ 0.7654,  0.6429,  0.1527,  ..., -0.3025, -0.6352, -0.7577],\n",
              "           [ 0.6779,  0.5203, -0.0224,  ..., -0.6001, -0.8102, -0.8277],\n",
              "           ...,\n",
              "           [ 0.8880,  0.6604,  0.7654,  ..., -1.4405, -1.1078, -0.5826],\n",
              "           [ 0.6604,  0.6254,  0.7829,  ..., -0.4076, -0.0399,  0.0826],\n",
              "           [ 0.5553,  0.5553,  0.6954,  ...,  0.2927,  0.3102,  0.2927]],\n",
              " \n",
              "          [[ 1.4548,  0.5659, -0.1487,  ..., -0.5670, -0.5670, -0.5844],\n",
              "           [ 1.1411,  0.8797,  0.2522,  ..., -0.4450, -0.7238, -0.7413],\n",
              "           [ 1.0539,  0.7925,  0.1476,  ..., -0.6890, -0.8458, -0.8458],\n",
              "           ...,\n",
              "           [ 1.0888,  0.9842,  1.1585,  ..., -1.1770, -0.8110, -0.2184],\n",
              "           [ 0.4265,  0.4614,  0.6705,  ..., -0.1312,  0.2871,  0.4788],\n",
              "           [ 0.2871,  0.3219,  0.5136,  ...,  0.6182,  0.6705,  0.7054]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2489,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
              "           [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
              "           [ 2.2489,  2.2318,  2.2318,  ...,  2.2318,  2.2318,  2.2318],\n",
              "           ...,\n",
              "           [-0.1828, -0.2171, -0.3198,  ..., -0.8849, -0.8849, -0.8849],\n",
              "           [-0.2171, -0.3369, -0.4226,  ..., -0.9534, -0.9192, -0.7822],\n",
              "           [-0.3027, -0.4226, -0.4911,  ..., -0.7822, -0.7650, -0.7479]],\n",
              " \n",
              "          [[ 2.4286,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
              "           [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
              "           [ 2.4286,  2.4111,  2.4111,  ...,  2.4111,  2.4111,  2.4111],\n",
              "           ...,\n",
              "           [ 0.0651,  0.0301, -0.0749,  ..., -0.6176, -0.6352, -0.6352],\n",
              "           [ 0.0301, -0.0924, -0.1800,  ..., -0.7227, -0.7052, -0.5651],\n",
              "           [-0.0574, -0.1800, -0.2500,  ..., -0.5476, -0.5476, -0.5301]],\n",
              " \n",
              "          [[ 2.6400,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
              "           [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
              "           [ 2.6400,  2.6226,  2.6226,  ...,  2.6226,  2.6226,  2.6226],\n",
              "           ...,\n",
              "           [ 0.1476,  0.1302,  0.0431,  ..., -0.4101, -0.4275, -0.4275],\n",
              "           [ 0.1128,  0.0082, -0.0964,  ..., -0.5321, -0.4973, -0.3753],\n",
              "           [ 0.0256, -0.0964, -0.1661,  ..., -0.3578, -0.3578, -0.3404]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[ 2.2489,  2.1804,  2.1804,  ...,  2.1975,  2.1804,  2.2489],\n",
              "           [ 2.2489,  2.1633,  2.2489,  ...,  2.2489,  2.2318,  2.2489],\n",
              "           [ 2.2489,  2.1462,  1.9064,  ...,  2.2318,  2.2318,  2.2489],\n",
              "           ...,\n",
              "           [ 2.2489,  2.2318,  2.2318,  ...,  1.3242,  2.0605,  2.2147],\n",
              "           [ 2.2489,  2.2318,  2.2489,  ...,  1.7180,  2.0434,  2.1975],\n",
              "           [ 2.2489,  2.1804,  2.1975,  ...,  2.0434,  2.1462,  2.2489]],\n",
              " \n",
              "          [[ 2.4286,  2.3235,  2.3410,  ...,  2.3761,  2.3585,  2.4286],\n",
              "           [ 2.4286,  2.4111,  2.4286,  ...,  2.4286,  2.4111,  2.4286],\n",
              "           [ 2.4286,  2.3936,  2.0084,  ...,  2.4111,  2.4111,  2.4286],\n",
              "           ...,\n",
              "           [ 2.4286,  2.4111,  2.4111,  ...,  1.2731,  2.1835,  2.4286],\n",
              "           [ 2.4286,  2.4111,  2.4286,  ...,  1.7458,  2.2185,  2.4286],\n",
              "           [ 2.4286,  2.3585,  2.3761,  ...,  2.1485,  2.3235,  2.4286]],\n",
              " \n",
              "          [[ 2.6400,  2.5877,  2.5877,  ...,  2.5877,  2.5703,  2.6400],\n",
              "           [ 2.6400,  2.6226,  2.6400,  ...,  2.6400,  2.6226,  2.6400],\n",
              "           [ 2.6400,  2.5354,  2.1346,  ...,  2.6226,  2.6226,  2.6400],\n",
              "           ...,\n",
              "           [ 2.6400,  2.6226,  2.6226,  ...,  0.9145,  1.9428,  2.4134],\n",
              "           [ 2.6400,  2.6226,  2.6400,  ...,  1.6640,  2.2043,  2.5006],\n",
              "           [ 2.6400,  2.5703,  2.5877,  ...,  2.2566,  2.4657,  2.5877]]],\n",
              " \n",
              " \n",
              "         [[[ 0.7419,  0.7591,  0.7933,  ...,  1.5810,  1.5639,  1.5297],\n",
              "           [ 0.6906,  0.6906,  0.7419,  ...,  1.6667,  1.6495,  1.6324],\n",
              "           [ 0.6734,  0.6906,  0.7248,  ...,  1.7694,  1.7694,  1.7523],\n",
              "           ...,\n",
              "           [-0.2513, -0.2684, -0.2513,  ...,  0.0227,  0.2796,  0.2796],\n",
              "           [-0.2684, -0.2856, -0.2856,  ...,  0.0398,  0.3138,  0.2282],\n",
              "           [-0.3369, -0.3369, -0.3369,  ..., -0.0458,  0.0912,  0.0569]],\n",
              " \n",
              "          [[ 1.3606,  1.3606,  1.4132,  ...,  1.9209,  1.9034,  1.8683],\n",
              "           [ 1.2906,  1.2906,  1.3431,  ...,  1.9559,  1.9209,  1.9034],\n",
              "           [ 1.2906,  1.2906,  1.3431,  ...,  1.9909,  1.9909,  1.9734],\n",
              "           ...,\n",
              "           [ 0.0301, -0.0924, -0.1625,  ..., -0.0749,  0.1702,  0.1702],\n",
              "           [ 0.0126, -0.0924, -0.1450,  ..., -0.0049,  0.2402,  0.0826],\n",
              "           [-0.0399, -0.1099, -0.1800,  ..., -0.0224,  0.0826, -0.0399]],\n",
              " \n",
              "          [[ 1.8034,  1.8034,  1.8557,  ...,  2.2566,  2.2217,  2.2043],\n",
              "           [ 1.7337,  1.7337,  1.7860,  ...,  2.3088,  2.2914,  2.2566],\n",
              "           [ 1.7337,  1.7337,  1.7685,  ...,  2.3786,  2.3611,  2.3437],\n",
              "           ...,\n",
              "           [ 0.2522,  0.1476,  0.1128,  ...,  0.2173,  0.4614,  0.4614],\n",
              "           [ 0.2522,  0.1825,  0.1302,  ...,  0.2173,  0.4614,  0.3219],\n",
              "           [ 0.1999,  0.1651,  0.1128,  ...,  0.1999,  0.3045,  0.2173]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2489,  2.1462,  2.1462,  ...,  2.2318,  2.2318,  2.2318],\n",
              "           [ 2.2489,  1.8893,  2.0777,  ...,  2.2489,  2.2489,  2.2489],\n",
              "           [ 2.1290,  1.7009,  2.0263,  ...,  2.2489,  2.2489,  2.2489],\n",
              "           ...,\n",
              "           [ 0.3823, -0.5082, -0.4226,  ..., -0.7822, -0.4911, -0.2856],\n",
              "           [ 0.5536,  0.2796,  0.1768,  ..., -0.3883, -0.1999,  0.0569],\n",
              "           [ 0.3652,  0.1597, -0.0287,  ..., -0.1999, -0.0287,  0.1768]],\n",
              " \n",
              "          [[ 2.4286,  2.3235,  2.3235,  ...,  2.4111,  2.4111,  2.4111],\n",
              "           [ 2.4286,  2.0609,  2.2535,  ...,  2.4286,  2.4286,  2.4286],\n",
              "           [ 2.3060,  1.8683,  2.2010,  ...,  2.4286,  2.4286,  2.4286],\n",
              "           ...,\n",
              "           [ 0.5203, -0.3901, -0.3025,  ..., -0.6702, -0.3725, -0.1625],\n",
              "           [ 0.6954,  0.4153,  0.3102,  ..., -0.2675, -0.0749,  0.1877],\n",
              "           [ 0.5028,  0.2927,  0.1001,  ..., -0.0749,  0.1001,  0.3102]],\n",
              " \n",
              "          [[ 2.6400,  2.5354,  2.5354,  ...,  2.6226,  2.6226,  2.6226],\n",
              "           [ 2.6400,  2.2740,  2.4657,  ...,  2.6400,  2.6400,  2.6400],\n",
              "           [ 2.5180,  2.0823,  2.4134,  ...,  2.6400,  2.6400,  2.6400],\n",
              "           ...,\n",
              "           [ 0.7402, -0.1661, -0.0790,  ..., -0.4450, -0.1487,  0.0605],\n",
              "           [ 0.9145,  0.6356,  0.5311,  ..., -0.0441,  0.1476,  0.4091],\n",
              "           [ 0.7228,  0.5136,  0.3219,  ...,  0.1476,  0.3219,  0.5311]]]]),\n",
              " tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6,\n",
              "         2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2,\n",
              "         2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9,\n",
              "         6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6,\n",
              "         1, 1, 4, 1, 8, 3, 9, 6, 6, 1, 8, 5, 2, 9, 9, 8, 1, 7, 7, 0, 0, 6, 9, 1,\n",
              "         2, 2, 9, 2, 6, 6, 1, 9])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "KayEsGOLpba5",
        "outputId": "1ec4523d-1f83-43d5-c22f-3dd1101d6c40"
      },
      "source": [
        "pprocess = postProcess()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from clones files in experiment AddingNoise2...\n",
            "Model counts: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
            "Clone folders: ['/AddingNoise2/Job 1/model2/clones/', '/AddingNoise2/Job 1/model1/clones/', '/AddingNoise2/Job 1/model3/clones/', '/AddingNoise2/Job 1/model0/clones/', '/AddingNoise2/Job 1/model4/clones/', '/AddingNoise2/Job 2/model2/clones/', '/AddingNoise2/Job 2/model1/clones/', '/AddingNoise2/Job 2/model3/clones/', '/AddingNoise2/Job 2/model0/clones/', '/AddingNoise2/Job 2/model4/clones/']\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ca9a5b3262f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-16b0c6d3d78d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mcatalog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforgot_correct_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_model_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/catalog.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-16b0c6d3d78d>\u001b[0m in \u001b[0;36mclassifyDataset\u001b[0;34m(self, data_loader, models, num_examples)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0ml_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Forget/open_lth/models/cifar_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 3, 3, 3], but got 3-dimensional input of size [3, 32, 32] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgh7ylvo0ww4",
        "outputId": "5e681c73-82df-456a-e3b2-7b58cda727b6"
      },
      "source": [
        "print(\"hi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGz9e7sExjm2"
      },
      "source": [
        "# Trial run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMCPLkggIOSa"
      },
      "source": [
        "!rm -r \"/AddingNoise2/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX03w1pfISbE"
      },
      "source": [
        "!rm -r \"/content/Forget/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwogn69ZxiOv",
        "outputId": "451f508a-4f0c-4f94-be22-ff67ebe3ef8d"
      },
      "source": [
        "from Forget.main import experiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Appending paths: /content/Forget\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "OK7TcTLKxqrR",
        "outputId": "985e0839-b00d-439b-dc54-eed103a767ff"
      },
      "source": [
        "experiment.run_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise2', 'storage directory': 'default', 'number of models': 6, 'number of jobs': 2}\n",
            "Division of jobs (models/job): [3 3]\n",
            "Starting training...\n",
            "Job 1: {'model parameters': 'default', 'save models': 'true', 'num epochs': '30', 'save every': '10', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Job 2: {'model parameters': 'default', 'save models': 'true', 'num epochs': '30', 'save every': '10', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Now processing output...\n",
            "Current working directory: /content\n",
            "Experiment info: {}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8eb93bf427da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Forget/main/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_file)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Now processing output...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcrtForget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateforgetdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateForgetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Forget/datasets/createforgetdataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, forget_thres, config_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#from the config file, obtain experiment name + number of jobs + number of models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Forget/config/parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_file)\u001b[0m\n",
            "\u001b[0;32m/content/Forget/config/parser.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#make experiment path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Experiment info: {self.exp_info}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_dir_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'name'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOohL9NtR9yl"
      },
      "source": [
        "# Full run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaqYNb35SE9j"
      },
      "source": [
        "!rm -r \"/AddingNoise2/\"\n",
        "!rm -r \"/content/Forget/\"\n",
        "!rm -r \"/content/open_lth/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evx5WICWSASw",
        "outputId": "341a617f-a460-48c4-95f3-233f56695500"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "!git clone https://github.com/nikhilanand91/Forget.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 7.35 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Cloning into 'Forget'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 176 (delta 76), reused 146 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (176/176), 29.86 KiB | 9.95 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUZ1WwDESS4M"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd()+\"/Forget/open_lth/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhKa_j3VSW-h"
      },
      "source": [
        "from Forget.main import experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0EToVj_SaUj",
        "outputId": "f8e651d1-11d9-47d8-a6eb-bca246e16b94"
      },
      "source": [
        "experiment.run_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Appending paths: /content/Forget\n",
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Division of jobs (models/job): [5 5]\n",
            "Starting training...\n",
            "Job 1: {'model parameters': 'default', 'save models': 'true', 'num epochs': '50', 'save every': '10', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Job 2: {'model parameters': 'default', 'save models': 'true', 'num epochs': '50', 'save every': '10', 'dataset': 'cifar10', 'dataset params': 'default', 'measure forget': 'true', 'track correct examples': 'true', 'save forget_dataset': 'true', 'storage directory': 'default', 'model noise': 'true', 'noise parameters': 'default', 'save clones': 'true'}\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Loading train dataset cifar10... batch size 128\n",
            "Files already downloaded and verified\n",
            "Now processing output...\n",
            "Appending path: /content/Forget\n",
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from output files in experiment AddingNoise...\n",
            "Fetching dataset...\n",
            "Files already downloaded and verified\n",
            "['/AddingNoise/Job 1/model2/forgetdata/', '/AddingNoise/Job 1/model1/forgetdata/', '/AddingNoise/Job 1/model3/forgetdata/', '/AddingNoise/Job 1/model0/forgetdata/', '/AddingNoise/Job 1/model4/forgetdata/', '/AddingNoise/Job 2/model2/forgetdata/', '/AddingNoise/Job 2/model1/forgetdata/', '/AddingNoise/Job 2/model3/forgetdata/', '/AddingNoise/Job 2/model0/forgetdata/', '/AddingNoise/Job 2/model4/forgetdata/']\n",
            "Number forgotten: 4364\n",
            "Number forgotten + correct: 2771\n",
            "Number forgotten: 4525\n",
            "Number forgotten + correct: 2788\n",
            "Number forgotten: 5262\n",
            "Number forgotten + correct: 3038\n",
            "Number forgotten: 4925\n",
            "Number forgotten + correct: 3127\n",
            "Number forgotten: 4862\n",
            "Number forgotten + correct: 3256\n",
            "Number forgotten: 4988\n",
            "Number forgotten + correct: 3222\n",
            "Number forgotten: 4088\n",
            "Number forgotten + correct: 2465\n",
            "Number forgotten: 3546\n",
            "Number forgotten + correct: 2280\n",
            "Number forgotten: 4927\n",
            "Number forgotten + correct: 3158\n",
            "Number forgotten: 3549\n",
            "Number forgotten + correct: 2051\n",
            "Appending /content/Forget/open_lth/\n",
            "Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from output files in experiment AddingNoise...\n",
            "['/AddingNoise/Job 1/model2', '/AddingNoise/Job 1/model1', '/AddingNoise/Job 1/model3', '/AddingNoise/Job 1/model0', '/AddingNoise/Job 1/model4', '/AddingNoise/Job 2/model2', '/AddingNoise/Job 2/model1', '/AddingNoise/Job 2/model3', '/AddingNoise/Job 2/model0', '/AddingNoise/Job 2/model4']\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Cloning models...Current working directory: /content\n",
            "Experiment info: {'name': 'AddingNoise', 'storage directory': 'default', 'number of models': 10, 'number of jobs': 2}\n",
            "Reading from clones files in experiment AddingNoise...\n",
            "Model counts: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
            "Clone folders: ['/AddingNoise/Job 1/model2/clones/', '/AddingNoise/Job 1/model1/clones/', '/AddingNoise/Job 1/model3/clones/', '/AddingNoise/Job 1/model0/clones/', '/AddingNoise/Job 1/model4/clones/', '/AddingNoise/Job 2/model2/clones/', '/AddingNoise/Job 2/model1/clones/', '/AddingNoise/Job 2/model3/clones/', '/AddingNoise/Job 2/model0/clones/', '/AddingNoise/Job 2/model4/clones/']\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 83]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 100]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 94]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 55]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 56]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 22]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 33]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 104]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 86]\n",
            "Files already downloaded and verified\n",
            "Classifying dataset... examples/batch: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Forget.main.experiment.run_experiment at 0x7fbecd28df90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuIMPauleBq8",
        "outputId": "47185d4b-ed8a-4fd5-dd27-24963f8759c3"
      },
      "source": [
        "!zip -r \"/AddingNoise.zip\" \"/AddingNoise\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: AddingNoise/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model2/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model2/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model2/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model2/epsilonForgotten.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 2/model2/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model2/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model2/epsilontotal.pt (deflated 65%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/88.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/95.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/92.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/65.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/103.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/93.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/99.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/112.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/105.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/109.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/98.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/108.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/94.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/114.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/120.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/122.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/96.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/100.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/73.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/102.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/101.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/106.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/85.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/111.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/97.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/72.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/83.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model2/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 2/model2/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model2/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 2/model2/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model2/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model2/timesForgotten.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 2/model1/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model1/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model1/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model1/epsilonForgotten.pt (deflated 76%)\n",
            "  adding: AddingNoise/Job 2/model1/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model1/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model1/epsilontotal.pt (deflated 65%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/95.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/92.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/103.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/93.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/99.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/112.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/105.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/109.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/98.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/108.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/114.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/120.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/96.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/100.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/89.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/102.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/101.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/106.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/85.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/111.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/97.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model1/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 2/model1/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model1/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 2/model1/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model1/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model1/timesForgotten.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 2/model3/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model3/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model3/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model3/epsilonForgotten.pt (deflated 76%)\n",
            "  adding: AddingNoise/Job 2/model3/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model3/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model3/epsilontotal.pt (deflated 66%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/95.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/92.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/103.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/84.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/93.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/99.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/112.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/105.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/109.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/98.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/108.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/114.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/120.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/127.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/96.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/100.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/89.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/102.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/101.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/106.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/85.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/111.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/97.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model3/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetmask_correct_epoch=50.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/num_forgotten.pt (deflated 55%)\n",
            "  adding: AddingNoise/Job 2/model3/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model3/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 2/model3/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model3/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model3/timesForgotten.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 2/model0/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model0/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model0/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model0/epsilonForgotten.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 2/model0/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model0/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model0/epsilontotal.pt (deflated 66%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/95.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/110.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/132.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/92.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/103.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/126.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/93.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/99.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/112.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/105.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/109.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/98.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/108.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/114.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/120.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/113.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/127.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/96.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/100.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/119.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/115.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/133.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/102.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/101.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/106.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/118.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/129.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/85.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/104.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/116.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/111.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/97.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model0/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 2/model0/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model0/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 2/model0/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model0/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model0/timesForgotten.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 2/model4/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model4/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model4/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model4/epsilonForgotten.pt (deflated 76%)\n",
            "  adding: AddingNoise/Job 2/model4/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model4/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model4/epsilontotal.pt (deflated 66%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/88.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/95.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/92.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/103.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/91.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/93.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/99.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/112.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/105.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/109.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/98.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/108.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/114.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/120.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/96.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/100.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/125.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/102.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/101.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/106.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/76.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/85.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/111.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/97.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/107.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 2/model4/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetmask_correct_epoch=50.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/num_forgotten.pt (deflated 55%)\n",
            "  adding: AddingNoise/Job 2/model4/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 2/model4/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 2/model4/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model4/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 2/model4/timesForgotten.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 1/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model2/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model2/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model2/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model2/epsilonForgotten.pt (deflated 76%)\n",
            "  adding: AddingNoise/Job 1/model2/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model2/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model2/epsilontotal.pt (deflated 60%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/67.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/95.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/92.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/65.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/74.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/103.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/55.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/62.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/91.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/84.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/93.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/58.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/99.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/77.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/112.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/60.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/105.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/70.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/109.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/98.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/108.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/94.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/114.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/120.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/49.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/86.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/96.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/42.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/7.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/100.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/69.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/1.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/90.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/73.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/89.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/6.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/26.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/52.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/79.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/81.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/102.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/66.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/78.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/57.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/101.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/106.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/48.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/76.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/85.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/46.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/87.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/10.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/64.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/111.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/41.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/97.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/40.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/61.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/59.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/72.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/107.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/54.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model2/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model2/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/correctstatsepoch=50.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 1/model2/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model2/timesforgottentotal.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 1/model2/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model2/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model2/timesForgotten.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 1/model1/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model1/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model1/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model1/epsilonForgotten.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 1/model1/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model1/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model1/epsilontotal.pt (deflated 62%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/95.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/92.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/103.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/91.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/117.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/93.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/99.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/77.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/112.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/105.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/70.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/109.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/98.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/108.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/94.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/114.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/120.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/96.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/42.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/100.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/90.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/102.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/101.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/106.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/85.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/82.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/111.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/31.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/97.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/107.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model1/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 1/model1/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model1/timesforgottentotal.pt (deflated 89%)\n",
            "  adding: AddingNoise/Job 1/model1/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model1/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model1/timesForgotten.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 1/model3/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model3/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model3/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model3/epsilonForgotten.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 1/model3/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model3/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model3/epsilontotal.pt (deflated 63%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/95.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/110.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/92.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/103.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/117.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/93.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/99.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/112.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/105.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/109.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/98.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/108.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/114.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/120.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/113.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/96.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/100.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/69.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/119.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/90.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/125.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/81.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/102.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/101.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/106.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/63.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/76.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/85.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/104.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/116.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/111.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/97.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model3/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 1/model3/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model3/timesforgottentotal.pt (deflated 89%)\n",
            "  adding: AddingNoise/Job 1/model3/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model3/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model3/timesForgotten.pt (deflated 87%)\n",
            "  adding: AddingNoise/Job 1/model0/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model0/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model0/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model0/epsilonForgotten.pt (deflated 77%)\n",
            "  adding: AddingNoise/Job 1/model0/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model0/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model0/epsilontotal.pt (deflated 64%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/95.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/110.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/92.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/103.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/126.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/117.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/93.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/99.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/128.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/112.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/105.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/109.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/98.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/121.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/108.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/114.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/120.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/113.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/122.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/96.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/100.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/119.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/125.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/115.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/130.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/123.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/102.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/101.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/106.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/118.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/134.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/85.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/104.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/116.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/111.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/140.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/97.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model0/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/correctstatsepoch=50.pt (deflated 94%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/num_forgotten.pt (deflated 55%)\n",
            "  adding: AddingNoise/Job 1/model0/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model0/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 1/model0/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model0/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model0/timesForgotten.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 1/model4/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model4/correctdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model4/epoch=50.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model4/epsilonForgotten.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 1/model4/epoch=20.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model4/epoch=10.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model4/epsilontotal.pt (deflated 65%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/159.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/192.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/37.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/67.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/139.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/182.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/161.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/188.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/88.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/45.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/179.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/152.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/95.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/136.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/194.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/110.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/132.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/50.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/92.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/65.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/149.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/74.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/162.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/8.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/103.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/80.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/56.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/189.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/55.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/62.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/36.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/135.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/91.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/126.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/117.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/84.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/38.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/33.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/93.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/4.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/150.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/18.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/58.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/166.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/184.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/170.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/99.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/35.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/153.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/131.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/176.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/128.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/30.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/77.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/112.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/172.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/43.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/60.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/105.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/70.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/142.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/109.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/44.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/137.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/98.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/121.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/108.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/144.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/169.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/94.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/114.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/120.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/141.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/21.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/49.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/113.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/148.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/122.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/156.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/86.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/138.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/127.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/158.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/171.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/96.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/165.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/42.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/186.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/7.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/12.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/100.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/180.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/69.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/198.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/119.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/1.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/25.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/90.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/68.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/124.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/125.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/73.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/115.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/130.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/89.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/190.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/6.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/20.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/164.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/26.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/145.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/52.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/79.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/47.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/196.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/53.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/123.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/133.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/11.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/3.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/147.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/146.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/168.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/81.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/24.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/157.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/34.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/102.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/167.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/195.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/66.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/78.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/57.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/101.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/106.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/63.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/160.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/48.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/27.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/187.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/193.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/175.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/29.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/118.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/14.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/16.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/183.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/173.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/134.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/76.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/129.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/178.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/85.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/163.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/5.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/17.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/32.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/13.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/82.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/181.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/177.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/28.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/46.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/87.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/154.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/10.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/143.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/104.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/191.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/151.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/64.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/9.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/116.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/174.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/39.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/111.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/140.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/31.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/197.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/199.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/41.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/97.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/15.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/40.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/22.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/75.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/61.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/155.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/2.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/185.pt (deflated 10%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/59.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/23.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/19.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/0.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/72.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/107.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/83.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/51.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/54.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/clones/71.pt (deflated 11%)\n",
            "  adding: AddingNoise/Job 1/model4/catalog.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/ (stored 0%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/full_forgotten_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetmask_correct_epoch=50.pt (deflated 78%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetstatsepoch=50.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/correctstatsepoch=10.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgotten_correct_stats_epoch=50.pt (deflated 88%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/correctstatsepoch=30.pt (deflated 96%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetstatsepoch=40.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgotten_correct_dataloader_epoch=50.pt (deflated 30%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/correctstatsepoch=40.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetmask_epoch=50.pt (deflated 79%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/correctstatsepoch=20.pt (deflated 98%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetstatsepoch=20.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetstatsepoch=10.pt (deflated 100%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/correctstatsepoch=50.pt (deflated 95%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/num_forgotten.pt (deflated 54%)\n",
            "  adding: AddingNoise/Job 1/model4/forgetdata/forgetstatsepoch=30.pt (deflated 99%)\n",
            "  adding: AddingNoise/Job 1/model4/timesforgottentotal.pt (deflated 90%)\n",
            "  adding: AddingNoise/Job 1/model4/epoch=30.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model4/epoch=40.pt (deflated 9%)\n",
            "  adding: AddingNoise/Job 1/model4/timesForgotten.pt (deflated 88%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvA5AjjL_r21"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/AddingNoise.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaG1Qs6vfMM8"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGrzltE4jV6o"
      },
      "source": [
        "totaltimesforgotten = torch.load(\"/AddingNoise/Job 2/model4/timesforgottentotal.pt\")\n",
        "totalepsilons = torch.load(\"/AddingNoise/Job 2/model4/epsilontotal.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIV57EeujdfI"
      },
      "source": [
        "timesforgottenmodel4 = torch.load(\"/AddingNoise/Job 2/model4/timesForgotten.pt\")\n",
        "epsilonsmodel4 = torch.load(\"/AddingNoise/Job 2/model4/epsilonForgotten.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY7u5M0WjzKJ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 14}\n",
        "\n",
        "matplotlib.rc('font', **font)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "VlRqiuRekxPC",
        "outputId": "7e1551bb-41ea-45de-9afb-3402766ecb10"
      },
      "source": [
        "plt.scatter(epsilonsmodel4, timesforgottenmodel4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "DhK6Tf0ej9O_",
        "outputId": "95b434c9-90c0-4a44-e34d-16c5a759812e"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "full_plot = plt.scatter(totalepsilons, totaltimesforgotten)\n",
        "plt.xlabel('$\\epsilon$')\n",
        "plt.ylabel('times forgotten')\n",
        "plt.show(full_plot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yv4A31tkVUD"
      },
      "source": [
        "idx_3 = [i for i in range(len(totaltimesforgotten)) if totaltimesforgotten[i]==3]\n",
        "epsilons_3 = totalepsilons[idx_3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "6oJRzvFWlPUc",
        "outputId": "8d5723de-c144-4cd6-cc4e-930e7a2cb564"
      },
      "source": [
        "length = len(epsilons_3)\n",
        "hist_3 = plt.hist(epsilons_3, alpha=0.5, weights = np.ones(length)/length)\n",
        "plt.ylabel('Fraction of examples forgotten 3 times')\n",
        "plt.xlabel('Examples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBQCmZHarGhg",
        "outputId": "682572c6-a2f9-457e-8e99-5ce24d2f6071"
      },
      "source": [
        "test = np.array([])\n",
        "np.append(test, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_p4I_-yrTvG",
        "outputId": "5f54fff4-acce-4ca5-cc8f-706cd68f8605"
      },
      "source": [
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6_6V41apQEK"
      },
      "source": [
        "import numpy as np\n",
        "epsilon_means = np.array([])\n",
        "epsilon_errors = np.array([])\n",
        "for n in range(3,11):\n",
        "    idx_n = [i for i in range(len(totaltimesforgotten)) if totaltimesforgotten[i]==n]\n",
        "    epsilons_n = totalepsilons[idx_n]\n",
        "    epsilon_n_mean = torch.mean(epsilons_n)\n",
        "    epsilon_means = np.append(epsilon_means, epsilon_n_mean)\n",
        "    epsilon_n_std = torch.std(epsilons_n)\n",
        "    epsilon_errors = np.append(epsilon_errors, epsilon_n_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "aFHXlCtgq-eY",
        "outputId": "c2d34d4a-b840-412f-892a-756e09b4181d"
      },
      "source": [
        "mean_and_error = plt.errorbar(epsilon_means, np.arange(3,11), yerr=None, xerr = epsilon_errors, linestyle='None', fmt='-o')\n",
        "plt.xlabel('Noise level $\\epsilon$')\n",
        "plt.ylabel('Times forgotten')\n",
        "plt.show(mean_and_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "51s6jzv4mD7B",
        "outputId": "0daf37b9-d977-4e88-c222-2922ab97212a"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [30, 12]\n",
        "fig, ax = plt.subplots(nrows=2, ncols=4)\n",
        "for n in range(3,11):\n",
        "    idx_n = [i for i in range(len(totaltimesforgotten)) if totaltimesforgotten[i]==n]\n",
        "    epsilons_n = totalepsilons[idx_n]\n",
        "    length = len(epsilons_n)\n",
        "    \n",
        "    plt.subplot(2,4,n-2)\n",
        "    plt.hist(epsilons_n, alpha=0.5, weights = np.ones(length)/length)\n",
        "    plt.ylabel('Examples forgotten {} times (fraction)'.format(n))\n",
        "    plt.xlabel('Noise level $\\epsilon$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x864 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj8gnCSBu0LL"
      },
      "source": [
        "import scipy.stats as ss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf3zimUBu4r0"
      },
      "source": [
        "totalforgotten_ranked = ss.rankdata(totaltimesforgotten)\n",
        "totalepsilons_ranked = ss.rankdata(totalepsilons)\n",
        "totalforgotten_ranked_std = np.std(totalforgotten_ranked)\n",
        "totalepsilons_ranked_std = np.std(totalepsilons_ranked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5RNtrYjwji7",
        "outputId": "6ca9001f-ebc4-45fd-f23c-0070622d834b"
      },
      "source": [
        "len(totalforgotten_ranked)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekQUdDyqw2tX"
      },
      "source": [
        "spearmanr, p_value = ss.spearmanr(totalepsilons_ranked, totalforgotten_ranked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVnQcUpQxC4G",
        "outputId": "1fbf20fa-48c0-44e7-c49d-daacca10a140"
      },
      "source": [
        "spearmanr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.02336989991509918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB5n3pXTnQPV"
      },
      "source": [
        "covariance = np.cov(totalepsilons_ranked, totalforgotten_ranked)[0,1]/(totalforgotten_ranked_std*totalepsilons_ranked_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FQUzQZNvqvx",
        "outputId": "1a5ef904-e0be-41b2-915b-96b814bc0bc8"
      },
      "source": [
        "covariance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.023370729959493252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w1C3IYgjsRQ"
      },
      "source": [
        "# Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZVA7ipojrtM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "60bfbf765a804650b4e4e089ea7fe56d",
            "14bb6a1a0d1b466eab8e1a2d80ae08d9",
            "f565a7f6ec934c8c8be6375ae9f1b7b7",
            "8940f65f5792489a9b7847fb428ab3d6",
            "5bd28a0c4c534be0a23896225e80a0cf",
            "bdcbb1f40d8b45c6a66c679954cceeb8",
            "d0a70b26bfa44fb6a4850ad5e43513c9",
            "35f5793010fe43c2bda76c91bbae1455"
          ]
        },
        "id": "JwkK-sMpclba",
        "outputId": "7b2e5d5a-d81a-41de-eb38-4a4620559eec"
      },
      "source": [
        "#First load models, dataset\n",
        "#make sure to restart runtime\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "\n",
        "# from datasets import registry\n",
        "\n",
        "# dataset_hparams = hparams.DatasetHparams(\n",
        "#     'cifar10',\n",
        "#     128, #batch size,\n",
        "#     do_not_augment = True\n",
        "# )\n",
        "\n",
        "# #set the platform\n",
        "# from platforms import base\n",
        "# import platforms.platform\n",
        "# import platforms.local\n",
        "\n",
        "# platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "# train_set = registry.get(\n",
        "#     dataset_hparams,\n",
        "#     train = True,\n",
        "# )\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "#And load the CIFAR10 dataset\n",
        "train_dataset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "train_set = DataLoader(train_dataset, batch_size=128, num_workers = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 6.79 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60bfbf765a804650b4e4e089ea7fe56d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /cifar-10-python.tar.gz to /\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsOopmhsPtg2"
      },
      "source": [
        "class measureForget:\n",
        "    def __init__(self, nb_epochs, num_batches, batch_size ):\n",
        "        self.forgetStatistics = torch.zeros(nb_epochs, num_batches, batch_size)\n",
        "        self.correctStatistics = torch.zeros(nb_epochs, num_batches, batch_size)\n",
        "        self.a_i = torch.zeros(nb_epochs, num_batches, batch_size) #measures if correctly classified or not\n",
        "        self.softmaxfunc = nn.Softmax(dim=1)\n",
        "        self.batch_tracker = 0\n",
        "        self.train_iteration = 0\n",
        "\n",
        "    #def trackExamples(self, batch_model_output, labels, model, batchx): #track examples check if examples in a batch were correctly classified before and aren't classified correctly now (where before and now refer to subsequent training iterations)\n",
        "    def trackExamples(self, batch_model_output, labels):\n",
        "\n",
        "        #model.eval()\n",
        "        #with torch.no_grad():\n",
        "        #    batch_model_output = model(input)\n",
        "\n",
        "        #print(model_output == batch_model_output)\n",
        "\n",
        "        #we can only track examples between two iterations of training, so if we're on the first iter, we just record correctStatistics and not forgetStatistics\n",
        "        #print(f\"Iteration {self.train_iteration}\")\n",
        "\n",
        "        counter = 0\n",
        "        for logit in batch_model_output: #_softmax:\n",
        "            if torch.argmax(logit) == labels[counter]:\n",
        "                self.correctStatistics[self.train_iteration, self.batch_tracker, counter] = 1\n",
        "                #if counter % 10 == 0:\n",
        "                   # print(f\"{torch.argmax(model_output[counter])==labels[counter]} and {torch.argmax(logit) == labels[counter]}\")\n",
        "                    #print(f\"{logit} and {labels[counter]}\")\n",
        "                    #print(f\"Model output: {model_output[counter]}\")\n",
        "\n",
        "            counter+=1\n",
        "\n",
        "        if self.train_iteration < 1:\n",
        "            return 0\n",
        "\n",
        "        #batch_model_output_softmax = self.softmaxfunc(batch_model_output)\n",
        "        counter = 0\n",
        "        for logit in batch_model_output: #_softmax:\n",
        "            if torch.argmax(logit) == labels[counter]:\n",
        "                #self.correctStatistics[self.train_iteration, self.batch_tracker, counter] = 1\n",
        "                self.a_i[self.train_iteration, self.batch_tracker, counter] = 1\n",
        "            else:\n",
        "                self.a_i[self.train_iteration, self.batch_tracker, counter] = 0\n",
        "            \n",
        "            if self.a_i[self.train_iteration, self.batch_tracker, counter] < self.a_i[self.train_iteration-1, self.batch_tracker, counter]:\n",
        "                self.forgetStatistics[self.train_iteration, self.batch_tracker, counter]+=1\n",
        "            \n",
        "            counter+=1\n",
        "        \n",
        "        #model.train()\n",
        "\n",
        "    def incrementBatch(self):\n",
        "        self.batch_tracker+=1\n",
        "    \n",
        "    def resetBatchTracker(self):\n",
        "        self.batch_tracker=0\n",
        "    \n",
        "    def incrementTrainIter(self):\n",
        "        self.train_iteration+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT4bqqW8yJU2"
      },
      "source": [
        "class debug_model_output:\n",
        "    def __init__(self, btchs, bsize):\n",
        "        self.othertracker = torch.zeros(btchs, bsize)\n",
        "\n",
        "    def test_fcn(self, model, input, compare):\n",
        "        out = model(input)\n",
        "        print(compare==out)\n",
        "    \n",
        "    def test_fcn2(self, batchoutput, labels, model, input, btracker):\n",
        "        ct=0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(input)\n",
        "        #print(out==batchoutput)\n",
        "        for logit in batchoutput:\n",
        "            if torch.argmax(logit) == labels[ct]:\n",
        "                self.othertracker[btracker, ct] = 1\n",
        "                #print(torch.argmax(out[ct])==labels[ct])\n",
        "            ct+=1\n",
        "        model.train()\n",
        "    \n",
        "    def get_forget(self):\n",
        "        #print(\"f!!!\")\n",
        "        databla = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "        msk = list()\n",
        "        #idx=0\n",
        "        flat=torch.flatten(self.othertracker)\n",
        "\n",
        "        for j in range(len(flat)):\n",
        "            if flat[j]==1:\n",
        "                msk.append(j)\n",
        "        trainsubset = torch.utils.data.Subset(databla, msk)\n",
        "        #print(\"hello!!!\")\n",
        "        print(msk)\n",
        "\n",
        "        return torch.utils.data.DataLoader(trainsubset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x1ii3v0ekM1"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib.pyplot import figure\n",
        "plt.rcParams['figure.figsize'] = [10, 7]\n",
        "\n",
        "class processMeasurements:\n",
        "    def __init__(self, forget_msrmt):\n",
        "        self.forget_stats = forget_msrmt.forgetStatistics\n",
        "        self.sum_over_ep_Forget = torch.sum(self.forget_stats, 0)\n",
        "    \n",
        "    def plotForgetHist(self):\n",
        "        length = len(torch.flatten(self.sum_over_ep_Forget))\n",
        "        hist = plt.hist(torch.flatten(self.sum_over_ep_Forget), alpha=0.5, weights = np.ones(length)/length)\n",
        "        plt.ylabel('Fraction of total events')\n",
        "        plt.xlabel('Number of forgetting events')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTuLT9gAWe0A"
      },
      "source": [
        "class manageForgetDataset:\n",
        "    def __init__(self, forget_msrmt, forget_thres = 4):\n",
        "        self.forget_stats = forget_msrmt.forgetStatistics\n",
        "        self.correct_stats = forget_msrmt.correctStatistics\n",
        "        self.sum_over_ep_flatten_forget = torch.flatten(torch.sum(self.forget_stats, 0))\n",
        "        self.forget_thres = forget_thres\n",
        "        self.trainset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "        self.forget_mask = list()\n",
        "        self.forget_mask_correct = list()\n",
        "        self.batch_size = self.forget_stats[0,0].size()[0] #infer batch size from input\n",
        "\n",
        "    #get a mask of most forgotten examples\n",
        "    def getForgetMask(self):\n",
        "        for k in range(len(self.sum_over_ep_flatten_forget)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres:\n",
        "                self.forget_mask.append(k)\n",
        "        return self.forget_mask\n",
        "\n",
        "    #get a mask of most forgotten examples that were classified *correctly* at the end of training\n",
        "    def getForgetMaskCorrect(self, which_epoch = None):\n",
        "        if which_epoch == None:\n",
        "            at_epoch = len(self.correct_stats)\n",
        "        else:\n",
        "            at_epoch = which_epoch\n",
        "\n",
        "    \n",
        "        correct_flat = torch.flatten(self.correct_stats[at_epoch-1])\n",
        "\n",
        "        #note that to add this which_epoch functionality correctly\n",
        "        #I'll have to make sure sum_over_ep_flatten is only summed\n",
        "        #up to which_epoch... can add that in later **TODO!!**\n",
        "\n",
        "        for k in range(len(correct_flat)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres and correct_flat[k]==1:\n",
        "                self.forget_mask_correct.append(k)\n",
        "        return self.forget_mask_correct\n",
        "\n",
        "    def getForgetMaskCorrectDebug(self, model, which_epoch = None):\n",
        "        if which_epoch == None:\n",
        "            at_epoch = len(self.correct_stats)\n",
        "        else:\n",
        "            at_epoch = which_epoch\n",
        "        \n",
        "        print(f\"{at_epoch-1}\")\n",
        "\n",
        "        correct_flat = torch.flatten(self.correct_stats[at_epoch-1])\n",
        "        print(f\"{len(self.correct_stats[at_epoch-1])}, {self.batch_size}\")\n",
        "        actual_model_correct = torch.zeros(len(self.correct_stats[at_epoch-1]), self.batch_size)\n",
        "        model.eval()\n",
        "\n",
        "        counter = 0\n",
        "        for batch in train_set:\n",
        "            x,y = batch\n",
        "            x=x.cuda()\n",
        "            with torch.no_grad():\n",
        "                modeloutput = model(x.detach().clone())\n",
        "            for j in range(len(modeloutput)):\n",
        "                if torch.argmax(modeloutput[j]) == y[j].cuda():\n",
        "                    actual_model_correct[counter, j] = 1\n",
        "            \n",
        "            counter+=1\n",
        "\n",
        "        print(actual_model_correct[counter-2]==self.correct_stats[at_epoch-1,389])\n",
        "\n",
        "        for k in range(len(correct_flat)):\n",
        "            if self.sum_over_ep_flatten_forget[k] >= self.forget_thres and correct_flat[k]==1:\n",
        "                self.forget_mask_correct.append(k)\n",
        "        return self.forget_mask_correct\n",
        "\n",
        "    def get_forgotten_dataset_correct(self): #return a mask of those examples that were forgotten AND classified correctly\n",
        "        #requires having run getForgetMask() first\n",
        "        train_subset_correct = torch.utils.data.Subset(self.trainset, self.getForgetMaskCorrect())\n",
        "        return torch.utils.data.DataLoader(train_subset_correct, batch_size=self.batch_size, )\n",
        "\n",
        "    def get_forgotten_dataset_correct_debg(self): #return a mask of those examples that were forgotten AND classified correctly\n",
        "        #requires having run getForgetMask() first\n",
        "        #train_subset_correct = torch.utils.data.Subset(self.trainset, self.getForgetMaskCorrect())\n",
        "        return torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size)\n",
        "\n",
        "    def get_num_forgotten(self):\n",
        "        return len(self.forget_mask)\n",
        "\n",
        "    def get_num_forgotten_correct(self, which_epoch = None):\n",
        "        if which_epoch == None:\n",
        "            return len(self.forget_mask_correct)\n",
        "        else:\n",
        "            return len(self.getForgetMaskCorrect(which_epoch))\n",
        "\n",
        "    def get_forgotten_dataset(self):\n",
        "        train_subset = torch.utils.data.Subset(self.trainset, self.getForgetMask())\n",
        "        return torch.utils.data.DataLoader(train_subset, batch_size=self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UN8EkV_rTSe",
        "outputId": "75b73fc5-4824-4bde-efb9-a4b3eb733ac6"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "nb_epochs = 20\n",
        "\n",
        "model_A_msrments = measureForget(nb_epochs, num_batches = len(train_set), batch_size=128)\n",
        "\n",
        "model_A.train()\n",
        "\n",
        "acc_A_global = list()\n",
        "\n",
        "output_vectors = {}\n",
        "output_vectors_eval = {}\n",
        "# output_vectors_eval_2 = {}\n",
        "# output_vectors_eval_3 = {}\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "    btrker =0 \n",
        "    for batch in train_set:\n",
        "\n",
        "        model_A.train()\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "        l_A = model_A(x)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_vectors[epoch,btrker] = l_A.detach().clone()\n",
        "        \n",
        "\n",
        "        #measureForget()\n",
        "        #model_A.eval()\n",
        "        #with torch.no_grad():\n",
        "        #    l_A = model_A(x)\n",
        "        #model_A.train()\n",
        "\n",
        "        #sdict = model_A.state_dict()\n",
        "        #model2.load_state_dict(sdict)\n",
        "\n",
        "       \n",
        "\n",
        "        #model_A.eval()\n",
        "        #with torch.no_grad():\n",
        "        #    l_Ap = model_A(x.detach().clone())\n",
        "        #print(l_A==l_Ap)\n",
        "        #model_A.train()\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        #l_Ap = model2(x.detach().clone())\n",
        "        #print(l_Ap == l_A)\n",
        "\n",
        "        #model_A_msrments.trackExamples(l_A.detach().clone(), y.detach().clone(), model2, x.detach().clone())\n",
        "\n",
        "\n",
        "        J_A = loss_A(l_A, y.cuda())\n",
        "\n",
        "        model_A.zero_grad()\n",
        "\n",
        "        J_A.backward()\n",
        "\n",
        "        optimizer_A.step()\n",
        "\n",
        "        model_A.eval()\n",
        "        with torch.no_grad():\n",
        "            l_Ap = model_A(x.detach().clone())\n",
        "            #l_Ap_2 = model_A(x.detach().clone())\n",
        "            output_vectors_eval[epoch,btrker] = l_Ap.detach().clone()\n",
        "            #output_vectors_eval_2[epoch,btrker] = l_Ap_2.detach().clone()\n",
        "            model_A_msrments.trackExamples(l_Ap.detach().clone(), y.detach().clone())\n",
        "\n",
        "            #l_Ap_3 = model_A(x.detach().clone())\n",
        "            #output_vectors_eval_3[epoch,btrker] = l_Ap_3.detach().clone()\n",
        "\n",
        "            \n",
        "        #dbg.test_fcn2(l_Ap.detach().clone(), y.detach().clone(), model_A, x.detach().clone(), btrker)\n",
        "        \n",
        "\n",
        "        losses_A.append(J_A.item())\n",
        "        accuracies_A.append(y.eq(l_Ap.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "        model_A_msrments.incrementBatch()\n",
        "        btrker+=1\n",
        "    \n",
        "    model_A_msrments.incrementTrainIter()\n",
        "    model_A_msrments.resetBatchTracker()\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A: {torch.tensor(losses_A).mean():.2f} accuracy A: {torch.tensor(accuracies_A).mean():.2f} \\n\")\n",
        "\n",
        "    acc_A_global.append(torch.tensor(accuracies_A).mean())\n",
        "\n",
        "model_A.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A: 2.03 accuracy A: 0.26 \n",
            "\n",
            "Epoch 2, train loss A: 1.76 accuracy A: 0.35 \n",
            "\n",
            "Epoch 3, train loss A: 1.66 accuracy A: 0.38 \n",
            "\n",
            "Epoch 4, train loss A: 1.58 accuracy A: 0.41 \n",
            "\n",
            "Epoch 5, train loss A: 1.51 accuracy A: 0.44 \n",
            "\n",
            "Epoch 6, train loss A: 1.44 accuracy A: 0.47 \n",
            "\n",
            "Epoch 7, train loss A: 1.37 accuracy A: 0.50 \n",
            "\n",
            "Epoch 8, train loss A: 1.31 accuracy A: 0.52 \n",
            "\n",
            "Epoch 9, train loss A: 1.26 accuracy A: 0.54 \n",
            "\n",
            "Epoch 10, train loss A: 1.21 accuracy A: 0.56 \n",
            "\n",
            "Epoch 11, train loss A: 1.16 accuracy A: 0.58 \n",
            "\n",
            "Epoch 12, train loss A: 1.13 accuracy A: 0.59 \n",
            "\n",
            "Epoch 13, train loss A: 1.09 accuracy A: 0.60 \n",
            "\n",
            "Epoch 14, train loss A: 1.06 accuracy A: 0.62 \n",
            "\n",
            "Epoch 15, train loss A: 1.04 accuracy A: 0.63 \n",
            "\n",
            "Epoch 16, train loss A: 1.01 accuracy A: 0.64 \n",
            "\n",
            "Epoch 17, train loss A: 0.98 accuracy A: 0.65 \n",
            "\n",
            "Epoch 18, train loss A: 0.96 accuracy A: 0.65 \n",
            "\n",
            "Epoch 19, train loss A: 0.94 accuracy A: 0.66 \n",
            "\n",
            "Epoch 20, train loss A: 0.92 accuracy A: 0.67 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LOtI_fB5xmV",
        "outputId": "29ae3305-8720-4d9b-d706-d8e14cc7bf7c"
      },
      "source": [
        "print(output_vectors_eval[19,20][29])\n",
        "print(output_vectors_eval_2[19,20][29])\n",
        "print(output_vectors_eval_3[19,20][29])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.1196, -1.9133,  2.7493,  2.8982,  1.0910,  0.8220,  0.8172,  0.0238,\n",
            "        -0.2683, -2.8538], device='cuda:0')\n",
            "tensor([-1.1196, -1.9133,  2.7493,  2.8982,  1.0910,  0.8220,  0.8172,  0.0238,\n",
            "        -0.2683, -2.8538], device='cuda:0')\n",
            "tensor([-1.1196, -1.9133,  2.7493,  2.8982,  1.0910,  0.8220,  0.8172,  0.0238,\n",
            "        -0.2683, -2.8538], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG5Vrxr0Hki2",
        "outputId": "162b3de7-9468-45d0-fc6b-c16e7fbf6cef"
      },
      "source": [
        "model_A.eval()\n",
        "temp=model_A(x)\n",
        "print(temp[0])\n",
        "print(output_vectors_eval_3[19,390][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.7102, -3.7843,  2.0747,  2.9617,  0.2061,  3.6672, -2.1881,  2.2155,\n",
            "        -1.1992, -1.2647], device='cuda:0', grad_fn=<SelectBackward>)\n",
            "tensor([-1.7102, -3.7843,  2.0747,  2.9617,  0.2061,  3.6672, -2.1881,  2.2155,\n",
            "        -1.1992, -1.2647], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlIw3w-nYfJT",
        "outputId": "daf4ecda-5168-4bd2-8886-b9db5f5d3e10"
      },
      "source": [
        "model_A.eval()\n",
        "z,y=next(iter(train_set))\n",
        "z=z.cuda()\n",
        "with torch.no_grad():\n",
        "    temp=model_A(z.detach().clone())\n",
        "print(temp[2])\n",
        "print(output_vectors_eval_3[19,0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.9210,  1.2676, -0.8179, -0.5290, -1.3860, -2.1118, -1.8039,  0.1976,\n",
            "         0.4948,  5.6216], device='cuda:0')\n",
            "tensor([-2.7335, -3.9993,  2.2827,  4.0484,  2.3747,  3.5750,  3.5971,  0.7792,\n",
            "        -3.7413, -3.6752], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7piN2h5EZmc8"
      },
      "source": [
        "for g in range(len(temp)):\n",
        "    for k in range(len(output_vectors_eval_3[19,0])):\n",
        "        for b in range(390):\n",
        "            if torch.equal(temp[g], output_vectors_eval_3[19,b][k]):\n",
        "                print(f\"{g}, {k}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "K35QGM48N_Bp",
        "outputId": "bb50f468-8016-43db-a1db-71f564724ee3"
      },
      "source": [
        "#new_train_set = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "#from datasets import cifar10\n",
        "#new_train_set_loader = cifar10.DataLoader(new_train_set, batch_size=128, num_workers=0)\n",
        "\n",
        "#torch.use_deterministic_algorithms(True)\n",
        "#CUBLAS_WORKSPACE_CONFIG=:16:8\n",
        "\n",
        "newoutput={}\n",
        "btrker=0\n",
        "model_A.eval()\n",
        "for batch in train_set:\n",
        "    x,y=batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out_p = model_A(x.detach().clone())\n",
        "    newoutput[btrker] = out_p.clone().detach()\n",
        "    btrker+=1\n",
        "print(newoutput[20][29])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7d900fd1d2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mnewoutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbtrker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mbtrker\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open_lth/models/cifar_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpaqizyXTlDX",
        "outputId": "09f5417e-9a76-4c7e-a901-545ae4720f7c"
      },
      "source": [
        "print(newoutput[20][29])\n",
        "print(output_vectors_eval_3[19,20][29])\n",
        "print(newoutput[289][10])\n",
        "print(output_vectors_eval_3[19,289][10])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.9626, -2.2677,  3.2166,  2.0658,  0.4167,  0.4271,  0.8047,  1.1838,\n",
            "        -3.1591, -2.9609])\n",
            "tensor([-1.1592, -1.9115,  3.2783,  2.3988, -0.0995,  0.7546,  0.3694,  0.8890,\n",
            "        -3.1869, -2.1761], device='cuda:0')\n",
            "tensor([ 1.8628,  2.1368, -0.9158,  0.0632, -0.4610, -1.4266, -3.1303, -0.5730,\n",
            "         4.7156,  1.6983])\n",
            "tensor([ 1.5665,  4.0622, -0.4723, -0.0706, -1.5701, -1.4448, -3.6055, -1.1219,\n",
            "         4.3932,  2.6519], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "VPGzy8IRJ2pQ",
        "outputId": "0ed23258-cd11-4c32-a8b4-3c062a1f8eab"
      },
      "source": [
        "new_train_set = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "from datasets import cifar10\n",
        "new_train_set_loader = cifar10.DataLoader(new_train_set, batch_size=128, num_workers=0)\n",
        "newoutput_2={}\n",
        "btrker=0\n",
        "model_A.eval()\n",
        "for batch in new_train_set_loader:\n",
        "    x,y=batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out_p=model_A(x.detach().clone())\n",
        "    newoutput_2[btrker] = out_p.clone().detach()\n",
        "    btrker+=1\n",
        "print(newoutput_2[20][29])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2b346a7835df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnewoutput_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbtrker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbtrker\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open_lth/models/cifar_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmf37vp2K7cV",
        "outputId": "b6b1aa05-358a-402b-8d76-a3c72bbec6de"
      },
      "source": [
        "newoutput[20][29]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.2112, -1.7018,  3.1729,  1.7978,  1.2208, -0.4915,  1.2666, -0.7934,\n",
              "        -1.4237, -3.1683], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3cnM-YGLTQp",
        "outputId": "6dc4aca5-037b-4365-b38b-c10844a122fe"
      },
      "source": [
        "from platforms.platform import get_platform\n",
        "get_platform().num_workers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdhc78y2uYix",
        "outputId": "7f3186d7-1241-4ce4-970d-b583ff62395f"
      },
      "source": [
        "model_A.eval()\n",
        "actual_correct_stats = torch.zeros(80)\n",
        "for batch in train_set:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.detach().clone())\n",
        "        \n",
        "num = 0\n",
        "for thing in out:\n",
        "    if(torch.argmax(thing)==y[num].cuda()):\n",
        "        actual_correct_stats[num] = 1\n",
        "    num+=1\n",
        "\n",
        "#print(out==l_Ap)\n",
        "print(model_A_msrments.correctStatistics[19,390,0:80])\n",
        "print(actual_correct_stats)\n",
        "print(model_A_msrments.correctStatistics[19,390,0:80]-actual_correct_stats)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_out = model_A(x.detach().clone())\n",
        "for k in range(len(actual_correct_stats)):\n",
        "    if actual_correct_stats[k]==1:\n",
        "        print(torch.argmax(last_out[k])==y[k].cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
            "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 1., 1., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWd8_T7NJwXb",
        "outputId": "cf350a29-a546-4b60-f7e3-5159dff22577"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "forget_data = mgdataset.get_forgotten_dataset()\n",
        "forget_data_crt = mgdataset.get_forgotten_dataset_correct()\n",
        "\n",
        "model_A.eval()\n",
        "for batch in forget_data_crt:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.detach().clone())\n",
        "        \n",
        "with torch.no_grad():\n",
        "    last_out = model_A(x.detach().clone())\n",
        "for k in range(len(last_out)):\n",
        "    print(torch.argmax(last_out[k])==y[k].cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(False, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n",
            "tensor(True, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "aYvgSDg0MYd6",
        "outputId": "783a7fa3-da86-463f-9ce1-df21cca5467e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "gridlist2 = [x[0]*.23 + 0.45,x[1]*0.23 + 0.45,(x[2]*.23)+0.45,(x[3]*.23)+0.45]\n",
        "gridimgs2 = utils.make_grid(gridlist2)\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "show(gridimgs2.cpu())\n",
        "print(y[0:4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([8, 9, 1, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "PjMju0xPNNex",
        "outputId": "91dc13dd-ac6f-4952-ebf3-61a3e145ada2"
      },
      "source": [
        "#let's try to find these examples in the train_set and see if they are misclassified there.\n",
        "catpic = x[0]\n",
        "count=0\n",
        "model_A.eval()\n",
        "for batch in train_set:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        output=model_A(x.detach().clone())\n",
        "    for j in range(len(x)):\n",
        "        xflat = torch.flatten(x[j].detach().clone())\n",
        "        if torch.equal(x[j],catpic):\n",
        "            show(x.cpu()[j]*.23 + 0.45)\n",
        "            print(f\"{count}, {j}, {y[j]}, output: {output[j]}\")\n",
        "            print(f\"{torch.argmax(output[j])==y[j]}\")\n",
        "            \n",
        "    count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27, 85, 8, output: tensor([-0.8794, -1.3386, -0.1487,  0.3173, -0.7304,  0.1706,  0.6876, -1.1127,\n",
            "         0.5433,  0.2617], device='cuda:0')\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DVHAraG5xqN",
        "outputId": "52227d13-a914-4c73-d309-21ca280988fe"
      },
      "source": [
        "print(output_vectors[19, 27][85])\n",
        "print(output_vectors_eval[19, 27][85])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.1006, -0.8767, -0.4181,  0.0194, -1.0642, -0.6152, -0.0719, -1.2814,\n",
            "         1.7363,  1.0252], device='cuda:0')\n",
            "tensor([-0.7893, -1.6244, -0.2600,  0.3854, -0.7042,  0.2815,  0.0265, -0.6295,\n",
            "         0.6979,  0.6053], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaYQgzKpBHFd",
        "outputId": "0a05c5c8-dcb5-4c68-919f-4f5e954b9429"
      },
      "source": [
        "for batch in train_set:\n",
        "    x,y=batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        out=model_A(x.detach().clone())\n",
        "    for k in range(len(x)):\n",
        "        if torch.equal(x[k],catpic):\n",
        "            print(out[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-2.8573, -4.0407,  1.6254,  3.0421,  1.9822,  1.9157,  3.3206, -0.8332,\n",
            "        -3.5770, -3.6559], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MP39qXHQxZE",
        "outputId": "21ca0d40-cb45-4f2a-d1ab-0a5d3b260baa"
      },
      "source": [
        "print(model_A_msrments.correctStatistics[19,50,43])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ImxP8bWZV5r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD0nsvUd5bKm",
        "outputId": "82cdbf71-eba0-4b12-af4b-523e06441f7e"
      },
      "source": [
        "print(len(model_A_msrments.correctStatistics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diHYMsUs-6Gq",
        "outputId": "bd4caaac-190c-4daf-8c78-87e957ace6da"
      },
      "source": [
        "print(model_A_msrments.correctStatistics[19,0:2])\n",
        "print(torch.flatten(model_A_msrments.correctStatistics[19,0:2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "         0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "         1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "         1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "         0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "         1., 1.],\n",
            "        [0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
            "         1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "         1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
            "         0., 1.]])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
            "        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Et6Vrnr0jz",
        "outputId": "2722a1ba-0a76-4432-a58c-33730fb5fa61"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "maskdebug = mgdataset.getForgetMaskCorrectDebug(model_A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "19\n",
            "391, 128\n",
            "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K6DpWkSCzp7"
      },
      "source": [
        "dbg_train = mgdataset.get_forgotten_dataset_correct_debg()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTzS3dLpDDt2",
        "outputId": "0ef5f7b7-e90d-4659-fb3a-e85375d8ec7d"
      },
      "source": [
        "model_A.eval()\n",
        "\n",
        "for batch in dbg_train:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        dbg_output = model_A(x)\n",
        "\n",
        "for batch in train_set:\n",
        "    x,y = batch\n",
        "    x=x.cuda()\n",
        "    with torch.no_grad():\n",
        "        train_output = model_A(x)\n",
        "\n",
        "print(dbg_output-train_output)\n",
        "#print(train_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN63U2e5WWTZ",
        "outputId": "699213e1-f0e2-4af0-bd7e-458d0abd39dd"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "forget_data = mgdataset.get_forgotten_dataset()\n",
        "forget_data_crt = mgdataset.get_forgotten_dataset_correct()\n",
        "#forget_data2 = dbg.get_forget() #not working..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaE9UXWAYeh7",
        "outputId": "13b006a1-722b-46cd-8ac7-16e507b09fef"
      },
      "source": [
        "dbg.get_forget()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fa03ca63210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWeQr20hXsiV",
        "outputId": "e80e6410-f7fd-4781-902d-2e11b0349d5e"
      },
      "source": [
        "count2=0 #count the number in forget_data2\n",
        "for j in forget_data2:\n",
        "    bc,lab = j\n",
        "    for thing in bc:\n",
        "        count2+=1\n",
        "print(count2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzDYW9zUYHei",
        "outputId": "e1b419c1-b43a-4a6c-a00a-c3fa17aae7a9"
      },
      "source": [
        "mgdataset.get_num_forgotten_correct()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM647BmKeLhp"
      },
      "source": [
        "#mgdataset.getForgetMaskCorrect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa85K-gZeLPF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEnnQRanW3M7",
        "outputId": "933d9742-6069-47ba-c708-903f113bc84d"
      },
      "source": [
        "#model2.eval()\n",
        "model_A.eval()\n",
        "testclass = torch.zeros(mgdataset.get_num_forgotten_correct())\n",
        "#testclass2 = torch.zeros(mgdataset.get_num_forgotten_correct())\n",
        "\n",
        "\n",
        "count = 0\n",
        "for b in forget_data_crt:\n",
        "    x,y = b\n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.cuda())\n",
        "        #out2 = model2(x.cuda())\n",
        "    for i in range(len(out)):\n",
        "        if torch.argmax(out[i]) == y.cuda()[i]:\n",
        "            testclass[i]=1\n",
        "        #if torch.argmax(out2[i]) == y.cuda()[i]:\n",
        "        #    testclass2[i]=1\n",
        "        #else:\n",
        "        #    print(f\"{torch.argmax(out[i])},{y[i]}\")\n",
        "        count+=1\n",
        "\n",
        "print(count)\n",
        "print(testclass)\n",
        "#print(testclass2)\n",
        "#print(testclass-testclass2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rB1j02KfhxS"
      },
      "source": [
        "Now we add noise, but first let's record the ordering of correctly classified examples from the train set. Our noise will be drawn from a normal distribution with mean 0 and std dv epsilon. Before adding noise we should clone the model 100 times corresponding to different epsilons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNivej31kz38"
      },
      "source": [
        "class damageModel:\n",
        "    def __init__(self, model):\n",
        "        from foundations import hparams\n",
        "        from models import registry\n",
        "        \n",
        "        model.eval()\n",
        "        #self.forget_msrmt = forget_msrmt\n",
        "        self.model_clones = []\n",
        "        self.model_state_dict = model.state_dict()\n",
        "        self.model_hparams = hparams.ModelHparams('cifar_resnet_20', 'kaiming_uniform', 'uniform')\n",
        "\n",
        "    def addNoise(self, num_points = 30, min_noise = 0., max_noise = 1.): #returns an array of length num_points, consisting of models increasingly damaged from Gaussian noise with stdev min_noise to max_noise\n",
        "        from foundations import hparams\n",
        "        from models import registry\n",
        "\n",
        "        epsilons = np.linspace(min_noise, max_noise, num_points)\n",
        "        for i in range(len(epsilons)):\n",
        "            sys.stdout.write(\"\\r{0}Cloning models...\".format(\"|\"*i))\n",
        "            sys.stdout.flush()\n",
        "            self.model_clones.append(registry.get(self.model_hparams).cuda())\n",
        "            self.model_clones[i].load_state_dict(self.model_state_dict)\n",
        "            \n",
        "\n",
        "        with torch.no_grad():\n",
        "            k = 0\n",
        "            for model in self.model_clones:\n",
        "                for param in model.parameters():\n",
        "                    param.multiply_(1+torch.empty(param.size()).cuda().normal_(mean=0,std=epsilons[k]))\n",
        "                k+=1\n",
        "            \n",
        "        for models in self.model_clones:\n",
        "            models.eval()\n",
        "        \n",
        "        return self.model_clones\n",
        "    \n",
        "    def getEpsilons(self, num_points = 30, min_noise = 0., max_noise = 1.):\n",
        "        return np.linspace(min_noise, max_noise, num_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM6S2AxuX3UU"
      },
      "source": [
        "class postProcess:\n",
        "    def __init__(self, num_examples = None):\n",
        "        #self.catalog = torch.zeros(num_examples)\n",
        "        self.num_examples = num_examples\n",
        "\n",
        "    #measure at which noise level an example that's classified correctly becomes misclassifed\n",
        "    #this function just classifies a dataset given a model\n",
        "    def classifyDataset(self, data_loader, models):\n",
        "        if self.num_examples==None:\n",
        "            raise ValueError(\"Specify the size of the dataset please.\")\n",
        "        num_models = len(models)\n",
        "        __catalog = torch.zeros(num_models, self.num_examples)\n",
        "        \n",
        "        modeltrcker = 0\n",
        "        for model in models:\n",
        "            model.eval()\n",
        "            for batch in data_loader:\n",
        "                x,y = batch\n",
        "                x=x.cuda()\n",
        "                with torch.no_grad():\n",
        "                    l_A = model(x)\n",
        "                for k in range(len(l_A)):\n",
        "                    if torch.argmax(l_A[k]) == y.cuda()[k]:\n",
        "                         __catalog[modeltrcker, k] = 1\n",
        "            modeltrcker+=1\n",
        "\n",
        "        return __catalog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGpeWsDTHuYZ"
      },
      "source": [
        "With our streamlined functions we can plot, e.g., the total number of forgetting events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPd6GvvX3pCO",
        "outputId": "d8589bb6-8da2-4f41-ebaf-dc65513a7018"
      },
      "source": [
        "model_A.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Block(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "ys1frakUH0db",
        "outputId": "cdb5089b-64d7-4461-f46b-34a8e49c1940"
      },
      "source": [
        "process_msrments = processMeasurements(model_A_msrments)\n",
        "process_msrments.plotForgetHist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-SB8WaoHkcc"
      },
      "source": [
        "Obtain dataset of forgotten examples as well as the subset of those which were classified correclty at the end of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct0GmSXtG2hF",
        "outputId": "3cbf1010-7e23-4666-8825-cd03cca1f2b3"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "forget_data = mgdataset.get_forgotten_dataset()\n",
        "forget_data_crt = mgdataset.get_forgotten_dataset_correct()\n",
        "forget_data2 = dbg.get_forget()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfbRX_T4I1BJ"
      },
      "source": [
        "We can get the size of these datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULwyAHvCI3iD",
        "outputId": "ce619aa8-e1b2-4a3c-dc73-4c4dfce57dee"
      },
      "source": [
        "print(mgdataset.get_num_forgotten())\n",
        "print(mgdataset.get_num_forgotten_correct())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108\n",
            "41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uBriNVAJFbp"
      },
      "source": [
        "Next let's try to add noise to the models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pQ8mxaaJK0z",
        "outputId": "938f9a27-7b10-4710-b3f5-9a6578dbb417"
      },
      "source": [
        "model_damage = damageModel(model_A)\n",
        "noisy_clones = model_damage.addNoise()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|||||||||||||||||||||||||||||Cloning models..."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge9hE8WyU1HA",
        "outputId": "ba6392f1-a6d1-40d0-b8a2-38da5dbe7af9"
      },
      "source": [
        "print(model_damage.getEpsilons())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.03448276 0.06896552 0.10344828 0.13793103 0.17241379\n",
            " 0.20689655 0.24137931 0.27586207 0.31034483 0.34482759 0.37931034\n",
            " 0.4137931  0.44827586 0.48275862 0.51724138 0.55172414 0.5862069\n",
            " 0.62068966 0.65517241 0.68965517 0.72413793 0.75862069 0.79310345\n",
            " 0.82758621 0.86206897 0.89655172 0.93103448 0.96551724 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0gbfP_NKWFW"
      },
      "source": [
        "We can see how well one of the noisy models does in classifying the correct highly forgotten examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dsiGVfWKT5b"
      },
      "source": [
        "post_process = postProcess(mgdataset.get_num_forgotten_correct())\n",
        "post_process2 = postProcess(128)\n",
        "classifications = post_process.classifyDataset(forget_data_crt, noisy_clones)\n",
        "#classifications2 = post_process2.classifyDataset(forget_data2, noisy_clones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toxolnQqPkQy",
        "outputId": "7c7cfedc-1cb4-4e19-85ab-2fdd0538dd62"
      },
      "source": [
        "classifications[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
              "        0., 1., 0., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihiqMfvWOsR6",
        "outputId": "0661e626-6d5d-46b8-b1b3-dc208e2c1369"
      },
      "source": [
        "bct=0\n",
        "for b in forget_data2:\n",
        "    t,tb = b\n",
        "    for ex in t:\n",
        "        bct+=1\n",
        "print(bct)\n",
        "\n",
        "bct=0\n",
        "for b in forget_data_crt:\n",
        "    t,tb=b\n",
        "    for ex in t:\n",
        "        bct+=1\n",
        "print(bct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMUuwWHCRSu8",
        "outputId": "4d17c585-e458-4a8d-fb69-289bf4f7817a"
      },
      "source": [
        "len(next(iter(forget_data2))[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OEqmTVds8Iv"
      },
      "source": [
        "x,y = next(iter(forget_data_crt))\n",
        "res = model_A(x.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSsGtHoLtUo9",
        "outputId": "b6197225-e825-4159-8575-8853eb2ede6f"
      },
      "source": [
        "res[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.3210, -3.8347,  2.2128,  2.9843,  2.9100,  3.8717, -0.4597,  2.5502,\n",
              "        -4.0845, -2.8960], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8HWkUYAtX_I",
        "outputId": "f42b6bb7-dc52-4b24-ffb2-e2f569837f53"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNToqe4vi9Jf",
        "outputId": "2269c032-0937-4a4f-fdde-cd8e705ae48d"
      },
      "source": [
        "model_A.eval()\n",
        "testclass = torch.zeros(mgdataset.get_num_forgotten_correct())\n",
        "#testclass = torch.zeros(128)\n",
        "count = 0\n",
        "for b in forget_data_crt:\n",
        "    x,y = b\n",
        "    with torch.no_grad():\n",
        "        out = model_A(x.cuda())\n",
        "    for i in range(len(out)):\n",
        "        if torch.argmax(out[i]) == y.cuda()[i]:\n",
        "            testclass[i]=1\n",
        "        else:\n",
        "            print(f\"{torch.argmax(out[i])},{y[i]}\")\n",
        "        count+=1\n",
        "\n",
        "print(count)\n",
        "print(testclass)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4,3\n",
            "0,8\n",
            "3,5\n",
            "6\n",
            "tensor([1., 0., 1., 1., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOsXJlnRznN1",
        "outputId": "bd6ec500-2148-41c0-fc0f-158423beddf2"
      },
      "source": [
        "model_A_msrments.correctStatistics[29,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bRO7FNnz3Uk",
        "outputId": "2d3bb454-430f-4349-dcde-57ae55f720ae"
      },
      "source": [
        "torch.flatten(model_A_msrments.correctStatistics[29])[mgdataset.getForgetMaskCorrect()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mixbuPSXRicq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf33869e-2a7c-4d74-fb2e-d16932e84a0e"
      },
      "source": [
        "torch.flatten(model_A_msrments.correctStatistics[29])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.,  ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_zgb9LWmldQ",
        "outputId": "3c6facb2-257f-4825-ddda-2a1fd88183b5"
      },
      "source": [
        "dataset = datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW8fEfmbmsmS"
      },
      "source": [
        "mask = list()\n",
        "mask.append(0)\n",
        "mask.append(3)\n",
        "trainsubset = torch.utils.data.Subset(dataset, mask)\n",
        "loader= torch.utils.data.DataLoader(trainsubset, batch_size=128)\n",
        "loaderold = torch.utils.data.DataLoader(dataset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LdGgH-JoVle",
        "outputId": "986b048f-502d-43c8-cf74-f2bb07145b7f"
      },
      "source": [
        "for batch in loader:\n",
        "    for examples in batch:\n",
        "        print(examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-1.1075, -1.3815, -1.2617,  ...,  0.5878,  0.4851,  0.4166],\n",
            "          [-1.8439, -2.1179, -1.8097,  ..., -0.0116, -0.0801, -0.0287],\n",
            "          [-1.6898, -1.8439, -1.2788,  ..., -0.0972, -0.0629, -0.2513],\n",
            "          ...,\n",
            "          [ 1.4440,  1.3242,  1.2728,  ...,  0.6221, -1.1589, -1.2103],\n",
            "          [ 0.9646,  0.8447,  1.0673,  ...,  1.0331, -0.4568, -0.6965],\n",
            "          [ 0.9132,  0.7591,  0.9474,  ...,  1.5810,  0.4679, -0.0116]],\n",
            "\n",
            "         [[-0.9503, -1.2304, -1.1954,  ...,  0.2752,  0.1527,  0.1352],\n",
            "          [-1.6856, -2.0357, -1.8957,  ..., -0.4951, -0.5826, -0.5126],\n",
            "          [-1.6155, -1.9132, -1.5630,  ..., -0.5651, -0.5651, -0.7577],\n",
            "          ...,\n",
            "          [ 0.9405,  0.6429,  0.7829,  ...,  0.2927, -1.4930, -1.4405],\n",
            "          [ 0.3978,  0.1176,  0.4853,  ...,  0.5553, -0.9503, -1.1078],\n",
            "          [ 0.4853,  0.2227,  0.4503,  ...,  1.1856,  0.0301, -0.4251]],\n",
            "\n",
            "         [[-0.7064, -1.0201, -1.0550,  ...,  0.0779, -0.0267, -0.0092],\n",
            "          [-1.4559, -1.8044, -1.8044,  ..., -0.8458, -0.9330, -0.8110],\n",
            "          [-1.4384, -1.8044, -1.6650,  ..., -0.9330, -0.9330, -1.0724],\n",
            "          ...,\n",
            "          [-0.1312, -1.2119, -1.3513,  ..., -0.5844, -1.6824, -1.4559],\n",
            "          [-0.1312, -1.0724, -1.2816,  ..., -0.1661, -1.2119, -1.2119],\n",
            "          [ 0.2173, -0.1661, -0.2881,  ...,  0.6356, -0.3404, -0.5495]]],\n",
            "\n",
            "\n",
            "        [[[-1.6384, -1.4843, -1.4672,  ..., -0.8164, -0.7308, -0.6623],\n",
            "          [-1.5528, -1.5357, -1.5699,  ..., -0.4911, -0.4739, -0.6623],\n",
            "          [-1.4500, -1.4329, -1.1418,  ..., -0.5253, -0.2856, -0.4911],\n",
            "          ...,\n",
            "          [-0.6965, -0.6281, -0.6794,  ..., -0.4226, -0.5767, -0.7308],\n",
            "          [-0.6109, -0.5767, -0.5253,  ..., -0.7479, -0.8164, -0.7137],\n",
            "          [-0.4568, -0.5082, -0.5253,  ..., -1.1932, -1.0390, -0.8849]],\n",
            "\n",
            "         [[-1.5980, -1.4405, -1.4230,  ..., -0.8627, -0.7752, -0.7052],\n",
            "          [-1.5455, -1.5105, -1.5630,  ..., -0.6001, -0.6001, -0.7752],\n",
            "          [-1.4755, -1.4580, -1.1604,  ..., -0.7052, -0.4776, -0.6877],\n",
            "          ...,\n",
            "          [-0.7577, -0.6877, -0.7402,  ..., -0.4076, -0.5651, -0.7227],\n",
            "          [-0.7752, -0.7402, -0.6877,  ..., -0.7402, -0.8102, -0.7052],\n",
            "          [-0.6702, -0.7227, -0.7227,  ..., -1.2129, -1.0553, -0.8978]],\n",
            "\n",
            "         [[-1.6302, -1.4733, -1.4559,  ..., -1.1247, -1.0550, -0.9853],\n",
            "          [-1.5779, -1.5604, -1.5953,  ..., -0.8458, -0.8284, -1.0201],\n",
            "          [-1.5430, -1.5081, -1.2293,  ..., -0.8981, -0.6541, -0.8633],\n",
            "          ...,\n",
            "          [-0.8981, -0.8284, -0.8981,  ..., -0.5844, -0.7413, -0.8981],\n",
            "          [-0.9156, -0.8981, -0.8284,  ..., -0.8807, -0.9504, -0.8458],\n",
            "          [-0.8284, -0.8807, -0.8807,  ..., -1.3164, -1.1596, -1.0027]]]])\n",
            "tensor([6, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ZAnJ32osrt"
      },
      "source": [
        "x,y = next(iter(loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agEiRNWKoyMc",
        "outputId": "7b7471da-3dc2-4260-ee9e-6d7db928ebba"
      },
      "source": [
        "print(x[1])\n",
        "print(y[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.6384, -1.4843, -1.4672,  ..., -0.8164, -0.7308, -0.6623],\n",
            "         [-1.5528, -1.5357, -1.5699,  ..., -0.4911, -0.4739, -0.6623],\n",
            "         [-1.4500, -1.4329, -1.1418,  ..., -0.5253, -0.2856, -0.4911],\n",
            "         ...,\n",
            "         [-0.6965, -0.6281, -0.6794,  ..., -0.4226, -0.5767, -0.7308],\n",
            "         [-0.6109, -0.5767, -0.5253,  ..., -0.7479, -0.8164, -0.7137],\n",
            "         [-0.4568, -0.5082, -0.5253,  ..., -1.1932, -1.0390, -0.8849]],\n",
            "\n",
            "        [[-1.5980, -1.4405, -1.4230,  ..., -0.8627, -0.7752, -0.7052],\n",
            "         [-1.5455, -1.5105, -1.5630,  ..., -0.6001, -0.6001, -0.7752],\n",
            "         [-1.4755, -1.4580, -1.1604,  ..., -0.7052, -0.4776, -0.6877],\n",
            "         ...,\n",
            "         [-0.7577, -0.6877, -0.7402,  ..., -0.4076, -0.5651, -0.7227],\n",
            "         [-0.7752, -0.7402, -0.6877,  ..., -0.7402, -0.8102, -0.7052],\n",
            "         [-0.6702, -0.7227, -0.7227,  ..., -1.2129, -1.0553, -0.8978]],\n",
            "\n",
            "        [[-1.6302, -1.4733, -1.4559,  ..., -1.1247, -1.0550, -0.9853],\n",
            "         [-1.5779, -1.5604, -1.5953,  ..., -0.8458, -0.8284, -1.0201],\n",
            "         [-1.5430, -1.5081, -1.2293,  ..., -0.8981, -0.6541, -0.8633],\n",
            "         ...,\n",
            "         [-0.8981, -0.8284, -0.8981,  ..., -0.5844, -0.7413, -0.8981],\n",
            "         [-0.9156, -0.8981, -0.8284,  ..., -0.8807, -0.9504, -0.8458],\n",
            "         [-0.8284, -0.8807, -0.8807,  ..., -1.3164, -1.1596, -1.0027]]])\n",
            "tensor(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jFml1hpPYk"
      },
      "source": [
        "xp, yp = next(iter(loaderold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83ukgYlmpTK9",
        "outputId": "42d397dc-58a4-4af2-a2db-c581b3defda6"
      },
      "source": [
        "print(xp[3])\n",
        "print(yp[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.6384, -1.4843, -1.4672,  ..., -0.8164, -0.7308, -0.6623],\n",
            "         [-1.5528, -1.5357, -1.5699,  ..., -0.4911, -0.4739, -0.6623],\n",
            "         [-1.4500, -1.4329, -1.1418,  ..., -0.5253, -0.2856, -0.4911],\n",
            "         ...,\n",
            "         [-0.6965, -0.6281, -0.6794,  ..., -0.4226, -0.5767, -0.7308],\n",
            "         [-0.6109, -0.5767, -0.5253,  ..., -0.7479, -0.8164, -0.7137],\n",
            "         [-0.4568, -0.5082, -0.5253,  ..., -1.1932, -1.0390, -0.8849]],\n",
            "\n",
            "        [[-1.5980, -1.4405, -1.4230,  ..., -0.8627, -0.7752, -0.7052],\n",
            "         [-1.5455, -1.5105, -1.5630,  ..., -0.6001, -0.6001, -0.7752],\n",
            "         [-1.4755, -1.4580, -1.1604,  ..., -0.7052, -0.4776, -0.6877],\n",
            "         ...,\n",
            "         [-0.7577, -0.6877, -0.7402,  ..., -0.4076, -0.5651, -0.7227],\n",
            "         [-0.7752, -0.7402, -0.6877,  ..., -0.7402, -0.8102, -0.7052],\n",
            "         [-0.6702, -0.7227, -0.7227,  ..., -1.2129, -1.0553, -0.8978]],\n",
            "\n",
            "        [[-1.6302, -1.4733, -1.4559,  ..., -1.1247, -1.0550, -0.9853],\n",
            "         [-1.5779, -1.5604, -1.5953,  ..., -0.8458, -0.8284, -1.0201],\n",
            "         [-1.5430, -1.5081, -1.2293,  ..., -0.8981, -0.6541, -0.8633],\n",
            "         ...,\n",
            "         [-0.8981, -0.8284, -0.8981,  ..., -0.5844, -0.7413, -0.8981],\n",
            "         [-0.9156, -0.8981, -0.8284,  ..., -0.8807, -0.9504, -0.8458],\n",
            "         [-0.8284, -0.8807, -0.8807,  ..., -1.3164, -1.1596, -1.0027]]])\n",
            "tensor(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLBgz5_qp0pc",
        "outputId": "585fad82-eacd-4136-c440-d58989a47a2c"
      },
      "source": [
        "model_A(x.cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.9894, -1.9731,  0.0483,  4.0130, -0.4503,  2.7578,  4.2070, -0.9853,\n",
              "         -3.1088, -2.5000],\n",
              "        [-2.6935, -2.2610,  1.0830,  0.1237,  6.2339, -0.0703,  2.2714,  3.1146,\n",
              "         -4.8501, -1.2848]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1uKXmkrp7wl",
        "outputId": "3e3e51ee-c786-40a6-bbb0-f44a6e65e0ea"
      },
      "source": [
        "model_A(xp.cuda())[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.6935, -2.2610,  1.0830,  0.1237,  6.2339, -0.0703,  2.2714,  3.1146,\n",
              "        -4.8501, -1.2848], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvaOa20iOK0Z"
      },
      "source": [
        "We can easily use this information to obtain a ranking of examples (that is, which ones were forgotten first as we dialed up the noise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z68sbJTWNZmU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGf3iq85HrPy"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W-319oqzO0i",
        "outputId": "03cf52d9-3d13-4429-da70-028bc5797f67"
      },
      "source": [
        "if torch.argmax(torch.tensor([1,4,2,3,5,1,1]))==4:\n",
        "    print(\"true!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlPojSXdr0fL",
        "outputId": "5eb63df6-6dd1-43e3-afa0-b4915f9c35ba"
      },
      "source": [
        "mgdataset = manageForgetDataset(model_A_msrments)\n",
        "fordata = mgdataset.get_forgotten_dataset()\n",
        "fordatacrt = mgdataset.get_forgotten_dataset_correct()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "At epoch: 30\n",
            "Len of correct_flat: 50048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYnv_BUnHD4Y"
      },
      "source": [
        "countfor = 0\n",
        "for batch in fordata:\n",
        "    x,y = batch\n",
        "    for j in range(len(x)):\n",
        "        countfor+=1\n",
        "\n",
        "countforcorrect = 0\n",
        "for batch in fordatacrt:\n",
        "    x,y = batch\n",
        "    for j in range(len(x)):\n",
        "        countforcorrect+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9u9l4i8Hho6",
        "outputId": "1491bb93-7750-4425-a59e-fdd9355ce9e4"
      },
      "source": [
        "mgdataset.forget_mask_correct"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZYYKdNAHz61",
        "outputId": "fea715c1-11cf-40c9-a8be-56cd24066060"
      },
      "source": [
        "len(mgdataset.forget_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CKCEqrAH6E6",
        "outputId": "0116a739-093c-4963-bfd8-926aec8769b6"
      },
      "source": [
        "countforcorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG6mblQDI3oL",
        "outputId": "4b07108b-f592-4f6b-dcac-c783d82ec90a"
      },
      "source": [
        "countfor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHApujovYeMt",
        "outputId": "b7f6a544-84c4-479a-95ed-f7ee80cb9134"
      },
      "source": [
        "len(fordata)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MQNhBaEv9Xu"
      },
      "source": [
        "#record the ordering before adding noise\n",
        "#we tabulate a matrix\n",
        "import numpy as np\n",
        "epsilons = np.linspace(0.05,1,25)\n",
        "model_A_state_dict = model_A.state_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrX1OwbMr7AH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izetXnTTbarL",
        "outputId": "76235123-fc94-4815-b746-f0362faa215c"
      },
      "source": [
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_clones = []\n",
        "\n",
        "for i in range(len(epsilons)):\n",
        "    print(f\"Adding model.. {i/25}\")\n",
        "    model_clones.append(registry.get(model_hparams).cuda())\n",
        "    model_clones[i].load_state_dict(model_A_state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding model.. 0.0\n",
            "Adding model.. 0.04\n",
            "Adding model.. 0.08\n",
            "Adding model.. 0.12\n",
            "Adding model.. 0.16\n",
            "Adding model.. 0.2\n",
            "Adding model.. 0.24\n",
            "Adding model.. 0.28\n",
            "Adding model.. 0.32\n",
            "Adding model.. 0.36\n",
            "Adding model.. 0.4\n",
            "Adding model.. 0.44\n",
            "Adding model.. 0.48\n",
            "Adding model.. 0.52\n",
            "Adding model.. 0.56\n",
            "Adding model.. 0.6\n",
            "Adding model.. 0.64\n",
            "Adding model.. 0.68\n",
            "Adding model.. 0.72\n",
            "Adding model.. 0.76\n",
            "Adding model.. 0.8\n",
            "Adding model.. 0.84\n",
            "Adding model.. 0.88\n",
            "Adding model.. 0.92\n",
            "Adding model.. 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU6rS80Igd_7"
      },
      "source": [
        "with torch.no_grad():\n",
        "    k = 0\n",
        "    for model in model_clones:\n",
        "        for param in model.parameters():\n",
        "            param.multiply_(1+torch.empty(param.size()).cuda().normal_(mean=0,std=epsilons[k]))\n",
        "        k+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RZkelqbrB7J",
        "outputId": "fee95497-eedb-4a45-aff8-442855b33484"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "for i in range(10):\n",
        "    sys.stdout.write(\"\\r{0}>\".format(\"=\"*i))\n",
        "    sys.stdout.flush()\n",
        "    time.sleep(0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========>"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6srqpyvbrUir"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEdvumWfrs7A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn6akr8CrUdL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxkyM4mEiVHg",
        "outputId": "667677b6-1014-479c-9c20-4646abb95585"
      },
      "source": [
        "transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHaUhqODyEXL",
        "outputId": "5dcbe052-0a7b-4588-d162-392b317398cb"
      },
      "source": [
        "test_dataset=datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "# subset_list = list(range(0,len(test_dataset)))\n",
        "# test_subset = torch.utils.data.Subset(test_dataset, subset_list)\n",
        "data_loader_2 =torch.utils.data.DataLoader(test_dataset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "035ba7db34bc4c31996774e572debcbb",
            "00553b164fe941cda24fa9f4d27f67e0",
            "6f34c72409ba4f4b93bc61fe8124eced",
            "13dab17f70ab43ff9223145bf9eb00cb",
            "eee3903f08b045d6a6b2dcb02428dcd0",
            "f704720ce9754447af5b206342012175",
            "7e85e88686d945a5956d4d67c5cd1359",
            "d546c4ce870c4f8abdec65eea824632a"
          ]
        },
        "id": "OvYxqmeza4S9",
        "outputId": "4bff9307-9226-4018-e572-fb73e9758852"
      },
      "source": [
        "#test_dataset=datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_dataset=datasets.CIFAR10('/', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
        "subset_list = list(range(0,len(test_dataset)))\n",
        "test_subset = torch.utils.data.Subset(test_dataset, subset_list)\n",
        "data_loader =torch.utils.data.DataLoader(test_subset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "035ba7db34bc4c31996774e572debcbb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting /cifar-10-python.tar.gz to /\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "o2vULbG8JK33",
        "outputId": "aaa6280b-f5f1-4b35-df16-44ba83c97423"
      },
      "source": [
        "#plot images\n",
        "import numpy as np\n",
        "\n",
        "images, labels = next(iter(data_loader_2))\n",
        "\n",
        "gridlist = [images[0]*.23 + 0.45,images[1]*0.23 + 0.45,(images[3]*.23)+0.45,(images[4]*.23)+0.45]\n",
        "gridimgs = utils.make_grid(gridlist)\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "show(gridimgs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "Szf4vFAWr587",
        "outputId": "060c7b64-7923-4284-fb2a-43bb4584069e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "images_2, labels_2 = next(iter(train_set))\n",
        "\n",
        "gridlist2 = [images_2[0]*.23 + 0.45,images_2[1]*0.23 + 0.45,(images_2[3]*.23)+0.45,(images_2[4]*.23)+0.45]\n",
        "gridimgs2 = utils.make_grid(gridlist2)\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "\n",
        "show(gridimgs2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb1SBHzDtyQa",
        "outputId": "9d78c18c-deb9-4d55-899d-f965d1aa01b5"
      },
      "source": [
        "print(images[0])\n",
        "print(images_2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.1075, -1.3815, -1.2617,  ...,  0.5878,  0.4851,  0.4166],\n",
            "         [-1.8439, -2.1179, -1.8097,  ..., -0.0116, -0.0801, -0.0287],\n",
            "         [-1.6898, -1.8439, -1.2788,  ..., -0.0972, -0.0629, -0.2513],\n",
            "         ...,\n",
            "         [ 1.4440,  1.3242,  1.2728,  ...,  0.6221, -1.1589, -1.2103],\n",
            "         [ 0.9646,  0.8447,  1.0673,  ...,  1.0331, -0.4568, -0.6965],\n",
            "         [ 0.9132,  0.7591,  0.9474,  ...,  1.5810,  0.4679, -0.0116]],\n",
            "\n",
            "        [[-0.9503, -1.2304, -1.1954,  ...,  0.2752,  0.1527,  0.1352],\n",
            "         [-1.6856, -2.0357, -1.8957,  ..., -0.4951, -0.5826, -0.5126],\n",
            "         [-1.6155, -1.9132, -1.5630,  ..., -0.5651, -0.5651, -0.7577],\n",
            "         ...,\n",
            "         [ 0.9405,  0.6429,  0.7829,  ...,  0.2927, -1.4930, -1.4405],\n",
            "         [ 0.3978,  0.1176,  0.4853,  ...,  0.5553, -0.9503, -1.1078],\n",
            "         [ 0.4853,  0.2227,  0.4503,  ...,  1.1856,  0.0301, -0.4251]],\n",
            "\n",
            "        [[-0.7064, -1.0201, -1.0550,  ...,  0.0779, -0.0267, -0.0092],\n",
            "         [-1.4559, -1.8044, -1.8044,  ..., -0.8458, -0.9330, -0.8110],\n",
            "         [-1.4384, -1.8044, -1.6650,  ..., -0.9330, -0.9330, -1.0724],\n",
            "         ...,\n",
            "         [-0.1312, -1.2119, -1.3513,  ..., -0.5844, -1.6824, -1.4559],\n",
            "         [-0.1312, -1.0724, -1.2816,  ..., -0.1661, -1.2119, -1.2119],\n",
            "         [ 0.2173, -0.1661, -0.2881,  ...,  0.6356, -0.3404, -0.5495]]])\n",
            "tensor([[[-1.1075, -1.3815, -1.2617,  ...,  0.5878,  0.4851,  0.4166],\n",
            "         [-1.8439, -2.1179, -1.8097,  ..., -0.0116, -0.0801, -0.0287],\n",
            "         [-1.6898, -1.8439, -1.2788,  ..., -0.0972, -0.0629, -0.2513],\n",
            "         ...,\n",
            "         [ 1.4440,  1.3242,  1.2728,  ...,  0.6221, -1.1589, -1.2103],\n",
            "         [ 0.9646,  0.8447,  1.0673,  ...,  1.0331, -0.4568, -0.6965],\n",
            "         [ 0.9132,  0.7591,  0.9474,  ...,  1.5810,  0.4679, -0.0116]],\n",
            "\n",
            "        [[-0.9503, -1.2304, -1.1954,  ...,  0.2752,  0.1527,  0.1352],\n",
            "         [-1.6856, -2.0357, -1.8957,  ..., -0.4951, -0.5826, -0.5126],\n",
            "         [-1.6155, -1.9132, -1.5630,  ..., -0.5651, -0.5651, -0.7577],\n",
            "         ...,\n",
            "         [ 0.9405,  0.6429,  0.7829,  ...,  0.2927, -1.4930, -1.4405],\n",
            "         [ 0.3978,  0.1176,  0.4853,  ...,  0.5553, -0.9503, -1.1078],\n",
            "         [ 0.4853,  0.2227,  0.4503,  ...,  1.1856,  0.0301, -0.4251]],\n",
            "\n",
            "        [[-0.7064, -1.0201, -1.0550,  ...,  0.0779, -0.0267, -0.0092],\n",
            "         [-1.4559, -1.8044, -1.8044,  ..., -0.8458, -0.9330, -0.8110],\n",
            "         [-1.4384, -1.8044, -1.6650,  ..., -0.9330, -0.9330, -1.0724],\n",
            "         ...,\n",
            "         [-0.1312, -1.2119, -1.3513,  ..., -0.5844, -1.6824, -1.4559],\n",
            "         [-0.1312, -1.0724, -1.2816,  ..., -0.1661, -1.2119, -1.2119],\n",
            "         [ 0.2173, -0.1661, -0.2881,  ...,  0.6356, -0.3404, -0.5495]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3CcbYPobftH",
        "outputId": "45040fd6-1992-47a7-d61e-97f40c4e9de1"
      },
      "source": [
        "list(range(0,len(test),2))[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2ttuW9DbYjT",
        "outputId": "3ed7b7c2-92ac-4525-c978-94a1bbcea27d"
      },
      "source": [
        "len(test_subset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNWQOWfYbT3w"
      },
      "source": [
        "iterated = iter(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "WirBjGm3ko_C",
        "outputId": "94903b2c-f39d-4b85-cad6-ce5a1d794360"
      },
      "source": [
        "first(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0b0fcde5a52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'first' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xGMyAXgtixma",
        "outputId": "08390c31-01f1-4175-fc08-0ab5e32e05ff"
      },
      "source": [
        "counter=0\n",
        "for batch in train_set:\n",
        "    x,y = batch\n",
        "    x2,y2 = next(iter(data_loader))\n",
        "    print(x[0]-x2[0])\n",
        "    print(y-y2)\n",
        "    if counter > 10:\n",
        "        exit()\n",
        "    counter+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -2,  1,  3,  1, -4, -2, -4, -8, -2,  1, -5,  4,  4, -3,  0, -1,  3,\n",
            "         5, -2, -1, -1,  5,  8, -2, -8, -5,  1, -3,  4,  8,  1,  7, -1, -2, -1,\n",
            "        -2,  4, -3,  0,  0,  2, -4,  0, -9,  4, -6,  8,  8, -1, -6, -9, -5,  0,\n",
            "         7, -4,  4, -1,  5, -9,  6, -4, -1,  2, -2, -2, -2,  2, -1,  6,  4, -1,\n",
            "         2, -6, -1,  1, -4, -1,  1,  3,  5,  2, -5, -6,  2, -7, -4,  0,  2, -6,\n",
            "         0,  3,  3, -4, -1, -6, -8, -1,  3,  0, -5, -2, -1,  3, -3, -3,  2, -4,\n",
            "         1,  4,  1, -1, -6,  2, -6, -3, -3,  2,  0, -1, -3,  2, -3, -2,  1,  7,\n",
            "        -5,  1])\n",
            "tensor([[[ 1.0617,  1.3529,  1.3015,  ..., -0.1370, -0.0342, -2.5345],\n",
            "         [ 1.8152,  2.1406,  1.8495,  ...,  0.4795,  0.5651, -2.0892],\n",
            "         [ 1.6269,  1.8323,  1.3015,  ...,  0.5651,  0.5480, -1.8666],\n",
            "         ...,\n",
            "         [-3.3907, -3.1852, -3.1852,  ..., -2.2605, -0.6165, -0.9076],\n",
            "         [-2.9112, -2.7400, -2.9968,  ..., -2.7742, -1.3357, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.8557,  2.1534,  2.1709,  ...,  0.9279,  1.0854, -2.1709],\n",
            "         [ 2.5735,  2.9762,  2.8887,  ...,  1.6632,  1.7857, -1.5231],\n",
            "         [ 2.4685,  2.8186,  2.5210,  ...,  1.6982,  1.7507, -1.2780],\n",
            "         ...,\n",
            "         [-2.5385, -2.3810, -2.4335,  ..., -1.1204,  0.3676, -0.5952],\n",
            "         [-1.8557, -1.6282, -1.9958,  ..., -1.4706, -0.1576, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.1786,  2.5098,  2.5970,  ...,  1.6906,  1.8126, -1.7952],\n",
            "         [ 2.9281,  3.3290,  3.3290,  ...,  2.5795,  2.7190, -0.9935],\n",
            "         [ 2.8758,  3.2767,  3.1198,  ...,  2.6144,  2.7015, -0.7320],\n",
            "         ...,\n",
            "         [-1.0109, -0.0349,  0.2266,  ...,  0.3660,  1.0980, -0.3486],\n",
            "         [-0.8192,  0.0871,  0.2963,  ..., -0.1569,  0.6449, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 2, -3,  4, -1, -3,  1, -5, -4,  0,  6, -1,  3,  3,  4,  3,  2,  7, -1,\n",
            "        -3,  3, -5,  6,  0,  4,  7, -5,  1,  5,  3, -2,  7,  1,  7,  2, -7, -5,\n",
            "         1,  0,  0,  1,  1, -1,  0, -1, -6,  1, -3,  4,  3,  1, -1, -5,  2, -4,\n",
            "         1, -1,  8, -3,  0, -9,  5, -9,  2,  6, -5,  2,  2,  5,  7, -2,  3, -2,\n",
            "         7, -7, -2,  1,  0, -1, -1, -4, -1,  3, -6, -2,  2, -4, -7,  3, -1, -6,\n",
            "         0,  1, -1, -4, -7,  0, -3,  1,  3, -4,  0, -3, -4,  1,  0, -4,  3, -3,\n",
            "        -1,  7,  0,  1, -1,  5, -5, -7,  0, -1,  3,  1, -1, -1, -2, -2, -5, -1,\n",
            "        -3,  4])\n",
            "tensor([[[ 0.7877,  1.5755,  0.8220,  ..., -1.3529, -1.5241, -2.5345],\n",
            "         [ 0.8049,  2.2433,  1.5584,  ..., -0.9247, -0.8391, -2.0892],\n",
            "         [ 0.5480,  1.9693,  1.2501,  ..., -0.8734, -0.9590, -1.8666],\n",
            "         ...,\n",
            "         [-0.9590, -0.5651, -0.5651,  ..., -1.0960,  0.5994, -0.9076],\n",
            "         [-0.7535, -0.4624, -0.6850,  ..., -1.6269, -0.1199, -1.4214],\n",
            "         [-0.9932, -0.8049, -0.8905,  ..., -2.2433, -1.0960, -2.1063]],\n",
            "\n",
            "        [[ 1.0679,  2.0308,  1.2955,  ..., -0.8929, -0.9104, -2.1709],\n",
            "         [ 1.0329,  2.7136,  2.2234,  ..., -0.2451, -0.0175, -1.5231],\n",
            "         [ 0.8053,  2.5210,  2.1008,  ..., -0.2276, -0.1751, -1.2780],\n",
            "         ...,\n",
            "         [-0.8228, -0.7003, -0.7703,  ..., -0.1576,  1.5581, -0.5952],\n",
            "         [-0.2101,  0.0175, -0.3501,  ..., -0.5602,  0.9804, -0.9279],\n",
            "         [-0.2276,  0.0350, -0.1751,  ..., -1.2605, -0.0700, -1.6106]],\n",
            "\n",
            "        [[ 1.0980,  2.6144,  1.1329,  ..., -1.0980, -0.9063, -1.7952],\n",
            "         [ 0.9586,  3.1198,  2.1612,  ..., -0.2266,  0.0871, -0.9935],\n",
            "         [ 0.6623,  2.8932,  2.6318,  ...,  0.1917,  0.1743, -0.7320],\n",
            "         ...,\n",
            "         [ 0.3660,  1.3072,  1.3420,  ..., -0.5403,  0.4706, -0.3486],\n",
            "         [ 0.3312,  1.1678,  1.3943,  ..., -1.0283,  0.0174, -0.5926],\n",
            "         [ 0.0000,  0.3486,  0.5926,  ..., -1.8649, -0.8715, -1.2549]]])\n",
            "tensor([-5, -8,  4,  0, -4, -2, -7, -5, -4,  5,  2,  3,  5,  5,  1,  4,  0,  3,\n",
            "        -2,  2,  2,  4,  5,  3, -2, -8, -4,  7, -3, -1,  4, -6,  7,  3,  0, -5,\n",
            "         0,  1, -4,  4,  4,  5, -3, -1, -3,  0,  0,  4,  5, -4,  0,  0,  2, -7,\n",
            "        -1, -7,  1, -1,  6, -8,  5, -2, -1,  4, -5, -2, -3,  2,  7,  3,  0,  4,\n",
            "         0, -7, -5,  1, -1, -3, -2,  2,  8,  3,  2, -7,  5, -4, -7,  0,  7,  2,\n",
            "         5, -2,  6, -5, -6,  0, -1,  1,  2, -1, -4, -2, -3,  3,  0, -5,  3,  0,\n",
            "        -2,  7,  6, -2,  1, -1, -6, -4,  0,  3,  0,  0, -8, -2,  1, -2, -1,  4,\n",
            "        -6, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.2195, -3.2366, -3.2023,  ..., -0.8734,  0.9076, -0.9076],\n",
            "         [-2.8941, -2.7400, -2.9283,  ..., -1.4727,  0.0685, -1.4214],\n",
            "         [-2.8085, -2.6543, -2.8256,  ..., -2.1748, -1.0275, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.6261, -2.4685, -2.6436,  ..., -0.3852,  1.4006, -0.5952],\n",
            "         [-2.2409, -1.9083, -2.2584,  ..., -0.7703,  0.7878, -0.9279],\n",
            "         [-2.2934, -2.0308, -2.2409,  ..., -1.5756, -0.4202, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.4641, -0.5054, -0.3834,  ..., -0.0349,  1.0806, -0.3486],\n",
            "         [-1.6209, -0.6449, -0.4357,  ..., -0.5577,  0.5577, -0.5926],\n",
            "         [-1.9346, -1.5512, -1.4292,  ..., -1.4641, -0.4532, -1.2549]]])\n",
            "tensor([-4, -3,  2,  6,  1, -2, -1,  0, -5, -1,  5, -6,  1,  0, -2,  6, -1, -2,\n",
            "         6, -3,  3,  1,  6, -1, -1, -6, -2,  7,  4,  5,  4,  0,  8,  3, -7, -3,\n",
            "         0,  6, -6,  1,  2,  2, -6, -4, -4,  1, -3,  6,  6,  1, -6, -1, -3, -1,\n",
            "         6,  0, -1,  2,  6, -6,  1, -9,  1,  3, -1, -4,  3,  1,  4,  7,  5,  5,\n",
            "         5,  0, -3, -2, -7,  3, -1,  2,  2, -3,  0, -2,  3,  1, -5,  2,  4, -2,\n",
            "        -3,  1, -1, -8,  0, -3, -7, -2, -1, -5,  0, -3,  1,  2, -9,  0,  0, -3,\n",
            "        -8, -1,  8, -3, -4,  2,  2, -3, -2,  3,  4,  5, -1,  2, -5, -1,  2,  7,\n",
            "        -6, -1])\n",
            "tensor([[[ 3.0825,  3.4078,  3.3222,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.8531,  4.1784,  3.9558,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 3.7846,  3.9729,  3.4592,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.3803, -2.2605, -2.2776,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 3.0812,  3.4139,  3.4139,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 3.8515,  4.2542,  4.2192,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 3.8866,  4.2192,  3.9216,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.7332, -1.4356, -1.6632,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 3.1547,  3.5033,  3.5730,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 3.9216,  4.3050,  4.3747,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 3.9390,  4.3050,  4.2179,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.4532,  0.6100,  0.6449,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 2,  0,  3, -1, -5,  5, -3, -7, -3,  7,  2, -3, -2,  2, -1,  1,  1,  0,\n",
            "         1,  5, -2,  5,  4,  5,  7, -9, -2,  4,  3, -3,  3, -3, -1,  0, -5,  3,\n",
            "        -4,  5,  0,  3,  6,  1, -4, -2, -1,  4, -7,  8,  2, -1, -8, -2, -6, -1,\n",
            "         5, -4,  0, -5,  8, -4,  6, -4, -1,  1, -3, -4, -6,  4,  0,  0,  7,  0,\n",
            "        -2, -4, -3, -1,  1, -3,  1,  4,  5,  3, -1, -1,  6, -2,  1,  3, -1,  2,\n",
            "        -1,  2,  1, -3, -3, -8, -1,  7,  1,  1, -2, -7,  2,  1, -1,  1,  8, -7,\n",
            "         0,  3,  9, -6, -3,  7, -6,  2,  0,  3,  0,  5, -2, -2, -6, -2, -1,  4,\n",
            "         0,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2569,  0.0171, -0.2911,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.3939, -0.2055, -0.7706,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3326,  0.0175, -0.1225,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.3852, -0.0525, -0.4027,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3312,  0.0174,  0.0174,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3312,  0.0697, -0.0697,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -8,  1,  4, -5,  2,  1, -3, -8,  5,  4, -1,  7,  0,  4,  5,  6, -3,\n",
            "        -3,  4, -2,  1,  8,  1,  5, -7, -5, -1,  1, -4,  6, -6,  5,  0, -5,  1,\n",
            "         1,  2, -4, -1,  2,  1, -5, -1,  0,  5, -8,  1,  0,  5, -7, -2, -5, -7,\n",
            "        -1, -8, -1, -3,  1, -7, -2, -5, -3,  4, -4, -2,  2,  3,  1,  4,  0, -3,\n",
            "        -2, -4,  2, -2,  1,  1,  2,  2,  0, -2, -6, -6,  5, -3, -3,  0,  7, -3,\n",
            "         1, -4, -1, -3, -1, -2, -5,  6,  6,  1,  0, -9, -6,  6, -8,  1,  3,  0,\n",
            "         1,  3,  1, -4,  2,  3, -1, -1,  1, -3,  6,  6, -4, -6, -7, -3,  0,  4,\n",
            "        -2,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ...,  0.9247, -0.0856, -1.5926],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  1.3357,  0.4452, -1.1302],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  1.4385,  1.0446, -0.5994],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.0700, -0.4377, -1.1380],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.7178,  0.2976, -0.4552],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.6653,  0.9629,  0.1050],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.2614, -0.3660, -0.6797],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.6275,  0.5403,  0.1394],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  0.6972,  1.2200,  0.6623],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -1,  4,  6, -5,  5,  2, -8, -9,  3,  0,  0,  2,  5,  1,  6,  8,  5,\n",
            "        -1,  3,  0,  5,  1, -1,  5,  0,  2,  4, -3,  3,  1, -5,  1,  3,  1,  1,\n",
            "        -5, -2, -2,  0,  0,  2, -2,  3, -9,  4, -6,  5, -1,  0, -3, -3,  1, -8,\n",
            "         5, -5,  0, -5,  2, -9,  4, -3, -6,  5,  1, -3, -2,  8,  2,  6,  7,  6,\n",
            "         6, -2, -3,  2,  0,  3,  4,  5,  5, -2, -5, -2,  0, -5,  1,  0,  0,  1,\n",
            "         5, -1,  3, -3, -6,  0, -1, -1,  7, -4,  3, -3, -5,  0, -6, -1,  0, -5,\n",
            "        -2,  2,  6, -3,  3,  7, -3,  2,  2, -3,  4,  7, -8,  3, -1, -7,  0, -1,\n",
            "        -1, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 0.3254,  0.7192,  0.3939,  ..., -0.9761, -0.9419, -1.0275],\n",
            "         [ 0.2569,  0.3939, -0.1712,  ..., -0.7706, -0.8734, -0.7364],\n",
            "         ...,\n",
            "         [-2.9968, -2.8598, -2.7913,  ..., -1.9865, -0.2055, -0.1370],\n",
            "         [-2.6030, -2.4488, -2.6201,  ..., -2.3461, -0.8562, -0.6336],\n",
            "         [-2.6030, -2.4317, -2.6030,  ..., -2.9112, -1.7810, -1.2844]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 0.4202,  0.9104,  0.8403,  ...,  0.4902,  0.5602,  0.4552],\n",
            "         [ 0.4202,  0.7353,  0.4202,  ...,  0.6127,  0.5777,  0.7703],\n",
            "         ...,\n",
            "         [-2.1359, -1.8207, -1.9433,  ..., -1.2255,  0.5602,  0.5077],\n",
            "         [-1.6807, -1.3831, -1.6982,  ..., -1.5231, -0.0525,  0.1050],\n",
            "         [-1.8382, -1.5581, -1.7682,  ..., -2.2059, -1.0329, -0.5602]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.1220,  0.8017,  0.8889,  ...,  2.1961,  2.2484,  2.0915],\n",
            "         [ 0.2440,  0.6972,  0.5752,  ...,  2.4401,  2.3878,  2.4575],\n",
            "         ...,\n",
            "         [-1.5338, -0.4357, -0.3137,  ..., -1.0632,  0.1569, -0.0523],\n",
            "         [-1.5861, -0.6275, -0.4009,  ..., -1.3943, -0.2440, -0.3137],\n",
            "         [-1.9695, -1.5512, -1.4118,  ..., -2.2135, -1.2200, -1.0109]]])\n",
            "tensor([ 3, -5,  0, -2, -6,  1, -1, -4, -4, -2,  5, -6,  7, -2,  4,  9,  3,  2,\n",
            "         5,  4,  1,  4,  8,  0,  3, -8, -7,  0,  1, -1,  0, -6,  7,  1,  2,  4,\n",
            "        -5,  4, -6,  0, -1,  2,  1, -1, -5,  0, -2,  4,  2,  1, -8, -1, -1, -6,\n",
            "        -1, -7, -1,  0,  3, -5,  1, -6, -3,  2,  0,  5,  1,  7,  2, -2,  5,  4,\n",
            "         0, -3, -3, -3, -5, -3,  4, -1,  0, -2, -5, -1,  7, -3,  1,  5,  3,  2,\n",
            "         0,  0,  1, -4, -1, -3, -8,  0,  6,  1,  3,  0, -3, -1,  0, -5,  3, -1,\n",
            "         0,  0,  7, -8,  2,  2,  3, -1, -2, -3,  0,  6,  1, -5, -2, -2,  3,  7,\n",
            "         1,  4])\n",
            "tensor([[[ 2.5173,  2.8598,  2.8085,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.2366,  3.5791,  3.3051,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 3.0653,  3.2880,  2.7571,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.7486,  3.0637,  3.0462,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 3.4139,  3.8165,  3.6940,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 3.3088,  3.6590,  3.3438,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.7364,  3.1024,  3.1721,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 3.4336,  3.8344,  3.8693,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 3.3987,  3.8170,  3.7124,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3,  0,  2,  7, -1, -2, -5, -5, -9,  5,  2,  3, -1, -2,  0,  8,  4, -2,\n",
            "         0,  5, -5,  2,  0,  8,  0, -5,  2,  0, -4,  0,  8,  1, -1, -1,  2,  4,\n",
            "        -3,  4, -9, -1,  5, -1,  1, -4,  0,  5, -1,  7,  7,  3,  1, -6, -1,  1,\n",
            "         6, -4,  4,  2,  5, -2, -2, -3, -1,  6, -2, -1, -3,  8,  6, -2,  7, -1,\n",
            "         0, -2,  1,  4, -2,  0,  2,  4, -1,  1, -2, -6, -1, -1, -3,  0,  0, -3,\n",
            "         4, -1,  2, -5, -1, -2,  0,  1, -2, -5, -6, -3, -3,  5, -4, -5,  1, -4,\n",
            "        -1,  5,  2, -3, -3,  0, -1, -2, -5, -5,  0, -1, -8, -2, -7, -3,  0,  6,\n",
            "        -6,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -1.8323,  0.3596,  0.0171],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -2.5173, -0.8562, -0.6507],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.0825, -1.9180, -1.1816]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -1.4881,  0.7353,  0.1576],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.0833, -0.3852, -0.1751],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -2.6961, -1.5056, -0.6478]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.4357,  1.0632,  0.3137],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.1503,  0.1220,  0.1743],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.8824, -0.9237, -0.2789]]])\n",
            "tensor([ 1, -4,  3,  3,  1, -2, -5,  0, -7, -2, -2,  3, -2, -1,  5,  7,  1,  5,\n",
            "         0,  3, -1,  0,  8,  4, -2, -1, -7,  5,  4,  0,  5, -8,  7,  1, -5, -4,\n",
            "         1, -2,  0,  2, -1,  2, -6, -2, -2,  1, -2,  4,  5,  4,  1, -7, -1,  1,\n",
            "         6, -7,  4, -3,  7, -2,  2, -6,  1, -1,  2,  5, -5,  3,  6,  5,  8,  1,\n",
            "        -2,  0,  2,  2,  2,  0, -1,  3,  8, -2,  0, -2,  4, -6, -3,  6, -1, -1,\n",
            "        -4, -4,  3, -4, -9,  1,  1,  1,  2,  2,  3, -8, -2,  1, -1, -1,  6, -6,\n",
            "        -4,  5,  3,  0, -4,  3,  1, -2,  3,  3,  0,  4, -2, -4, -3,  1, -3,  0,\n",
            "        -7, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.8531,  4.1442,  3.8359,  ...,  1.6954, -2.0378, -2.0892],\n",
            "         [ 3.5619,  3.7332,  3.1510,  ...,  1.7296, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.9693, -1.7810, -1.7638,  ..., -0.1370, -0.9590, -0.9076],\n",
            "         [-1.1816, -0.8049, -1.2844,  ..., -0.2226, -1.6611, -1.4214],\n",
            "         [-0.9076, -0.3939, -0.8391,  ..., -0.5309, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 3.9041,  4.2542,  4.1141,  ...,  2.5210, -1.4531, -1.5231],\n",
            "         [ 3.7290,  4.0266,  3.6590,  ...,  2.5210, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.4706, -1.1905, -1.4181,  ...,  0.1401, -0.5427, -0.5952],\n",
            "         [-0.6478, -0.1926, -0.8754,  ...,  0.2451, -1.0854, -0.9279],\n",
            "         [-0.5427,  0.0175, -0.5252,  ..., -0.0700, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 3.8519,  4.2179,  4.2179,  ...,  3.1721, -0.8715, -0.9935],\n",
            "         [ 3.7298,  4.1133,  3.9564,  ...,  3.2767, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.0697,  1.0283,  1.0980,  ...,  1.4815, -0.1220, -0.3486],\n",
            "         [ 0.2266,  1.4118,  1.3072,  ...,  1.3595, -0.5926, -0.5926],\n",
            "         [ 0.1046,  0.8366,  0.6797,  ...,  0.8017, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -8,  3,  1, -3,  2, -3, -2,  0,  3,  0, -4,  2, -1,  0,  7,  4,  0,\n",
            "         5, -2, -2,  5,  3,  6,  1, -8, -4,  1,  4,  1,  6, -4,  2, -4, -2, -4,\n",
            "         1,  2, -9,  3,  2,  2, -7,  4, -5,  6, -4,  7,  2,  0, -7, -2,  3, -3,\n",
            "         1, -8,  3, -6,  8, -6,  4,  0,  3,  3,  3,  5, -2,  6,  7,  3,  0,  5,\n",
            "         7, -5,  0,  6, -5, -4,  0,  0,  1, -2, -1, -2,  2, -5, -1,  5,  1, -2,\n",
            "         5,  0,  8, -2, -6, -4, -6,  6,  2, -1,  2, -1, -4,  6, -2, -6,  0, -4,\n",
            "         0,  7,  2, -1, -4,  7, -5, -5, -1, -2,  4,  1, -8, -4, -7, -4, -6,  8,\n",
            "        -1, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.5345,  2.5002,  2.4660,  ...,  0.4966, -2.0378, -2.0892],\n",
            "         [ 2.2947,  1.9180,  1.6782,  ...,  0.6336, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.6679, -0.6679, -0.6507,  ..., -0.3082, -0.9590, -0.9076],\n",
            "         [-0.1370, -0.1199, -0.3254,  ..., -0.8220, -1.6611, -1.4214],\n",
            "         [-0.2911, -0.3767, -0.4452,  ..., -1.2159, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.0658,  2.1359,  2.2759,  ...,  0.7003, -1.4531, -1.5231],\n",
            "         [ 1.9083,  1.6982,  1.6807,  ...,  0.8228, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.4027, -0.3151, -0.4377,  ..., -0.2626, -0.5427, -0.5952],\n",
            "         [ 0.1926,  0.2451, -0.0875,  ..., -0.6478, -1.0854, -0.9279],\n",
            "         [-0.1050, -0.1751, -0.2626,  ..., -1.1029, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.6906,  1.6906,  1.9521,  ...,  0.8192, -0.8715, -0.9935],\n",
            "         [ 1.5338,  1.3595,  1.5512,  ...,  0.9586, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.2789,  1.2200,  1.4118,  ...,  0.3660, -0.1220, -0.3486],\n",
            "         [ 0.3312,  1.1155,  1.4118,  ..., -0.1569, -0.5926, -0.5926],\n",
            "         [-0.1220, -0.0174,  0.2266,  ..., -0.8017, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -8,  5,  7, -5, -1, -3, -2, -4,  2,  5, -5,  1,  0,  2,  4,  0,  5,\n",
            "        -3,  5, -1,  2,  8,  3,  3, -1,  1,  5,  2, -3,  6, -1,  8,  5, -7,  2,\n",
            "        -2,  6, -7, -3, -3, -2, -3,  2, -8,  3, -1,  6,  7,  4,  1, -9,  1,  0,\n",
            "         2, -5,  2, -2,  5, -1,  6, -6,  3, -1, -3, -3, -2, -1,  1,  1,  6,  4,\n",
            "        -2, -7,  1,  6, -6,  3,  2,  2,  5, -3, -3, -7,  4, -3, -2,  3,  2, -5,\n",
            "         3, -4,  3, -8, -3, -6, -8,  2,  2,  0, -3, -4, -4,  8,  0,  1,  2, -6,\n",
            "        -1,  4,  3,  1,  0,  1,  3,  0,  2, -1,  5,  5, -8, -5, -7, -6, -5,  8,\n",
            "        -2,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.4452, -0.2226, -0.1370,  ..., -0.2740,  1.4042,  0.7706],\n",
            "         [-0.1027,  0.1541,  0.0514,  ..., -0.3939,  0.9247,  0.8049],\n",
            "         [-0.0342,  0.1884,  0.1541,  ..., -0.7877,  0.1712,  0.4110]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.1225,  0.2801,  0.1751,  ...,  0.3326,  2.0308,  1.2780],\n",
            "         [ 0.2801,  0.7003,  0.4552,  ...,  0.3676,  1.7157,  1.5056],\n",
            "         [ 0.1751,  0.5252,  0.4552,  ..., -0.0525,  0.9279,  1.1029]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.2026,  2.3878,  2.5621,  ...,  1.4989,  2.5970,  1.6558],\n",
            "         [ 1.0632,  2.1438,  2.4749,  ...,  1.3595,  2.3529,  1.9695],\n",
            "         [ 0.6797,  1.1678,  1.4641,  ...,  0.6972,  1.6209,  1.5686]]])\n",
            "tensor([-3, -9,  2,  2, -3,  1, -4, -8, -5,  5,  5, -6,  7, -1,  1,  9,  2,  4,\n",
            "         3, -3, -2,  5,  4,  4,  7, -2,  2,  0,  1, -1,  3, -4,  8, -2, -7,  3,\n",
            "         1,  1, -8, -3, -3,  2, -6, -4, -5,  2, -1,  5,  8, -2, -6, -2, -1, -1,\n",
            "         7, -7,  1, -3,  1, -8,  7, -7,  3,  3, -4, -2, -6,  2,  1,  2,  3,  2,\n",
            "         1, -6, -2, -3, -4, -2, -1, -3, -1,  3,  0, -4,  3, -4, -4,  2,  4, -2,\n",
            "         3, -1,  0, -3, -2, -8, -1,  1,  2, -2, -2,  0, -5,  1, -3,  2,  6, -1,\n",
            "        -1,  7,  3, -4,  1,  8, -3, -6,  2, -4,  2,  6,  1, -1, -4, -4, -5,  7,\n",
            "        -7, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.0685,  0.0856,  0.0000],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.3082,  0.2055,  0.1541],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -1.7981,  0.1370,  0.1541],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -2.0721, -0.4795, -0.3254],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -2.4831, -1.3529, -0.9590]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.9104,  0.9454,  0.7528],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.1204,  1.0679,  0.9454],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.8403,  1.2080,  1.2605],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.9104,  0.8053,  0.9804],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.2780, -0.0175,  0.3852]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.5338,  1.5338,  1.3072],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.7255,  1.6906,  1.5163],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.7495,  0.4009,  0.1220],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.0109,  0.0174, -0.1917],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.7778, -0.8540, -0.8715]]])\n",
            "tensor([-4, -4,  1, -2, -7,  1, -4, -1, -8, -1, -1, -3,  2,  6, -4,  5, -1,  4,\n",
            "         1,  2, -4,  2, -1,  8,  5, -4, -2,  7,  0,  4,  6, -2,  6, -1, -4,  4,\n",
            "        -1, -3, -5,  1,  0,  1, -6,  5, -7,  0,  0,  8, -1,  0, -6, -9, -1, -5,\n",
            "        -1, -8,  6, -6,  4, -9, -2, -7, -4,  6, -3,  2, -5,  2,  5,  0,  3,  4,\n",
            "         7, -1, -5,  5,  2,  3, -1,  5,  8,  1,  0, -4,  4, -3,  2,  0,  7, -1,\n",
            "        -3,  0,  1, -3, -9, -3,  1,  1,  4,  2,  2,  0, -2,  5, -7,  3,  7, -8,\n",
            "        -4,  0,  7, -8,  0,  4,  2, -1,  3,  1,  0,  4, -4,  0,  0, -3, -2,  5,\n",
            "        -4,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.1577,  2.6030,  2.2091,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.5651, -0.4452, -0.7877,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.0856,  0.0514, -0.4110,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.1027,  0.1712, -0.1884,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.4356,  2.1359,  2.0483,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.3326, -0.0350, -0.2801,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.3676,  0.5077,  0.2101,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.3151,  0.4727,  0.1576,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.6383,  2.3704,  2.4401,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.0109,  2.0044,  1.9521,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 1.1329,  1.8824,  2.1264,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.7843,  1.0458,  1.1329,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -7,  3,  7, -1,  1,  1, -7, -2,  3, -3,  0, -1, -2,  3,  3,  4,  0,\n",
            "         5,  4, -1,  1,  5, -1,  0, -5, -4, -1,  4,  3,  4, -1,  2, -4, -7, -5,\n",
            "        -2,  1, -7, -3,  0, -1, -7,  5, -1,  4, -8,  8, -1, -1, -7, -4, -6, -3,\n",
            "         2, -8,  8,  1,  6, -2,  5, -5,  0,  3,  0,  5,  2,  4, -1,  2,  4,  2,\n",
            "         4, -8, -1,  0, -3, -2, -3,  5,  4,  2,  0, -6,  1, -6, -1,  1,  1,  0,\n",
            "        -2,  4,  1, -6, -3, -8, -7,  2,  6, -1, -4, -5, -2,  4, -1, -6,  1, -7,\n",
            "        -4,  2,  0, -2,  0,  6, -5,  1,  0, -5,  0,  6, -7, -2, -3,  0, -6,  1,\n",
            "        -4,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.6336,  0.8905,  0.3939,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.1338, -3.0311, -3.0311,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-2.7057, -2.4317, -2.8085,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-2.7742, -2.3975, -2.6030,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.6127,  0.9804,  0.6478,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.4860, -2.2059, -2.3810,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.0133, -1.5581, -2.0658,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.2234, -1.7157, -1.9258,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.5054,  0.8889,  0.7146,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.1503, -0.0871,  0.0349,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.2549, -0.1394, -0.0174,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-1.7429, -1.0980, -0.9237,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -7, -1,  7,  1, -1, -1,  0, -4,  5,  1, -2,  6, -3,  0,  4,  7, -4,\n",
            "        -2,  4, -2,  6,  1,  7,  1, -8, -5,  2,  4, -2,  7,  1, -1, -4, -1, -3,\n",
            "        -3, -1, -5,  0, -1,  4, -7,  2,  0,  5,  1,  5,  6,  5,  1, -9, -2, -1,\n",
            "         5,  0,  0, -1,  2, -5,  0,  0, -5,  6, -4, -1, -3,  1,  8,  7,  4,  6,\n",
            "         0,  0, -3,  3, -3, -4,  0, -2,  6, -3,  0,  0,  3, -5, -1, -2,  2, -7,\n",
            "         2,  1,  7, -5, -5,  0,  1,  7,  6, -2, -3, -1,  3, -1, -5, -5,  5, -8,\n",
            "        -5,  2,  4, -4,  0,  4,  1,  0, -5,  1, -1, -1,  0, -3, -7,  0, -2, -1,\n",
            "        -8,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -1.1645, -1.0960, -0.7877],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -1.0617, -1.1816, -0.6679],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.8734,  1.9865,  2.0207],\n",
            "         [-3.0825, -2.9626, -3.1852,  ...,  0.3082,  1.5412,  1.4556],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -0.6507,  0.1027,  0.3425]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.2976,  0.2626,  0.3326],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.4202,  0.2451,  0.5427],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.9454,  2.0483,  1.9083],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  0.5427,  1.7857,  1.5406],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.5077,  0.2801,  0.4202]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.3137, -0.2614, -0.1743],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.1569, -0.2789,  0.0697],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.1678,  1.5861,  1.3072],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.7320,  1.5163,  1.0458],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -0.4183,  0.2266,  0.0523]]])\n",
            "tensor([ 0, -5,  0,  4, -5,  1,  2, -1, -8,  5, -1, -2,  4,  5, -1,  4,  4, -1,\n",
            "         2,  3,  4,  6,  7,  4,  5, -6,  1,  7, -3,  1,  5, -3,  5, -4,  0, -2,\n",
            "        -4, -1, -9,  3,  3,  1, -2,  1, -1,  7,  1,  8,  1,  3, -5, -1, -5, -3,\n",
            "         1, -9,  1, -5,  0,  0,  4, -8, -6,  4,  2,  1, -3,  7,  0,  7,  3,  6,\n",
            "         3, -7, -2,  1, -7, -2,  2,  4,  7, -2, -5, -8,  4,  1, -4,  0,  8, -5,\n",
            "        -2,  1,  8, -3, -6, -6, -6, -1,  0,  3,  1, -9, -4, -1,  0, -3,  3,  0,\n",
            "        -1,  2,  0, -5, -6,  2, -2,  0, -4,  0,  2,  5, -8, -5, -3, -3, -3,  0,\n",
            "        -1,  3])\n",
            "tensor([[[-1.0104,  1.7296,  1.7981,  ...,  0.2226,  0.4624,  0.8220],\n",
            "         [-0.2740,  2.6886,  2.4146,  ...,  1.0789,  1.1645,  1.2159],\n",
            "         [-0.4281,  2.5173,  1.9865,  ...,  0.7877,  0.8049,  1.1131],\n",
            "         ...,\n",
            "         [-3.5619, -1.0960, -1.0446,  ...,  0.3767,  1.9351,  1.8152],\n",
            "         [-3.0825, -0.2397, -0.8905,  ..., -0.2397,  1.1645,  1.5926],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  1.6632,  1.8557,  ...,  0.6653,  0.9279,  1.2080],\n",
            "         [-0.3501,  2.6261,  2.5385,  ...,  1.6982,  1.8207,  1.8207],\n",
            "         [-0.4202,  2.5910,  2.2759,  ...,  1.3831,  1.4706,  1.7507],\n",
            "         ...,\n",
            "         [-2.9762, -0.5952, -0.7003,  ...,  0.5952,  2.1534,  1.9433],\n",
            "         [-2.4335,  0.2976, -0.4552,  ...,  0.1401,  1.5056,  1.8382],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  0.6972,  0.8366,  ..., -0.2963, -0.0174,  0.2440],\n",
            "         [-0.3486,  1.9346,  1.8649,  ...,  0.9935,  1.1678,  1.1329],\n",
            "         [-0.3660,  1.9346,  1.7952,  ...,  0.8889,  1.0283,  1.2723],\n",
            "         ...,\n",
            "         [-1.6732,  0.8715,  1.0806,  ...,  1.0283,  1.9346,  1.6906],\n",
            "         [-1.6732,  1.1155,  0.9586,  ...,  0.5054,  1.4292,  1.6732],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -9,  6,  1, -8,  3,  0, -1,  0,  7, -1, -2,  7,  1,  0,  9,  0,  2,\n",
            "        -2, -1, -2,  3,  8,  8,  0, -5, -6,  4,  0,  1,  4,  1,  7, -3,  2,  4,\n",
            "        -2, -3, -8,  6, -1,  0,  2,  1, -1,  7, -1,  1,  6, -2, -5, -2, -1, -7,\n",
            "         4, -6, -1, -2,  6, -4,  5,  0, -1,  7,  2,  4,  2,  5,  4,  0,  7,  1,\n",
            "         2, -2, -5,  5, -3, -2,  3, -4,  5,  4,  2, -4,  3,  0, -7, -1,  7, -5,\n",
            "         4, -5,  0, -4, -1, -7, -2, -2,  0, -1, -3, -7, -4,  4, -6,  0,  3, -2,\n",
            "        -6,  3,  8,  0, -2,  8, -1, -3, -1, -1,  6,  1,  0, -2, -4,  1,  3, -1,\n",
            "         0,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 0.0171,  0.2569, -0.0514,  ..., -0.2397, -0.0685, -0.4452],\n",
            "         [-0.1884, -0.0342, -0.5994,  ...,  0.9932,  1.0960,  0.8049],\n",
            "         ...,\n",
            "         [-2.1748, -1.9180, -1.6097,  ..., -1.6097,  1.3529,  1.4214],\n",
            "         [-1.5755, -1.2844, -1.3700,  ..., -2.0550,  0.6165,  0.9761],\n",
            "         [-1.3871, -1.1474, -1.3186,  ..., -2.7742, -0.5137,  0.2397]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.0525,  0.2801,  0.1050,  ...,  0.3151,  0.4902,  0.1225],\n",
            "         [-0.1576,  0.1225, -0.2451,  ...,  1.5056,  1.6457,  1.4006],\n",
            "         ...,\n",
            "         [-1.5931, -1.1380, -1.0504,  ..., -1.2430,  1.7507,  1.7332],\n",
            "         [-0.9279, -0.4552, -0.7353,  ..., -1.4881,  1.1555,  1.4531],\n",
            "         [-0.8929, -0.5602, -0.7878,  ..., -2.2759, -0.0350,  0.6653]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.1220,  0.2092,  0.1917,  ...,  0.8366,  1.0109,  0.6100],\n",
            "         [-0.1220,  0.2266,  0.0523,  ...,  1.9869,  2.1438,  1.8998],\n",
            "         ...,\n",
            "         [-1.2723, -0.1394,  0.1394,  ..., -0.4357,  0.6100,  0.3834],\n",
            "         [-1.2026, -0.2092,  0.0349,  ..., -0.7843,  0.1220,  0.1046],\n",
            "         [-1.4989, -1.0806, -1.0109,  ..., -1.6035, -0.7843, -0.6449]]])\n",
            "tensor([-3, -8,  2,  7, -8, -1,  0, -9, -4,  5,  0,  1,  5,  6,  1,  8,  2,  0,\n",
            "         2,  1,  3,  3,  5,  5,  4, -2,  0, -2,  3,  2, -1, -4,  6,  2, -4, -1,\n",
            "        -6,  2, -8,  2, -3,  1, -3,  1, -7,  7, -3,  6,  6, -3, -8, -1, -1,  1,\n",
            "         6,  0,  7, -7,  0, -6,  2,  0, -4,  2,  0,  3, -5,  5,  8,  0,  4,  6,\n",
            "         6, -4,  3,  6, -5, -6,  3,  5,  1,  3, -6, -2,  7, -7, -3,  2,  5, -4,\n",
            "        -3,  4,  5,  0, -2, -3, -8, -1,  2, -5,  3, -8,  2,  4, -8, -6,  8, -3,\n",
            "         0,  2,  0, -1,  2, -1, -5,  2, -3,  0,  4,  6, -7, -6, -6, -7, -3,  2,\n",
            "        -1, -2])\n",
            "tensor([[[ 1.2844,  1.1131,  0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.9693,  1.9351,  1.2159,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.0036,  1.8152,  0.9761,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.7528,  0.6478,  0.5427,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 1.4706,  1.6106,  1.1730,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.6282,  1.7157,  1.2430,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.2614,  0.2092,  0.1743,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.0109,  1.1503,  0.8715,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.2375,  1.3943,  1.1503,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -9,  2, -2, -8,  4, -6, -5, -5, -1,  2, -4,  5, -2,  5,  5,  6,  0,\n",
            "         0, -1, -5,  7,  0,  2,  4, -7, -4, -1,  1,  2,  4, -8,  0,  5,  1, -2,\n",
            "        -6,  2, -4,  6,  1,  3, -3, -1, -5, -1, -5,  4, -1, -3, -1, -6, -2,  0,\n",
            "        -1, -2,  5, -6,  5, -6, -2, -4, -1,  1, -4,  0, -5,  4,  3,  5,  6,  5,\n",
            "         5, -8,  1, -1, -5, -3, -4,  3,  1, -4, -4, -1,  1, -1,  0,  6,  8, -4,\n",
            "        -3,  4,  1, -1, -9, -7,  0,  7,  3, -2,  1, -9,  2,  2, -3, -5, -1, -3,\n",
            "        -8,  7,  3, -2, -1,  6, -2, -2, -5, -3,  6,  3, -1,  2,  0, -5,  1,  2,\n",
            "        -6,  5])\n",
            "tensor([[[ 1.1474,  1.9865,  2.2091,  ..., -1.2844, -1.1645, -2.5345],\n",
            "         [ 1.4556,  2.6886,  3.3222,  ..., -0.6850, -0.5994, -2.0892],\n",
            "         [ 1.3186,  1.9351,  2.2605,  ..., -0.5822, -0.5994, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.7703,  1.5056,  1.7682,  ..., -1.1380, -0.9979, -2.1709],\n",
            "         [ 1.2605,  2.2934,  3.0812,  ..., -0.3852, -0.2801, -1.5231],\n",
            "         [ 1.2605,  1.7332,  2.2234,  ..., -0.3151, -0.2976, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.4009,  0.9760,  1.1852,  ..., -1.3246, -1.2026, -1.7952],\n",
            "         [ 1.0632,  1.7603,  2.5447,  ..., -0.3137, -0.2092, -0.9935],\n",
            "         [ 1.2200,  1.3595,  1.9172,  ..., -0.1743, -0.1394, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -7,  2, -1, -6,  0,  2, -5, -2,  4,  0,  0,  3, -1,  0,  1,  1,  2,\n",
            "        -3, -3,  3,  4,  6,  0,  5, -9, -3, -1,  1,  5,  2, -4,  0, -3, -1,  2,\n",
            "        -5,  4, -2,  6,  1,  3,  0,  2, -1,  6, -1,  8, -1, -1, -7, -2, -6, -4,\n",
            "         1,  0, -1, -2,  5, -5, -1, -4,  0, -1, -3, -2, -2,  1,  3,  7,  7, -3,\n",
            "         2,  0,  2,  6, -1, -3, -3,  4,  6,  1, -3, -2,  0, -7, -3,  2,  0,  0,\n",
            "         0,  0,  5, -6, -3, -4, -1,  5,  4,  4, -2,  0,  1,  4, -2, -5,  7, -9,\n",
            "        -7,  6,  0, -4, -6, -1, -5, -1, -5,  0,  3,  7, -7, -4, -3, -3,  2,  8,\n",
            "        -3,  4])\n",
            "tensor([[[ 1.2330,  1.1816,  1.0960,  ..., -1.0104, -2.6030, -2.5345],\n",
            "         [ 1.6954,  2.2776,  1.9693,  ..., -0.3939, -2.0378, -2.0892],\n",
            "         [ 1.5412,  1.9180,  1.4214,  ..., -0.6165, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.5756,  1.5581,  1.5756,  ..., -0.4027, -2.1884, -2.1709],\n",
            "         [ 2.0833,  2.7311,  2.5910,  ...,  0.3852, -1.4531, -1.5231],\n",
            "         [ 2.0658,  2.5210,  2.2234,  ...,  0.1401, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.4880,  0.5926,  0.4880,  ..., -0.5054, -1.7778, -1.7952],\n",
            "         [ 0.8540,  1.4641,  1.4641,  ...,  0.5229, -0.8715, -0.9935],\n",
            "         [ 0.7320,  1.3246,  1.2723,  ...,  0.3312, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -5,  3,  0, -7,  3, -2, -1, -2, -1, -3, -1, -2, -1, -4,  6,  8, -4,\n",
            "         2, -1, -2,  7,  7, -1,  1, -5, -6,  2, -2,  0,  8, -4, -1,  1, -1,  3,\n",
            "         2,  3, -8, -1,  4,  5, -4, -1, -1,  0, -4,  0,  4,  5, -4, -4, -1, -3,\n",
            "         3, -7,  0, -5,  4, -8,  0, -5, -1,  8, -4,  5, -6,  5,  2,  0,  8,  6,\n",
            "        -2, -6, -4, -3,  0,  3,  2,  2,  2,  3,  0,  0,  3, -5, -1,  1,  8, -5,\n",
            "         2,  4, -1, -8, -3, -7,  0,  5,  3, -1, -2, -4,  3,  8, -2, -5,  0, -3,\n",
            "         0,  0,  7, -6, -6, -1, -6, -5, -1,  3,  8,  3, -8, -1, -6, -3,  2,  5,\n",
            "        -4,  5])\n",
            "tensor([[[-0.9932, -0.7192, -0.8391,  ..., -2.6886, -2.5858, -2.5345],\n",
            "         [-0.2569,  0.0171, -0.2911,  ..., -2.0892, -2.0207, -2.0892],\n",
            "         [-0.4110, -0.2569, -0.8220,  ..., -2.0036, -2.0378, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0679, -0.7878, -0.8228,  ..., -2.2934, -2.1709, -2.1709],\n",
            "         [-0.3326,  0.0175, -0.1225,  ..., -1.5231, -1.4356, -1.5231],\n",
            "         [-0.4027, -0.1050, -0.4552,  ..., -1.4531, -1.4531, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0806, -0.7669, -0.7320,  ..., -1.8649, -1.7603, -1.7952],\n",
            "         [-0.3312,  0.0174,  0.0174,  ..., -0.9412, -0.8540, -0.9935],\n",
            "         [-0.3486,  0.0174, -0.1220,  ..., -0.8540, -0.8540, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -8,  7,  1, -7,  2, -5, -6, -1,  3, -2, -1, -2,  4,  1,  8,  5, -1,\n",
            "        -2,  5,  3,  1,  4,  4,  3, -8, -2,  0, -5,  2,  6, -5,  8, -4,  1,  4,\n",
            "        -4,  4, -3,  4, -3, -1, -5,  5, -4,  6, -2,  6,  3,  3, -2, -9,  1, -8,\n",
            "         2, -4,  2,  0,  1, -4, -1, -6, -2,  1,  2,  2,  0,  8,  4,  2,  6, -3,\n",
            "         1, -5,  1,  5, -1,  2,  1,  5,  6, -2,  1, -1,  3, -2,  0,  3,  5, -2,\n",
            "         2,  3,  4, -5, -4,  1, -8,  1,  7, -5, -1, -7, -4,  4, -2, -4,  3, -6,\n",
            "        -1, -2,  1,  1,  1,  2, -5, -2, -5,  0,  8,  0, -3,  2, -7, -7,  0,  0,\n",
            "        -2,  5])\n",
            "tensor([[[-0.5994, -0.3596, -0.5480,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 0.0514,  0.3425, -0.0514,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.2569, -0.0856, -0.6336,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.0171, -0.2569, -0.3254,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.5651, -0.0342, -0.1199,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-0.1027, -0.1027, -0.1884,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-0.4202, -0.1751, -0.2801,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 0.2276,  0.5952,  0.3676,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.0000,  0.2976, -0.0350,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.5952,  0.5252,  0.2976,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 1.2605,  0.8228,  0.6127,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.4202,  0.5252,  0.4202,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.6797, -0.4009, -0.4357,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.0000,  0.3660,  0.2789,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.1394,  0.2266,  0.1046,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.7255,  2.4575,  2.5098,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 1.8824,  2.1089,  2.4924,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.8192,  1.0458,  1.2898,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -7,  7,  7, -8,  5, -2, -7, -8,  5, -1, -2,  1, -1, -4,  1,  8,  0,\n",
            "         5,  0, -3,  4,  6,  8,  0, -2, -7,  7,  4,  3,  2, -3,  3,  0,  0,  2,\n",
            "        -5,  6, -6,  1, -3, -3,  1, -1, -3,  5, -4,  8,  2, -2, -6, -4, -2, -5,\n",
            "        -1, -1,  3,  2,  5, -5,  7, -8,  0,  5, -2, -1,  0,  1,  4,  2,  3, -2,\n",
            "         3, -8, -3, -3, -3,  0, -4, -3,  8, -4, -5, -9,  1, -3, -1,  0,  4, -2,\n",
            "        -3,  4, -1, -2, -2, -1, -7,  0,  6, -4,  0, -7, -4,  3, -7, -2, -1, -3,\n",
            "        -3, -2,  4, -7,  3,  2, -2, -2, -1, -1,  1,  5, -6, -6, -4, -6,  0,  2,\n",
            "        -1, -3])\n",
            "tensor([[[ 3.2880,  3.5619,  3.4763,  ...,  1.6269,  1.7638, -2.5345],\n",
            "         [ 3.7161,  3.7161,  3.2023,  ...,  2.2433,  2.3290, -2.0892],\n",
            "         [ 2.6372,  2.9968,  2.3461,  ...,  2.3118,  2.3118, -1.8666],\n",
            "         ...,\n",
            "         [ 0.7877,  0.9076,  0.9419,  ...,  1.5926,  3.3907, -0.9076],\n",
            "         [ 1.2844,  1.4042,  1.1816,  ...,  1.1987,  2.7057, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 3.3789,  3.6590,  3.5889,  ...,  2.1183,  2.2759, -2.1709],\n",
            "         [ 3.5189,  3.5539,  3.0987,  ...,  2.9062,  3.0112, -1.5231],\n",
            "         [ 2.0833,  2.3459,  1.8382,  ...,  2.9587,  2.9937, -1.2780],\n",
            "         ...,\n",
            "         [ 1.4706,  1.7682,  1.6106,  ...,  2.1008,  3.9041, -0.5952],\n",
            "         [ 2.0308,  2.3109,  1.9433,  ...,  1.8557,  3.3789, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 3.2244,  3.6253,  3.6078,  ...,  2.5272,  2.6667, -1.7952],\n",
            "         [ 3.2767,  3.0675,  2.5098,  ...,  3.4684,  3.5730, -0.9935],\n",
            "         [ 0.8715,  0.8192,  0.3660,  ...,  3.5556,  3.5730, -0.7320],\n",
            "         ...,\n",
            "         [ 2.7538,  3.8344,  3.9564,  ...,  3.1895,  4.3050, -0.3486],\n",
            "         [ 2.7712,  3.7124,  3.9216,  ...,  2.7887,  3.8519, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -7,  8, -2, -4,  1, -1, -4, -7, -2,  4,  2,  7, -2, -2,  8,  0, -2,\n",
            "         3,  2,  1,  3,  0,  6,  2, -6, -4, -2, -2,  5,  8, -8,  2, -2,  1,  3,\n",
            "        -4,  6, -2,  1,  0,  5,  2,  3, -9,  1, -3,  6,  4,  3, -2, -4, -6, -5,\n",
            "         3, -2,  7, -3,  5, -6,  4, -9, -4,  6,  0,  2, -3,  3,  4,  3, -1,  5,\n",
            "         2, -3,  0,  6, -5,  0, -5,  0,  3,  5, -1, -5,  4, -3, -2,  4,  5, -6,\n",
            "         4, -4,  6, -9, -5, -8,  0,  2,  0,  2, -3, -6,  3,  7, -3, -1,  8, -4,\n",
            "        -7,  7,  8,  1, -5,  3, -3, -4, -4, -2,  0, -1, -5, -2, -8, -4, -2,  3,\n",
            "        -7,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -1.4556, -1.2672, -1.2844],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.7535, -0.5480, -0.5994],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.6165, -0.6336, -0.6507],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.5137,  2.7571,  2.9968],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.0685,  1.8666,  2.2776],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -0.4452,  0.7706,  1.1987]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.3852, -0.1401, -0.1926],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.4902,  0.7353,  0.7003],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.5777,  0.6303,  0.6478],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  1.2955,  3.5014,  3.5889],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  0.9104,  2.7836,  3.0462],\n",
            "         [-2.5210, -2.2584, -2.4860,  ...,  0.4202,  1.6982,  2.0483]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  0.2614,  0.4706,  0.3660],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.2898,  1.5338,  1.4292],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.3943,  1.4466,  1.3769],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  2.6492,  4.0784,  3.9564],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  2.0566,  3.4510,  3.5556],\n",
            "         [-2.0218, -1.6383, -1.5163,  ...,  1.3595,  2.4749,  2.6144]]])\n",
            "tensor([ 2, -2,  1,  2, -7, -3, -1, -1, -6,  2, -3, -3,  5,  0, -2,  7,  5, -4,\n",
            "        -2,  0,  1, -1,  0,  3,  1, -4, -7,  0, -5,  4,  3, -3,  6, -4, -5, -2,\n",
            "         1,  1, -6,  0, -2,  4,  1,  1, -8,  3,  1,  0,  0, -2,  1, -4, -1, -1,\n",
            "         3, -1,  1, -2,  1, -3,  4, -8, -4,  8, -2, -4, -6,  1,  6,  4,  4,  6,\n",
            "         0, -8,  3,  1, -6, -1, -3,  5,  2,  0, -5, -9,  6, -4,  2, -3,  8, -2,\n",
            "        -2, -2,  3, -7, -2, -1, -7, -2,  6,  4, -1,  0, -1,  3, -5,  1,  0,  0,\n",
            "         1,  3,  1, -1, -6,  3,  1,  2,  1, -6,  7, -1, -1, -6,  0, -1, -6,  5,\n",
            "        -7, -2])\n",
            "tensor([[[-1.0104, -0.7364,  2.3290,  ...,  0.8905,  1.0446,  1.1131],\n",
            "         [-0.2740,  0.0000,  2.9112,  ...,  1.4899,  1.6097,  1.5584],\n",
            "         [-0.4281, -0.2740,  2.4317,  ...,  1.6440,  1.6269,  1.8323],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.7810,  ..., -0.6850,  1.1131,  1.1645],\n",
            "         [-3.0825, -2.9626, -1.6954,  ..., -1.2159,  0.2911,  0.5309],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053,  2.9762,  ...,  1.6982,  1.8207,  1.8382],\n",
            "         [-0.3501,  0.0000,  3.6765,  ...,  2.4685,  2.5560,  2.4860],\n",
            "         [-0.4202, -0.1225,  3.3789,  ...,  2.5910,  2.5910,  2.7836],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -0.4727,  ...,  0.3501,  2.1359,  2.0833],\n",
            "         [-2.4335, -2.1534, -0.2976,  ..., -0.0700,  1.4706,  1.6282],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843,  3.6950,  ...,  2.5621,  2.6492,  2.6318],\n",
            "         [-0.3486,  0.0000,  4.3747,  ...,  3.4336,  3.5207,  3.3987],\n",
            "         [-0.3660,  0.0000,  4.2004,  ...,  3.5556,  3.5556,  3.6950],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  2.0392,  ...,  1.6558,  2.7712,  2.5447],\n",
            "         [-1.6732, -0.7320,  1.8301,  ...,  1.0806,  2.1786,  2.1786],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 2, -1, -1,  7,  1, -2, -5, -7, -2, -1, -3, -2,  6, -2, -1,  6,  1,  3,\n",
            "         2,  2, -3,  7,  7,  6,  3, -6,  1,  6,  1, -3,  0,  1,  8,  1, -3, -4,\n",
            "        -3,  4, -9,  2,  6,  3, -5,  0, -9,  3,  1,  0,  4,  3, -8, -1,  0, -7,\n",
            "         3, -6,  2,  1,  6, -2,  7, -8,  0,  2,  1,  2, -4,  4,  2,  3,  5,  2,\n",
            "         7, -6,  4, -3, -4, -4,  0, -3,  3,  2, -3, -6,  2,  1,  1,  0,  6,  0,\n",
            "         3, -4,  4, -6, -4, -7, -4,  7,  5,  3, -3, -8,  2,  4, -4, -4,  3, -2,\n",
            "        -1,  3,  3, -8, -1,  3, -4,  2, -5,  3,  3,  6, -6, -5, -5,  0, -2, -1,\n",
            "         1,  4])\n",
            "tensor([[[-1.0104,  2.1063,  2.1748,  ..., -0.0514, -0.1199, -0.1027],\n",
            "         [-0.2740,  2.9283,  2.7742,  ...,  0.6850,  0.7364,  0.4966],\n",
            "         [-0.4281,  2.6030,  2.1577,  ...,  0.8905,  0.9076,  0.8734],\n",
            "         ...,\n",
            "         [-3.5619, -1.0789, -1.0275,  ...,  0.2569,  1.4556,  1.4385],\n",
            "         [-3.0825, -1.4042, -1.6611,  ..., -1.3700, -0.3254, -0.1370],\n",
            "         [-3.0311, -1.7981, -2.0036,  ..., -3.0996, -2.0892, -1.6611]],\n",
            "\n",
            "        [[-1.0854,  2.4510,  2.5210,  ...,  0.8228,  0.8053,  0.7878],\n",
            "         [-0.3501,  3.3088,  3.2563,  ...,  1.7157,  1.7857,  1.5581],\n",
            "         [-0.4202,  3.1338,  2.8186,  ...,  1.9083,  1.9258,  1.9433],\n",
            "         ...,\n",
            "         [-2.9762,  0.0525, -0.0700,  ...,  0.7878,  2.2234,  2.1183],\n",
            "         [-2.4335, -0.1926, -0.5952,  ..., -0.5427,  0.6828,  0.8053],\n",
            "         [-2.5210, -0.7703, -1.0154,  ..., -2.2059, -1.0504, -0.6478]],\n",
            "\n",
            "        [[-1.0980,  2.9978,  3.0675,  ...,  1.8475,  1.8649,  1.7778],\n",
            "         [-0.3486,  3.8344,  3.8519,  ...,  2.8758,  2.9455,  2.6841],\n",
            "         [-0.3660,  3.7647,  3.5904,  ...,  3.0675,  3.0675,  3.0675],\n",
            "         ...,\n",
            "         [-1.6732,  2.4749,  2.6144,  ...,  1.9869,  2.9281,  2.6667],\n",
            "         [-1.6732,  1.6035,  1.7603,  ...,  0.6100,  1.5338,  1.5163],\n",
            "         [-2.0218,  0.2614,  0.3486,  ..., -1.0980, -0.0174,  0.1569]]])\n",
            "tensor([ 2, -9,  1,  7, -8,  1,  0, -3, -2,  2, -3,  2, -1,  1,  0,  4,  8,  5,\n",
            "         0,  5, -3,  3,  7,  1,  4, -2, -4,  2, -2,  5,  3, -6,  0,  2, -5,  4,\n",
            "        -6,  3,  0,  3,  0,  2, -3, -4, -5,  2, -4,  5, -1,  5, -2, -3,  0, -5,\n",
            "         3, -5,  8, -4,  9,  0,  7, -8, -6,  5, -4, -3,  3,  5,  6,  2,  1,  2,\n",
            "         2, -5,  4,  1, -2, -5,  3, -2,  5,  3, -2, -6, -1, -2,  2, -2,  4, -2,\n",
            "        -3,  3,  8, -8, -7, -8, -4, -2,  1,  2, -4,  0,  1,  2, -9, -2,  1, -5,\n",
            "        -2,  1,  7,  1,  3,  8,  2, -6, -2, -6, -1,  7, -7, -3, -1,  1, -4,  1,\n",
            "         1,  0])\n",
            "tensor([[[ 0.3425,  0.3082,  0.1884,  ..., -1.4385, -2.6030, -2.5345],\n",
            "         [ 1.0275,  1.1302,  0.5480,  ..., -1.3015, -2.0378, -2.0892],\n",
            "         [ 0.8220,  0.8391, -0.2397,  ..., -0.1199, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.3501,  0.1926,  0.1225,  ..., -1.4006, -2.1884, -2.1709],\n",
            "         [ 1.0329,  1.1380,  0.6303,  ..., -0.8929, -1.4531, -1.5231],\n",
            "         [ 0.9629,  1.0504,  0.1050,  ..., -0.1225, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.4183, -0.1917, -0.2092,  ..., -1.2549, -1.7778, -1.7952],\n",
            "         [ 0.3834,  0.6797,  0.5926,  ..., -0.4532, -0.8715, -0.9935],\n",
            "         [ 0.2614,  0.6100,  0.3486,  ..., -0.0174, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -2,  8,  1, -7, -4, -7, -3, -5,  7,  2,  1,  5,  6, -2,  8,  2, -1,\n",
            "         1, -2,  0, -1,  6,  8,  5, -8,  2,  5, -4, -1,  5, -5,  5,  2, -3, -5,\n",
            "        -4,  6,  0,  3,  6, -4, -6,  4,  0,  0, -7,  0, -1,  2,  1, -6,  1, -4,\n",
            "         7, -1,  3, -1,  5, -7, -2, -2, -6,  2,  4, -2, -2,  4,  0,  4,  3,  0,\n",
            "         3, -9,  4,  3, -4, -5,  3, -1,  0,  0, -1, -5,  4, -6, -6,  2,  6, -4,\n",
            "        -4, -4,  4, -8, -5, -2, -8,  5,  4, -4,  0, -8, -6,  7, -8, -6,  0, -5,\n",
            "        -2, -2,  6, -8,  0,  1, -4, -6, -4,  3,  4,  1, -3,  1,  1, -8, -5,  0,\n",
            "        -2, -3])\n",
            "tensor([[[-1.0104,  1.3357,  1.0275,  ..., -2.2605, -2.1063, -1.8323],\n",
            "         [-0.2740,  1.6269,  1.7638,  ..., -1.6954, -1.7125, -1.6440],\n",
            "         [-0.4281,  1.1645,  1.6269,  ..., -1.7810, -1.8495, -1.6611],\n",
            "         ...,\n",
            "         [-3.5619,  0.5651,  0.5994,  ...,  0.0514,  1.6097,  1.9008],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  0.8228,  0.5427,  ..., -1.9958, -1.7507, -1.5056],\n",
            "         [-0.3501,  1.1905,  1.4706,  ..., -1.2255, -1.1730, -1.1029],\n",
            "         [-0.4202,  1.0154,  1.5931,  ..., -1.2955, -1.3130, -1.1204],\n",
            "         ...,\n",
            "         [-2.9762,  1.2080,  1.0504,  ...,  0.2101,  1.6282,  1.6632],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  0.3312,  0.1569,  ..., -1.6035, -1.3769, -1.1852],\n",
            "         [-0.3486,  0.7843,  1.2200,  ..., -0.6623, -0.6100, -0.6275],\n",
            "         [-0.3660,  0.7669,  1.4815,  ..., -0.6972, -0.6972, -0.5926],\n",
            "         ...,\n",
            "         [-1.6732,  1.7429,  1.8126,  ...,  0.1220,  1.1155,  1.1503],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -6,  0, -1,  0,  4, -6, -1, -2,  3, -1,  1,  1, -2,  3,  6,  7,  0,\n",
            "        -2,  4,  1,  6,  8,  8, -2, -6, -7,  4,  3, -4,  4, -8,  7, -2, -6, -4,\n",
            "        -6,  6, -2, -1,  6,  0, -5,  3,  0,  4, -3,  6,  5,  0,  0, -7, -6, -1,\n",
            "         2, -9,  3,  0,  7, -1,  4, -4, -4,  5, -1,  0, -6,  1, -1,  4,  4,  3,\n",
            "         7, -3,  0,  4, -1, -4,  0,  2,  4,  1, -5, -6,  3, -5, -2,  5, -1,  2,\n",
            "         1, -3,  3, -5, -2,  0, -2, -1, -1,  0, -6, -9,  2,  1, -2, -4,  8, -5,\n",
            "        -4,  6,  5, -5, -4,  4,  0, -1,  2, -5,  0,  8, -3, -4, -2,  1,  0,  7,\n",
            "         1, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.3529, -0.8734, -0.6336,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-1.5241, -1.2159, -1.0617,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-2.0892, -2.1235, -2.0550,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.2955, -0.6127, -0.5427,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-1.2080, -0.7353, -0.7003,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-1.8207, -1.7332, -1.6982,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.1569,  1.2723,  1.5512,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-0.4357,  0.5926,  1.0980,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-1.3769, -1.2200, -0.9237,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -2,  2,  5, -8,  0,  1, -8, -7,  1,  1, -3,  3,  4,  1,  8,  5,  2,\n",
            "         6,  2, -2,  5,  8,  4,  1, -3, -3,  1, -5, -3,  1, -4,  8, -3, -2,  3,\n",
            "         2,  5, -6,  5, -3,  3, -2, -2, -6, -1, -1, -1,  2,  0,  0, -4, -6,  0,\n",
            "        -1,  0,  3, -7,  3, -2,  6, -7, -3,  1,  4, -4,  0,  8,  4,  3,  7, -1,\n",
            "        -1, -9,  4, -3, -1, -4, -5,  5,  2,  1, -5, -6,  7, -4,  0, -2,  3,  2,\n",
            "         4, -3,  1, -2, -2, -1, -7,  6,  4, -1, -6, -7, -6,  0, -9, -5,  0, -3,\n",
            "        -6,  3,  6,  1, -5,  7, -1, -6,  2,  1,  4,  7, -2, -3, -3, -6,  2,  8,\n",
            "        -3, -1])\n",
            "tensor([[[ 0.2397,  0.1884, -0.0685,  ...,  0.0856, -2.6030, -2.5345],\n",
            "         [ 1.0446,  1.2159,  0.7535,  ..., -0.0342, -2.0378, -2.0892],\n",
            "         [ 0.9761,  0.9419,  0.4624,  ..., -0.5480, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.0875,  0.1050, -0.0350,  ...,  0.2101, -2.1884, -2.1709],\n",
            "         [ 0.8053,  1.1204,  0.8754,  ...,  0.2101, -1.4531, -1.5231],\n",
            "         [ 0.8053,  0.9279,  0.6828,  ..., -0.2976, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.2092, -0.0697, -0.2092,  ...,  0.2963, -1.7778, -1.7952],\n",
            "         [ 0.5229,  0.8715,  0.6623,  ...,  0.5403, -0.8715, -0.9935],\n",
            "         [ 0.5229,  0.7320,  0.6449,  ...,  0.1917, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -6,  1, -1, -2,  2,  2, -4, -8,  3, -4, -1,  7, -1,  0,  3,  7,  5,\n",
            "         4, -3,  2, -1,  8,  0,  5, -9, -5,  0, -5,  0,  7, -8,  7,  4,  2,  0,\n",
            "         3,  1, -9,  0,  2,  5, -3, -2, -7,  2, -1,  4,  0,  4, -1, -2, -6, -1,\n",
            "         5, -9,  4, -5,  7, -3,  7, -5, -1,  4, -1, -3, -6,  3,  1,  0,  2, -2,\n",
            "         4, -9,  4,  0,  0, -4,  2,  4,  6,  1, -4,  0,  6, -8, -4, -3,  6, -3,\n",
            "        -2, -5,  4, -1, -6, -4, -1, -2, -2,  2,  3, -2, -2,  6,  0, -4,  7, -8,\n",
            "         0,  4,  2, -4,  0,  1, -4, -2, -3, -3,  6,  5, -1,  2,  0, -2, -3,  0,\n",
            "         0,  4])\n",
            "tensor([[[ 0.9419,  0.9247,  0.1370,  ..., -1.4385, -2.6030, -2.5345],\n",
            "         [ 2.5687,  1.6782,  0.9247,  ..., -0.8049, -2.0378, -2.0892],\n",
            "         [ 0.6336,  0.2911,  0.1884,  ..., -0.0171, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.2605, -1.8666, -2.3632,  ..., -1.7467, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.2955,  1.1204,  0.3326,  ..., -0.3326, -2.1884, -2.1709],\n",
            "         [ 2.8887,  1.6807,  0.9629,  ...,  0.4727, -1.4531, -1.5231],\n",
            "         [ 0.8929,  0.4552,  0.5077,  ...,  1.3480, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.2255, -0.4902, -1.5756,  ..., -1.4356, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.2614,  0.3486, -0.2614,  ..., -1.1329, -1.7778, -1.7952],\n",
            "         [ 1.7255,  1.0980,  0.5926,  ...,  0.0000, -0.8715, -0.9935],\n",
            "         [ 0.2266,  0.1917,  0.3834,  ...,  0.6972, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.7669,  0.6100,  0.3137,  ..., -0.4706, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0,  0,  3, -1,  1, -1,  0, -8, -9,  1, -2, -4,  6, -1, -3,  1,  4,  1,\n",
            "        -1, -1, -4, -2,  6,  6,  4,  0,  1,  2, -5,  4,  7, -1,  1, -4, -5,  1,\n",
            "        -6,  4, -1, -3,  3,  0, -3,  5, -2,  5, -6,  3,  0,  3,  0, -1, -6, -2,\n",
            "         7, -3,  5,  2,  0, -4,  2, -4, -1, -1,  0, -3,  2,  0,  7, -1,  7,  3,\n",
            "         1, -4, -1,  1, -7,  2,  0,  1,  8, -3,  1, -7,  2,  0,  1, -3,  8, -2,\n",
            "         4, -1,  7, -2, -3, -7, -7,  1, -1,  0, -1, -3,  0,  3, -2, -5,  5, -9,\n",
            "        -3,  7,  1, -7, -5,  0, -3,  1, -4, -5,  1,  5, -3, -1, -3, -8,  3,  3,\n",
            "        -7,  3])\n",
            "tensor([[[ 2.0550,  1.7981,  1.1131,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.7913,  3.1852,  2.6543,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.6543,  3.2537,  2.6030,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.4706,  1.2780,  0.6653,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.3109,  2.7661,  2.2584,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.3459,  3.0462,  2.3459,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.1678,  1.0806,  0.5926,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.9869,  2.4924,  2.1438,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.0392,  2.8235,  2.3704,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1,  0,  7,  2, -7, -3, -3, -3, -4,  4, -1, -2,  3, -3,  2,  3,  6,  0,\n",
            "         2,  6, -5,  0,  5,  5,  1, -5,  1,  7, -1, -2, -1, -5, -1,  1, -2,  2,\n",
            "        -6,  0, -5,  5, -3, -1, -4,  5, -1,  2, -6,  2,  6,  4, -7, -8, -5,  0,\n",
            "         4, -9, -1, -7,  8, -4, -2, -7,  1,  2, -5,  2, -1,  3,  6, -2,  0,  5,\n",
            "         3, -7,  1,  4,  0, -5, -3,  4,  6, -3, -4, -7,  0,  1,  0, -2,  1, -1,\n",
            "         3,  3,  6, -9,  0,  0, -6,  3,  2, -2,  3, -7,  3, -1, -1,  2,  7, -3,\n",
            "        -7,  6,  8, -2, -3,  6,  3, -3, -6, -6,  1,  1, -8, -1, -3, -4, -4,  7,\n",
            "        -5,  4])\n",
            "tensor([[[ 0.7535,  1.5926,  1.8837,  ..., -0.7021, -2.6030, -2.5345],\n",
            "         [ 1.4727,  2.2605,  2.4317,  ..., -0.1370, -2.0378, -2.0892],\n",
            "         [ 1.3015,  1.9865,  1.9351,  ..., -0.0514, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.3803, -1.9693, -1.2672,  ..., -0.9076, -0.9590, -0.9076],\n",
            "         [-1.8323, -1.5926, -1.5412,  ..., -2.3803, -1.6611, -1.4214],\n",
            "         [-1.5926, -1.2844, -1.5070,  ..., -2.9968, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.2976,  1.2430,  1.5406,  ..., -1.2255, -2.1884, -2.1709],\n",
            "         [ 0.9804,  1.9608,  2.2059,  ..., -0.4727, -1.4531, -1.5231],\n",
            "         [ 0.9279,  1.8732,  1.9608,  ..., -0.4027, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.9258, -1.3480, -0.8228,  ..., -0.5602, -0.5427, -0.5952],\n",
            "         [-1.3305, -0.9629, -0.9979,  ..., -1.9783, -1.0854, -0.9279],\n",
            "         [-1.2430, -0.8403, -1.0329,  ..., -2.7311, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.2789,  1.1678,  1.5163,  ..., -1.0632, -1.7778, -1.7952],\n",
            "         [ 0.9237,  1.8126,  2.1786,  ..., -0.1394, -0.8715, -0.9935],\n",
            "         [ 0.9063,  1.8475,  2.1089,  ..., -0.0349, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.5752,  0.8017,  1.5861,  ...,  0.6275, -0.1220, -0.3486],\n",
            "         [-0.5229,  0.5229,  1.0458,  ..., -0.9237, -0.5926, -0.5926],\n",
            "         [-0.6972, -0.1569, -0.0349,  ..., -1.8475, -1.4641, -1.2549]]])\n",
            "tensor([-5, -8,  7,  3,  1,  3, -1, -8, -6,  3,  3, -5,  6,  3, -3,  1,  7,  1,\n",
            "         2,  3,  4,  4,  0,  6,  4, -9,  2,  1, -5,  4,  2, -4,  7,  5, -6,  4,\n",
            "        -4, -1,  0,  5,  6,  3, -1, -2, -4,  1, -1,  0, -1,  2, -8, -7,  1, -6,\n",
            "        -1, -7,  3, -6,  5, -3,  4, -6, -5,  2,  0,  1,  2,  0,  3,  0,  2, -1,\n",
            "        -1, -7,  3,  6, -2, -6,  2,  2,  6,  2,  2, -1,  6, -4,  2, -2,  1, -1,\n",
            "         1,  4,  3, -7, -4, -7, -8, -1, -1,  4, -6, -2, -1,  2,  0,  0,  2, -6,\n",
            "        -5,  2,  1, -3,  3,  6,  1, -5,  3, -6,  1,  7, -3,  3, -2, -1,  2,  3,\n",
            "        -8,  0])\n",
            "tensor([[[-0.0856,  0.1884,  0.1027,  ..., -1.7638, -2.6030, -2.5345],\n",
            "         [ 0.7192,  0.9932,  0.7021,  ..., -1.0960, -2.0378, -2.0892],\n",
            "         [ 0.5994,  0.7535,  0.2055,  ..., -0.9761, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.4556, -1.3529, -1.2501,  ..., -0.8734, -0.9590, -0.9076],\n",
            "         [-1.0789, -0.8734, -1.0789,  ..., -1.1645, -1.6611, -1.4214],\n",
            "         [-0.9247, -0.8049, -0.9590,  ..., -1.8323, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.9454,  1.2255,  1.1905,  ..., -0.3151, -2.1884, -2.1709],\n",
            "         [ 1.7507,  2.1008,  1.9608,  ...,  0.5427, -1.4531, -1.5231],\n",
            "         [ 1.7157,  2.0133,  1.6632,  ...,  0.6478, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.2255, -0.9454, -1.0504,  ..., -0.7878, -0.5427, -0.5952],\n",
            "         [-0.7878, -0.4377, -0.8053,  ..., -0.9279, -1.0854, -0.9279],\n",
            "         [-0.7703, -0.5427, -0.7528,  ..., -1.6632, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.2135,  2.5098,  2.5621,  ...,  1.3943, -1.7778, -1.7952],\n",
            "         [ 3.0327,  3.3813,  3.3638,  ...,  2.3878, -0.8715, -0.9935],\n",
            "         [ 3.0153,  3.3813,  3.2418,  ...,  2.4924, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.4880,  0.5752,  0.7669,  ..., -0.2266, -0.1220, -0.3486],\n",
            "         [-0.5926,  0.4183,  0.6449,  ..., -0.5229, -0.5926, -0.5926],\n",
            "         [-0.8366, -0.4706, -0.3312,  ..., -1.4292, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -7,  6, -1, -8,  1, -6, -1, -8,  4,  1,  3,  5, -3,  0,  5,  1, -1,\n",
            "         2,  2,  1, -2,  1,  0, -1, -4,  1,  3, -3,  0,  1, -6,  4, -1, -7, -2,\n",
            "        -6, -1,  0, -2, -2,  3, -4,  4, -4,  2, -3,  8,  5,  1, -2, -5, -2, -3,\n",
            "         3, -8,  0, -2,  6, -7,  3, -9,  2,  5, -3, -2,  0,  7,  7,  4,  6,  1,\n",
            "        -2, -5, -4,  2, -3, -4, -5, -1, -1,  5, -1, -2,  7, -7, -6,  4,  7,  1,\n",
            "        -1, -5, -1, -5, -2,  1, -2, -2,  7,  4,  3, -7, -5,  0, -2, -2,  2, -2,\n",
            "        -3,  3,  7,  1, -3,  5,  3,  0, -1,  1,  0,  6, -8, -1, -2, -1, -3, -1,\n",
            "        -4,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.7877, -0.8734, -1.1302,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-0.1370, -0.1199, -0.4966,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.0171,  0.1199, -0.1541,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.2801, -0.1576, -0.6127,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.4202,  0.6303,  0.1050,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.4202,  0.6478,  0.3501,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.0980,  1.9869,  1.8126,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 1.2375,  2.1089,  2.1612,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.9760,  1.3420,  1.3769,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -8,  7,  6, -2, -1, -5,  0, -2,  6, -3, -3,  7,  5,  2,  9,  1,  0,\n",
            "         1, -2, -1,  0,  3,  4,  0, -9, -6,  3, -4, -2,  0, -6, -1, -2, -3, -1,\n",
            "        -5, -2, -7,  6,  2,  2, -3, -2, -1,  5, -6,  6,  3, -4, -6, -3, -1,  0,\n",
            "         6,  0,  6,  0,  3, -9,  2, -8, -3, -1, -5,  4,  1,  4,  8,  6,  6,  2,\n",
            "        -2, -8, -1,  2, -1,  3, -3, -2,  2,  0, -1, -5,  5, -7,  1, -3,  6, -7,\n",
            "         3,  3,  3, -4, -3,  0, -8,  1,  2, -1, -1,  0, -1,  2, -7, -5,  0, -7,\n",
            "        -7,  0,  3, -1,  2, -1,  2, -1, -6, -4,  3,  8,  0,  2, -1, -8, -3,  1,\n",
            "        -2,  4])\n",
            "tensor([[[ 3.0825,  3.3736,  3.2537,  ...,  1.6611, -2.6030, -2.5345],\n",
            "         [ 2.8598,  3.2023,  2.9797,  ...,  1.9693, -2.0378, -2.0892],\n",
            "         [ 2.9626,  3.1510,  2.7400,  ...,  2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.1541,  0.3082,  0.6679,  ...,  1.2844, -0.9590, -0.9076],\n",
            "         [-0.0171,  0.4452,  0.3254,  ...,  0.9076, -1.6611, -1.4214],\n",
            "         [ 0.9761,  1.1474,  0.9761,  ...,  0.6165, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 3.0987,  3.3964,  3.3613,  ...,  2.1534, -2.1884, -2.1709],\n",
            "         [ 2.5735,  3.0112,  2.9587,  ...,  2.5910, -1.4531, -1.5231],\n",
            "         [ 2.6611,  3.0112,  2.8186,  ...,  2.6261, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.3326,  1.1029,  1.2955,  ...,  1.7682, -0.5427, -0.5952],\n",
            "         [ 0.5602,  1.2605,  0.9804,  ...,  1.5406, -1.0854, -0.9279],\n",
            "         [ 1.5581,  1.8557,  1.6457,  ...,  1.1905, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 3.0327,  3.3813,  3.4161,  ...,  2.5447, -1.7778, -1.7952],\n",
            "         [ 1.9172,  2.3529,  2.4401,  ...,  3.1198, -0.8715, -0.9935],\n",
            "         [ 1.6906,  2.1089,  2.1264,  ...,  3.1547, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.0806,  2.6492,  3.0850,  ...,  2.7887, -0.1220, -0.3486],\n",
            "         [ 0.9063,  2.1961,  2.5098,  ...,  2.4401, -0.5926, -0.5926],\n",
            "         [ 2.0218,  2.4052,  2.5447,  ...,  1.9521, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -4,  2,  1, -1,  0, -2,  0, -8,  2, -1, -3,  5,  2,  2,  9,  1,  0,\n",
            "         1, -3,  0,  6,  3,  4,  7, -6, -6,  7,  4, -2,  7, -2,  6,  5, -2, -3,\n",
            "        -1,  5, -8,  2, -3, -2, -7,  5, -1,  3, -1, -1,  3,  3, -5, -4,  2, -5,\n",
            "         4, -7,  8, -2,  6, -4,  0, -2,  2,  0, -5,  3, -4,  1,  8,  0,  7, -2,\n",
            "        -2, -2,  2,  5,  2, -2, -1, -4,  8,  3, -5, -2,  8, -1,  0,  5,  2, -3,\n",
            "        -4,  1, -1, -9, -2, -1, -5,  6,  5, -1, -2, -7,  2,  0, -9, -1,  3, -1,\n",
            "         0, -1,  1, -1,  2,  3, -2, -5, -3, -4,  1,  6, -3, -5, -7, -5,  2,  2,\n",
            "         1, -3])\n",
            "tensor([[[ 1.5070,  1.7125,  1.6782,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.2262,  2.5173,  2.1920,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.0892,  2.2776,  1.6782,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.2397, -0.1199, -0.1541,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.4452,  0.5651,  0.3082,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.6457,  1.8382,  1.8908,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.3634,  2.7136,  2.5560,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.3284,  2.6436,  2.2409,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.1751,  0.4727,  0.2626,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.8403,  1.1380,  0.7528,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.6797,  0.9412,  1.0980,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.3769,  1.7603,  1.7778,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.3420,  1.7603,  1.6209,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.7146,  1.7778,  1.7952,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 0.9063,  1.8301,  1.9695,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2,  0,  1, -2, -4,  2, -5, -8, -1,  3,  0, -4,  1,  2, -2,  3,  1,  2,\n",
            "         2,  5,  2,  4,  0, -1,  6, -8, -4,  4, -4, -2,  5, -7,  5, -4,  0, -1,\n",
            "        -1,  1, -4,  6,  5, -2, -2,  0, -1,  6,  0,  1,  5, -2,  0, -1, -5, -8,\n",
            "         2, -4, -1, -7,  4, -4,  0,  0,  0,  3, -1,  4, -5,  1,  1,  6,  0, -3,\n",
            "         4, -9,  2, -3, -2, -3, -3,  1,  7, -3,  3, -5,  6, -1,  1,  1,  6, -6,\n",
            "         1,  4,  6, -1,  0, -4, -7,  2,  5,  1, -6,  0,  3,  4, -2,  0,  0, -6,\n",
            "        -6,  1,  3, -8,  0,  7,  3, -2,  3,  1,  0,  5, -6, -2, -5, -5,  0,  3,\n",
            "        -3, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.9761,  1.1302,  0.6679,  ..., -0.3425, -0.2569, -1.8666],\n",
            "         ...,\n",
            "         [-2.1063, -1.9008, -1.8323,  ..., -1.1816,  0.7535, -0.9076],\n",
            "         [-1.6440, -1.5755, -1.6269,  ..., -1.3015,  0.1884, -1.4214],\n",
            "         [-1.5755, -1.4214, -1.4385,  ..., -1.8152, -0.6336, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.4181,  1.7157,  1.4881,  ...,  0.6303,  0.7703, -1.2780],\n",
            "         ...,\n",
            "         [-1.1029, -0.7178, -0.8578,  ..., -0.3852,  1.5756, -0.5952],\n",
            "         [-0.5777, -0.3501, -0.5777,  ..., -0.3501,  1.1555, -0.9279],\n",
            "         [-0.6828, -0.4202, -0.5252,  ..., -0.9454,  0.2801, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.4357,  0.8017,  0.8540,  ...,  0.0871,  0.1569, -0.7320],\n",
            "         ...,\n",
            "         [-1.0632,  0.1046,  0.4183,  ..., -0.5229,  0.7320, -0.3486],\n",
            "         [-1.0283, -0.1569,  0.2266,  ..., -0.6449,  0.3834, -0.5926],\n",
            "         [-1.3072, -0.9586, -0.8017,  ..., -1.4118, -0.3660, -1.2549]]])\n",
            "tensor([ 1, -5,  5, -2, -6, -4,  2, -2, -5,  6, -2,  0, -2, -3, -1,  8,  5, -4,\n",
            "         3,  0, -3, -1,  3,  8,  2, -2, -2,  7,  4,  2,  4,  1,  2,  2, -1, -1,\n",
            "        -4,  2, -5, -1, -1,  4, -4, -4, -9,  7, -4,  7,  7, -3, -3, -9,  0, -8,\n",
            "        -1, -1,  8,  2,  1, -2, -2, -6, -4,  3,  4,  1,  3,  6,  7, -1,  4, -1,\n",
            "         6, -7,  1, -3, -4, -4,  2,  3,  1, -3, -2,  0,  0, -4,  2,  4,  1, -3,\n",
            "         2, -2,  4, -3, -1, -5, -6,  3,  0,  3,  3, -8, -4, -1, -8,  3,  4, -9,\n",
            "        -6,  3,  9, -3, -3,  1,  0, -3, -3, -6, -1,  4, -7, -1, -7, -4, -5,  8,\n",
            "        -3,  0])\n",
            "tensor([[[-0.9932, -0.7192, -0.8391,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2397,  0.0342, -0.2911,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.3939, -0.2397, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0679, -0.7878, -0.8228,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0175, -0.1050,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4027, -0.1050, -0.4552,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0806, -0.7669, -0.7320,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3137,  0.0174,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3486,  0.0000, -0.0871,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -6,  8,  3, -2,  0, -5, -3,  0,  7, -4, -4,  3,  6,  0,  5,  8, -1,\n",
            "        -2,  3, -3,  4,  4, -1, -2, -1, -5, -1,  1, -4,  0, -6,  0, -1, -4,  3,\n",
            "        -3,  2,  0,  3,  4,  3,  1,  1,  0,  0, -3,  1,  0, -4,  0,  0, -3,  1,\n",
            "         1,  0,  8, -4,  2, -6,  7, -2, -5,  0, -2,  5, -4,  1,  7,  1,  3, -1,\n",
            "        -1, -4, -1, -1,  2,  1,  1,  0,  5,  0, -2, -2,  0, -1, -4,  5,  7, -4,\n",
            "        -3, -5,  5, -3, -8, -4, -2,  1,  0,  0, -6, -8, -2,  8, -2,  1,  3, -9,\n",
            "        -2, -2,  0, -3, -2,  5, -4,  1, -4, -6,  2,  2, -7, -2, -2,  1,  2,  6,\n",
            "         0,  1])\n",
            "tensor([[[ 1.2844,  1.5412,  1.4385,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.0721,  2.3290,  2.0378,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.9180,  2.0892,  1.5241,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.0960, -0.9590, -0.8905,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-0.6507, -0.5480, -0.7364,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-0.6165, -0.4966, -0.6336,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.2780,  1.5231,  1.5056,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.0483,  2.3810,  2.2584,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.9783,  2.2759,  1.9433,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.4027, -0.0875, -0.2101,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.1401,  0.3852,  0.0525,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.0175,  0.2276,  0.0525,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.1155,  1.3943,  1.4641,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.8998,  2.2309,  2.2484,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.8824,  2.2484,  2.1264,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.7320,  1.8301,  1.9869,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 0.7669,  1.6732,  1.9172,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.3660,  0.6972,  0.8889,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -7,  8, -1, -1,  5,  1, -7, -8,  2, -2,  2,  6,  6, -1,  8,  5, -3,\n",
            "         0,  2, -2,  2,  3,  4,  6,  0, -5,  2,  2,  2,  1, -7,  2,  0,  1,  2,\n",
            "        -2,  1, -6, -1, -3,  1,  0, -3, -5,  0, -6, -1,  2,  3,  1,  0, -2,  1,\n",
            "        -1,  0, -1, -5,  6, -5,  4, -9,  0,  3, -4, -4, -6,  3,  7,  2,  5,  4,\n",
            "         4, -5,  1, -2, -7,  1,  3,  1,  6, -4, -6, -4,  8, -5,  2,  4,  8,  1,\n",
            "         2,  2,  1, -1, -5, -7, -2, -1,  7, -2,  2, -8, -2,  0, -4,  0,  8, -1,\n",
            "        -1,  6,  4, -8, -3,  7, -2, -7, -3, -1,  8,  3, -8, -4, -4, -8,  1,  8,\n",
            "        -6, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.5584,  1.6611,  1.1131,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.7981, -1.2844, -0.9419,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-1.4214, -1.0617, -1.0275,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-1.1302, -0.8391, -0.8220,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.7682,  1.9958,  1.7332,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.1730, -0.6127, -0.5602,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-0.4902, -0.0875, -0.3501,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-0.3151,  0.1050,  0.0525,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.4118,  1.7429,  1.6732,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.0697,  1.4292,  1.7778,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 0.0871,  1.1329,  1.4641,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.0697,  0.4357,  0.6623,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -9,  4, -2, -6,  3, -1, -6, -2,  7, -1,  1,  0,  2,  4,  7,  0,  3,\n",
            "         2,  0,  2,  6,  8,  0, -2, -1, -5,  2, -5, -2,  3, -6,  8,  5,  2, -4,\n",
            "         3,  0,  0, -3,  0,  1,  1, -3, -7,  3, -5, -1,  6,  2, -6, -5, -1, -8,\n",
            "         2, -4,  7, -3,  7, -7,  5, -1, -1,  8, -3, -2,  0,  4,  4,  0,  0,  2,\n",
            "        -2, -2, -2,  5,  1, -2, -3,  1,  8, -2, -4, -4,  3, -4, -2, -1,  4, -4,\n",
            "         4,  4,  8, -1, -1, -2, -7,  5,  0, -1,  2,  0, -3,  0, -3, -3,  6, -9,\n",
            "        -4,  6,  0, -8, -2,  8, -4, -2, -2,  1,  2,  8,  1,  2, -8, -2, -5,  0,\n",
            "        -7, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -0.3767,  ...,  0.0171,  1.6269,  1.2501],\n",
            "         [-3.0825, -2.9626, -0.2740,  ..., -0.2397,  1.0446,  0.5309],\n",
            "         [-3.0311, -2.8770, -0.2569,  ..., -0.8562,  0.0856, -0.2569]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786,  0.2626,  ...,  0.4027,  2.1008,  1.6282],\n",
            "         [-2.4335, -2.1534,  0.4552,  ...,  0.3151,  1.6807,  1.0679],\n",
            "         [-2.5210, -2.2584,  0.3852,  ..., -0.3151,  0.6653,  0.2801]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  2.6492,  ...,  1.5686,  2.5272,  1.8649],\n",
            "         [-1.6732, -0.7320,  2.4749,  ...,  1.2898,  2.1786,  1.3943],\n",
            "         [-2.0218, -1.6383,  1.3769,  ...,  0.5054,  1.2723,  0.6275]]])\n",
            "tensor([ 2, -9,  8, -1, -2,  1, -6, -4, -3, -1,  5,  3,  3,  4,  2,  0,  4,  4,\n",
            "        -2,  4, -1,  6,  0, -1,  4, -4, -5,  6,  0,  2,  2, -2,  2,  1, -6,  1,\n",
            "         0,  0, -3,  1, -3,  1, -4, -3, -6,  3, -5,  0,  0,  0, -1, -4,  1,  0,\n",
            "        -1, -3,  3, -4,  9, -5, -2, -9, -5,  3,  3, -2,  2,  1,  6,  0,  8,  3,\n",
            "         7, -7, -2,  0, -2,  0,  3,  4,  7,  1,  3, -1,  7,  0, -2, -1,  6, -6,\n",
            "        -1, -4,  7, -3, -6, -5, -3, -2,  6, -4, -3,  0,  1, -1, -8, -5,  8, -5,\n",
            "        -1, -2,  0, -1,  3,  6, -1,  1,  2, -1,  4,  1, -1, -1, -3, -4,  0,  1,\n",
            "        -6, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.7296,  1.0960,  0.4624,  ..., -0.9932, -2.0378, -2.0892],\n",
            "         [ 1.4727,  1.1474, -0.0685,  ..., -0.8562, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.2947, -2.0036, -1.9865,  ..., -1.0275, -0.9590, -0.9076],\n",
            "         [-1.7125, -1.6097, -1.8152,  ..., -1.2159, -1.6611, -1.4214],\n",
            "         [-1.7296, -1.2844, -1.4385,  ..., -2.3290, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 0.7178,  0.4552,  0.4377,  ..., -0.9629, -1.4531, -1.5231],\n",
            "         [ 0.5077,  0.4727,  0.1225,  ..., -0.9279, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.3810, -2.0483, -2.2759,  ..., -1.3655, -0.5427, -0.5952],\n",
            "         [-1.7507, -1.5056, -1.8732,  ..., -1.3130, -1.0854, -0.9279],\n",
            "         [-1.9608, -1.6457, -1.8732,  ..., -2.3985, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.5229,  0.4183,  0.5054,  ..., -0.4706, -0.8715, -0.9935],\n",
            "         [ 0.3660,  0.5054,  0.4009,  ..., -0.3834, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.2026, -0.0697,  0.1569,  ..., -0.5403, -0.1220, -0.3486],\n",
            "         [-1.0283, -0.2963,  0.0871,  ..., -0.6100, -0.5926, -0.5926],\n",
            "         [-1.4641, -1.0980, -1.0458,  ..., -1.7603, -1.4641, -1.2549]]])\n",
            "tensor([ 0,  0,  4,  4, -1, -2, -1, -5, -4, -1, -1, -1,  6, -2, -4,  7,  4, -4,\n",
            "         5, -2,  4, -1,  2,  4,  2, -3,  1,  6, -2, -1,  6, -1,  4,  3,  1,  3,\n",
            "        -5, -1, -3,  1, -1,  1, -6, -2, -2,  2,  0,  5,  1,  0, -7,  0, -6, -6,\n",
            "        -2,  0,  0, -5,  4,  0,  3, -7, -4,  2,  4, -3,  3,  5,  0,  4,  4,  2,\n",
            "         5, -5, -1, -1, -5, -1, -1, -3,  5,  4,  2, -3,  6,  0, -5, -2,  6,  0,\n",
            "        -3, -4,  8, -3, -2,  0,  0, -1,  3,  2,  2, -7,  0,  8, -2, -3,  0, -9,\n",
            "         0,  5,  6, -6, -3,  0, -3, -2, -5, -2,  6,  0, -7,  0,  0, -3,  0,  6,\n",
            "        -8,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [ 0.1199,  0.2911,  0.1712,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.6165,  0.8049,  0.4624,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.6336,  0.7706,  0.4624,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.7528,  1.0854,  0.7703,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 1.2955,  1.6457,  1.1555,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 1.1730,  1.4181,  1.0679,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.9346,  3.0501,  3.0327,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 1.9521,  2.9630,  3.0501,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 1.5686,  1.9346,  1.9346,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -7,  7, -1, -1,  0, -4, -1, -2, -2,  4, -4,  6,  3, -2,  2,  4, -2,\n",
            "         6,  5, -2,  5,  5,  2,  4,  0,  1,  7,  4,  3,  7, -8,  7,  0, -5, -4,\n",
            "         0, -3, -3,  6,  0, -2, -4,  5, -6,  4, -5,  0,  5,  1, -6, -7, -6, -2,\n",
            "         1, -6,  5,  1,  2, -7,  0, -3,  3,  7,  3,  4,  2,  5,  1,  2,  5,  1,\n",
            "         5, -5, -1, -3,  0, -3, -3,  1,  2,  5, -1, -1,  6, -7, -3,  3,  5, -4,\n",
            "        -4,  1,  6,  0, -7, -2,  0,  4,  7, -5, -3, -2,  2,  2, -5,  0,  5, -7,\n",
            "        -1,  0,  4, -8,  1,  0, -1, -5, -2,  1,  1,  2,  0, -6, -1,  1, -2,  4,\n",
            "        -6,  2])\n",
            "tensor([[[ 1.1302,  1.4214,  1.3015,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.8837,  2.1748,  1.8666,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.7296,  1.9008,  1.3357,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.3357, -1.2159, -1.1302,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-0.9932, -0.8562, -1.0789,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-1.1131, -0.9590, -1.2330,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.4181,  1.6807,  1.6457,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.1709,  2.5035,  2.3634,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.1008,  2.3810,  2.0308,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.4902, -0.2101, -0.3326,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-0.1050,  0.2101, -0.1926,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-0.3676, -0.0875, -0.4202,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.7778,  2.0915,  2.1264,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 2.5447,  2.8932,  2.8932,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.5272,  2.8932,  2.7538,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.0109,  2.1089,  2.2658,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 0.8540,  1.8126,  2.0044,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.3312,  0.7320,  0.7495,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-6, -1,  8,  5, -6, -2, -2, -7, -7,  6,  4, -3,  4, -1, -3,  3,  6,  0,\n",
            "        -3,  1,  1,  7,  0,  7, -1, -2, -3,  2,  0,  5,  6, -6,  6, -4, -6, -4,\n",
            "         3,  2, -1,  0,  4,  3,  1, -2, -7,  7, -7,  2,  3,  4,  1, -8,  3,  0,\n",
            "         1, -4,  2,  2,  2, -5,  3, -5, -4,  4, -4,  4,  1,  4, -1,  3,  5,  2,\n",
            "         1, -4, -3,  5, -1,  2,  4, -1,  4,  3,  0,  0,  1, -1, -2,  3,  7, -1,\n",
            "         1, -1,  5, -1, -4, -7, -5,  2, -2, -4, -2, -2,  2, -1, -4, -6,  7, -2,\n",
            "         0,  5,  6, -7, -6,  5, -3, -4, -5,  3,  2,  0,  0, -2, -5, -3, -2,  8,\n",
            "        -6,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.6679,  1.3871,  1.1816,  ...,  2.0721,  2.2091, -1.8666],\n",
            "         ...,\n",
            "         [-2.3290, -2.1577, -1.9522,  ...,  1.2159,  2.8941, -0.9076],\n",
            "         [-1.8666, -1.5926, -1.8837,  ...,  0.8905,  2.1406, -1.4214],\n",
            "         [-1.7981, -1.7296, -2.0207,  ...,  0.2911,  1.4727, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.6653,  1.4881,  1.4881,  ...,  2.5210,  2.7136, -1.2780],\n",
            "         ...,\n",
            "         [-1.9083, -1.5231, -1.4706,  ...,  1.5231,  3.1863, -0.5952],\n",
            "         [-1.3480, -0.8929, -1.3130,  ...,  1.3305,  2.5910, -0.9279],\n",
            "         [-1.4006, -1.1905, -1.5231,  ...,  0.6303,  1.8732, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.5403,  1.3595,  1.5512,  ...,  3.2941,  3.4858, -0.7320],\n",
            "         ...,\n",
            "         [-0.6972,  0.3660,  0.6100,  ...,  2.7015,  3.6950, -0.3486],\n",
            "         [-0.6797,  0.3312,  0.4009,  ...,  2.3704,  3.1547, -0.5926],\n",
            "         [-0.9935, -0.7495, -0.7843,  ...,  1.4815,  2.5621, -1.2549]]])\n",
            "tensor([ 0, -1,  8, -1, -3, -2, -1, -7, -2,  6, -1, -2,  6,  3,  5,  5,  2, -1,\n",
            "        -2, -3, -3,  7,  8,  5,  6, -4, -3,  7,  3, -3,  4, -3, -1, -1, -2,  1,\n",
            "         2,  4, -9, -2,  3, -3, -7,  3, -3,  6,  1,  7,  8, -3, -4, -4, -2, -7,\n",
            "         6, -2,  5, -2,  0, -1,  1, -7, -6,  3,  3,  2,  0,  3,  1,  4,  8, -1,\n",
            "         3, -5,  4, -1,  1, -6, -4,  4,  4,  4, -3, -5, -1, -4, -3, -2,  1, -5,\n",
            "         2, -3,  4, -5, -4, -7, -5,  7,  6, -5, -1, -1,  1,  6, -8, -4,  8, -2,\n",
            "        -7,  6,  0, -2, -6,  4,  2, -4,  2, -2,  5,  3, -1, -5, -1, -1, -3,  1,\n",
            "         1, -2])\n",
            "tensor([[[ 2.2947,  2.6201,  2.5687,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.0311,  3.3565,  3.1510,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.8770,  3.0996,  2.5345,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.8220, -0.6850, -0.6336,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.0133,  2.3459,  2.3985,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.7486,  3.1513,  3.0987,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.6961,  3.0462,  2.7136,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.1050,  0.2101,  0.0700,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.7429,  2.0915,  2.1961,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 2.4924,  2.8758,  2.9804,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.4749,  2.8932,  2.7538,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.2723,  2.3355,  2.4749,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -1,  6, -1, -2, -3, -3, -5, -3,  4, -1, -2, -2, -3,  5,  2,  8, -3,\n",
            "         5,  5,  2, -1,  1,  3,  7, -5, -5,  7,  4, -2,  8, -4,  0,  3,  1,  2,\n",
            "         0,  2, -4, -2,  3, -2, -6,  4, -6,  5,  1,  4,  5,  1, -2, -1, -2, -5,\n",
            "         4, -5,  6, -2,  2, -7,  2,  0,  0,  6,  1, -2,  2,  8,  5,  0,  7,  6,\n",
            "         6, -4,  2, -3, -1, -4,  2, -2,  8, -1,  0,  0,  6,  1, -6, -2,  7, -2,\n",
            "         2,  1,  4, -9,  0, -1, -4,  6,  4, -1, -3,  0,  2,  6, -5, -2,  1, -2,\n",
            "        -2,  6,  1, -8, -1,  5,  0, -1, -2, -3,  8, -1, -3, -6, -4, -3, -3,  8,\n",
            "         1,  3])\n",
            "tensor([[[ 2.2605,  2.5858,  2.4488,  ...,  0.8562, -2.6030, -2.5345],\n",
            "         [ 3.0482,  3.3907,  3.0653,  ...,  1.4556, -2.0378, -2.0892],\n",
            "         [ 2.9283,  3.1167,  2.5687,  ...,  1.5755, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.9522, -2.0721, -2.1920,  ..., -0.3596, -0.9590, -0.9076],\n",
            "         [-1.4385, -1.3529, -1.5755,  ..., -0.8049, -1.6611, -1.4214],\n",
            "         [-1.4214, -1.2672, -1.4556,  ..., -1.4556, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.5385,  2.8536,  2.7836,  ...,  1.5231, -2.1884, -2.1709],\n",
            "         [ 3.3438,  3.7465,  3.5714,  ...,  2.3634, -1.4531, -1.5231],\n",
            "         [ 3.3088,  3.6239,  3.3088,  ...,  2.4685, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.9629, -0.9454, -1.2430,  ...,  0.4027, -0.5427, -0.5952],\n",
            "         [-0.3852, -0.1050, -0.4552,  ...,  0.1401, -1.0854, -0.9279],\n",
            "         [-0.5077, -0.2276, -0.4552,  ..., -0.6303, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.8584,  3.2244,  3.2244,  ...,  2.2658, -1.7778, -1.7952],\n",
            "         [ 3.6776,  4.0959,  4.0610,  ...,  3.3115, -0.8715, -0.9935],\n",
            "         [ 3.6950,  4.0959,  3.9739,  ...,  3.4336, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.5926,  1.3769,  1.2898,  ...,  1.6035, -0.1220, -0.3486],\n",
            "         [ 0.5926,  1.5338,  1.7255,  ...,  1.4292, -0.5926, -0.5926],\n",
            "         [ 0.2092,  0.6100,  0.7320,  ...,  0.4183, -1.4641, -1.2549]]])\n",
            "tensor([ 2, -2,  1,  3, -1,  1,  2, -1, -9,  7, -2, -1,  0,  6, -1,  2, -1,  2,\n",
            "        -2,  3,  0,  2,  5,  5, -2, -7, -3, -1, -2,  1,  3,  0, -1,  5, -1, -2,\n",
            "        -3,  6, -6,  6,  0, -1,  0,  4, -5,  6,  1,  7,  8,  3, -3, -6,  1, -7,\n",
            "         0, -9,  5,  1,  8, -3,  2, -3, -3,  0, -2,  5,  2,  3,  2,  0,  5,  3,\n",
            "         5, -8,  3, -3, -1, -1,  3, -3,  6,  0, -2, -9,  8, -7,  0, -2,  4, -7,\n",
            "         5,  1,  5, -2, -1, -1,  0, -1, -2,  4,  0, -4, -3,  2, -9, -1,  2, -7,\n",
            "        -1,  6,  7,  1,  0,  0,  3, -7, -5,  3,  6, -1, -1, -2, -3, -3,  2,  1,\n",
            "        -4, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.3461,  3.6476,  3.3907,  ..., -0.7877, -2.0378, -2.0892],\n",
            "         [ 2.1406,  2.2091,  2.8427,  ..., -0.0514, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.3254, -0.2226, -0.1027,  ...,  0.0000, -0.9590, -0.9076],\n",
            "         [-0.1370, -0.0342, -0.2397,  ..., -0.2226, -1.6611, -1.4214],\n",
            "         [-0.0856, -0.0685, -0.7364,  ..., -0.3767, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.0483,  3.6064,  3.6415,  ..., -0.2451, -1.4531, -1.5231],\n",
            "         [ 1.6106,  2.1183,  3.1513,  ...,  0.4027, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.0700,  0.3676,  0.2801,  ...,  0.2451, -0.5427, -0.5952],\n",
            "         [ 0.3326,  0.5952,  0.2626,  ...,  0.1926, -1.0854, -0.9279],\n",
            "         [ 0.2276,  0.3501, -0.3501,  ..., -0.0175, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.7603,  3.3638,  3.5207,  ...,  0.1569, -0.8715, -0.9935],\n",
            "         [ 1.4292,  1.9695,  3.2244,  ...,  0.8192, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.5403,  1.6035,  1.9172,  ...,  0.5926, -0.1220, -0.3486],\n",
            "         [ 0.3312,  1.2549,  1.5861,  ...,  0.3660, -0.5926, -0.5926],\n",
            "         [ 0.2266,  0.4706,  0.1220,  ...,  0.0174, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -4, -1, -1, -1,  0, -4, -5,  0, -2,  2, -5,  3, -1,  4,  5, -1,  0,\n",
            "         1,  5,  2,  1, -1,  7,  2, -9, -3,  4, -3,  0,  1, -3,  6, -4,  1,  1,\n",
            "        -4,  5, -1,  6,  6,  5, -6, -1, -4,  0, -5,  1,  4, -2, -4, -1, -3, -1,\n",
            "         1, -4, -1, -5,  0, -7,  4, -2, -6,  5,  4,  5, -2,  1,  6,  2, -1, -1,\n",
            "         7, -2,  1,  0, -7,  2, -5,  1,  5, -4,  0, -8,  8, -5,  1, -2,  3,  1,\n",
            "        -4, -5,  7, -4, -5, -4,  1,  5,  1, -4, -2, -7,  3,  1,  0,  0,  1, -9,\n",
            "        -5, -2,  9,  0,  3,  2,  2,  2,  1, -2,  4,  3, -4, -5, -3,  1, -1,  5,\n",
            "        -2, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.4452, -0.4624, -0.2397],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  1.5584,  3.3736,  3.4250],\n",
            "         [-3.0825, -2.9626, -3.1852,  ...,  0.9761,  2.5002,  2.7913],\n",
            "         [-3.0311, -2.8770, -3.0653,  ...,  0.5822,  1.6782,  2.1920]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.4027,  0.4202,  0.6828],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.4552,  2.3109,  2.2409],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.0875,  1.5231,  1.8382],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.4027,  0.5952,  0.9804]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.3595,  1.3769,  1.5338],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  2.2309,  3.3813,  3.1024],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  1.5686,  2.6667,  2.7364],\n",
            "         [-2.0218, -1.6383, -1.5163,  ...,  1.0458,  1.8649,  2.0044]]])\n",
            "tensor([-3,  0,  3, -2,  0,  5, -4, -8, -7,  3,  4, -3, -2, -1, -1,  3, -1, -1,\n",
            "         6, -2,  2, -2,  6,  1,  5, -7,  0,  1,  4,  3,  8, -7,  2, -3,  0,  4,\n",
            "         3,  5, -7, -1, -2,  0, -1,  4, -6, -2, -2,  8,  8,  3,  0, -5, -3, -6,\n",
            "         6, -1,  4,  0,  4, -2, -2, -8, -6,  5,  4,  2, -6,  4,  4,  4,  5,  2,\n",
            "         6, -5, -4,  5, -7,  2, -5,  4,  6,  5, -5, -2,  5,  0, -3,  0,  4, -7,\n",
            "        -4, -2, -1,  0, -6,  1, -4,  2, -1,  0,  0, -5, -3,  4, -9, -6,  7, -8,\n",
            "        -1,  4,  5, -1,  0,  6,  3,  1, -2,  3,  7, -1, -2,  3, -5, -8, -5,  3,\n",
            "         1,  1])\n",
            "tensor([[[-1.0104, -0.7364,  1.1474,  ..., -0.6507, -0.5651, -0.5309],\n",
            "         [-0.2740,  0.0000,  1.6440,  ..., -0.0856, -0.0342, -0.1027],\n",
            "         [-0.4281, -0.2740,  1.0446,  ..., -0.0171, -0.0514,  0.1027],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053,  1.1555,  ..., -0.2976, -0.1926, -0.1926],\n",
            "         [-0.3501,  0.0000,  1.8207,  ...,  0.4552,  0.5252,  0.4377],\n",
            "         [-0.4202, -0.1225,  1.4356,  ...,  0.5252,  0.5077,  0.6828],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843,  1.8126,  ...,  0.6275,  0.6972,  0.6275],\n",
            "         [-0.3486,  0.0000,  2.5272,  ...,  1.5338,  1.5861,  1.3943],\n",
            "         [-0.3660,  0.0000,  2.2658,  ...,  1.5338,  1.5163,  1.6209],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-6, -7,  0,  6, -4, -1, -6, -9, -6,  6,  5, -6,  2,  4,  0,  8,  4,  5,\n",
            "        -1,  5, -1,  3,  1,  7,  2, -1,  0, -2, -1,  0,  6, -1,  3, -2, -7, -3,\n",
            "         3,  0, -8, -2,  4, -1,  0, -2, -5,  7, -4,  3,  2, -3,  0, -2, -4, -8,\n",
            "         0, -8,  2,  1,  6, -5, -2, -2, -6,  0, -2, -2,  2,  7,  4,  1, -1,  5,\n",
            "         0, -2, -3,  5, -5, -1,  1,  3,  4,  5,  0, -3,  5, -8, -2,  4,  5, -1,\n",
            "        -4,  2,  0, -1, -6,  0, -3,  0,  5,  4,  2, -1, -3,  5, -1, -4,  5, -9,\n",
            "        -3, -2,  1, -7, -2, -1,  2, -4, -3, -2,  6,  1, -8, -4, -8, -5,  3,  5,\n",
            "        -1,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.7877, -0.6336, -0.2569],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.2055,  0.0685,  0.0514],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.2569, -0.0856, -0.3767],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -1.3015,  0.4110,  0.5651],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -1.6611, -0.2740,  0.0000],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  0.1401,  0.3326,  0.7528],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.9454,  1.1204,  1.1204],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.8578,  0.9454,  0.6653],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.6653,  1.1204,  1.1905],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.8403,  0.5952,  0.7703],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.1917,  0.1220,  1.0458],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.6972,  1.6558,  1.9172],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.0109,  1.7081,  1.0980],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.1917,  1.2375,  1.0806],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -0.1917,  0.7669,  0.7495],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -3,  4,  0, -4, -1, -1, -5, -6,  3,  4,  1,  4,  3, -4,  7,  2, -4,\n",
            "        -2, -3, -2,  1,  1,  8,  5, -7, -6, -2,  1,  0,  6, -4,  2,  3, -6, -3,\n",
            "        -4,  5, -3,  0,  5,  1,  1,  1, -2,  6, -6,  4, -1,  3, -2, -3, -6, -6,\n",
            "        -1,  0,  6,  1,  9, -6, -2, -5, -2,  7,  3, -1, -2,  2,  4, -1,  5, -3,\n",
            "         0, -7, -4,  1, -7,  2, -4,  4,  7,  4,  3, -7,  4, -1, -5, -3,  0,  2,\n",
            "        -3,  3,  8, -7, -5, -8, -6,  3,  0, -5, -6, -2, -5,  6, -8,  3,  6, -5,\n",
            "        -7,  4,  4,  1, -2,  5,  1, -5,  0,  2,  0,  6, -5, -6,  0, -1, -6,  2,\n",
            "        -5,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.1920, -0.4624, -0.4110],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -2.5516, -1.1131, -0.9076],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -2.3632, -1.3529, -0.8391]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -1.9608, -0.2101, -0.2801],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.1183, -0.6653, -0.5427],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.9608, -0.9104, -0.4202]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.9586,  0.1046, -0.1394],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.2723, -0.2789, -0.3137],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.2723, -0.4009, -0.1569]]])\n",
            "tensor([ 2, -7,  6,  4, -3, -1, -3, -5, -6,  4, -4, -2,  1,  1, -2,  3,  6,  1,\n",
            "         2,  4,  3,  2,  5,  3,  4, -3, -7,  2,  3, -1,  3,  0,  1,  3,  0,  3,\n",
            "        -1,  6,  0,  5,  6,  0, -2, -1, -4,  1, -5,  4,  7,  3,  0,  0, -5, -2,\n",
            "         3, -1,  6, -1,  2, -5,  4, -9,  2,  8,  0,  4,  2,  6,  0,  4,  1,  0,\n",
            "        -1, -5,  2, -1,  2,  3, -1,  5, -1,  0,  1,  0,  2, -1, -2,  3,  8, -6,\n",
            "        -4, -4,  1,  0, -3, -5, -4,  6,  5,  3,  3, -9, -3,  3, -3, -6,  7, -1,\n",
            "        -4,  1,  8, -7, -5,  4, -6, -7,  0, -6,  2, -1, -1,  1, -4,  1, -1,  5,\n",
            "        -1, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.0104, -2.1235, -2.1577,  ..., -1.0960,  1.4385, -0.9076],\n",
            "         [-0.4966, -1.2330, -1.7981,  ..., -1.6782,  0.7535, -1.4214],\n",
            "         [-0.3082, -0.3596, -0.5994,  ..., -2.6543, -0.5480, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.0875, -0.8403, -1.2080,  ...,  0.2626,  2.5385, -0.5952],\n",
            "         [ 0.6828,  0.1576, -0.6653,  ..., -0.1751,  1.9433, -0.9279],\n",
            "         [ 0.7353,  0.8403,  0.5602,  ..., -1.2605,  0.7353, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.1743,  0.2440,  0.3312,  ...,  1.6906,  2.3529, -0.3486],\n",
            "         [ 0.2092,  0.4532,  0.3486,  ...,  1.0806,  1.8475, -0.5926],\n",
            "         [-0.0349,  0.2440,  0.3834,  ..., -0.0523,  0.5577, -1.2549]]])\n",
            "tensor([ 1, -4,  1,  2,  0,  0,  0, -8, -3,  1,  0,  2, -1,  6,  0,  4,  4,  0,\n",
            "         2, -3,  2, -2, -1,  7,  7, -8,  2,  3,  2, -4, -1,  0,  2,  3, -1,  1,\n",
            "         2,  2, -7, -2,  1, -3, -4, -4, -8,  1, -5,  1,  6,  2, -2, -1, -2, -6,\n",
            "         4, -6,  6, -5,  8, -3,  5,  0, -3, -1,  2, -2, -4,  5, -1,  4,  5, -1,\n",
            "         7, -9,  3,  0,  1,  3, -5,  1,  0, -4, -2, -7,  8, -5,  1,  5,  7,  2,\n",
            "        -2, -1,  2, -2, -9,  1, -7, -1,  6,  1,  1, -7,  3,  0, -9,  3,  5, -4,\n",
            "         0, -1,  7, -3,  0,  4, -2,  2, -2, -2,  4,  5,  0, -6, -2, -6, -1,  7,\n",
            "         1,  4])\n",
            "tensor([[[ 0.5651,  0.8391,  0.7706,  ..., -1.0960, -1.0789, -2.5345],\n",
            "         [ 1.2501,  1.5241,  1.3015,  ..., -0.5822, -0.5480, -2.0892],\n",
            "         [ 1.0960,  1.3015,  0.7535,  ..., -0.5309, -0.5651, -1.8666],\n",
            "         ...,\n",
            "         [-0.8562, -0.7364, -0.7021,  ..., -0.3596,  1.4214, -0.9076],\n",
            "         [-0.3425, -0.2226, -0.4624,  ..., -0.5994,  0.9076, -1.4214],\n",
            "         [-0.3082, -0.1541, -0.3082,  ..., -1.1474, -0.0171, -2.1063]],\n",
            "\n",
            "        [[ 0.5427,  0.8228,  0.8228,  ..., -0.6478, -0.6127, -2.1709],\n",
            "         [ 1.2255,  1.5756,  1.5056,  ...,  0.0350,  0.0875, -1.5231],\n",
            "         [ 1.1555,  1.5056,  1.1555,  ...,  0.0700,  0.0700, -1.2780],\n",
            "         ...,\n",
            "         [-0.2276,  0.0875, -0.0875,  ...,  0.1225,  1.9083, -0.5952],\n",
            "         [ 0.3501,  0.6303,  0.2451,  ...,  0.0175,  1.5406, -0.9279],\n",
            "         [ 0.2451,  0.5077,  0.3151,  ..., -0.6127,  0.5602, -1.6106]],\n",
            "\n",
            "        [[ 0.6100,  0.9063,  0.9586,  ..., -0.1917, -0.1743, -1.7952],\n",
            "         [ 1.3072,  1.6558,  1.6732,  ...,  0.6449,  0.6972, -0.9935],\n",
            "         [ 1.2898,  1.6906,  1.5163,  ...,  0.6972,  0.6972, -0.7320],\n",
            "         ...,\n",
            "         [ 1.0458,  2.1264,  2.2309,  ...,  1.1852,  2.2832, -0.3486],\n",
            "         [ 1.0632,  2.0044,  2.1961,  ...,  0.9237,  1.9869, -0.5926],\n",
            "         [ 0.6972,  1.0806,  1.2375,  ...,  0.1220,  1.1155, -1.2549]]])\n",
            "tensor([ 0, -4,  4, -1, -2,  1, -4, -9, -5,  6,  5,  1,  3,  3,  1,  8,  3,  1,\n",
            "         1, -2,  0,  2, -1,  1,  0, -3, -4,  3,  0,  0,  4, -4,  7,  1, -1, -1,\n",
            "        -1,  2, -8, -1,  0, -2,  2, -3, -3,  1,  0,  6,  1,  1, -4, -1,  3, -3,\n",
            "         2, -1,  4, -4,  3, -8,  4, -7, -6, -1,  3,  1,  2,  4,  7, -1,  8, -3,\n",
            "         6,  0, -1,  0, -4, -1, -2, -3,  8,  1, -4,  0,  7, -2, -3,  3,  5, -7,\n",
            "        -2,  1,  0, -1, -4, -3, -3,  6,  4, -1,  0, -3, -6,  5, -1, -6, -1, -2,\n",
            "         0,  3,  9,  1,  0,  2, -2, -6, -6, -6,  0,  7, -1, -1, -4, -3, -2,  3,\n",
            "        -2,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.9419,  0.8734,  0.9590],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -1.4727,  0.0342,  0.3082],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.9865, -0.8734, -0.3767]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.0350,  1.7332,  1.6632],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.3151,  1.1555,  1.2955],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.8578,  0.2801,  0.7178]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.6035,  2.6667,  2.4227],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  1.2375,  2.2484,  2.2135],\n",
            "         [-2.0218, -1.6383, -1.5163,  ...,  0.5054,  1.4641,  1.6383]]])\n",
            "tensor([ 2, -1,  6,  0, -3,  0,  2, -9, -3, -2,  1,  0,  0,  1,  1,  7,  4, -3,\n",
            "         0,  5, -3,  1,  5,  4,  5,  0, -1,  1,  0,  5,  4,  0,  3,  0, -3, -5,\n",
            "         1,  5, -9,  2,  0, -3, -4, -1,  0, -1, -3,  1,  4,  5, -3, -5,  1, -2,\n",
            "         3, -7,  8, -1,  0, -1,  0, -4,  0,  0, -1,  0,  3,  2,  8,  7,  5, -3,\n",
            "        -2, -5,  1, -3, -5,  1,  3,  2,  4,  4, -1, -8,  2, -6, -4, -2,  8, -1,\n",
            "        -1,  3,  0, -5, -9, -6, -1,  4, -1,  2, -4, -5, -3,  4, -9, -1,  3, -9,\n",
            "        -4,  6,  5, -5, -3,  6,  3, -7,  0, -2,  7,  3, -4, -1, -2, -1, -2,  1,\n",
            "         0, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.0140, -2.4488,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.8598, -2.5516,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0311,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.2409, -1.8908,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.0308, -1.8908,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4510,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.1743,  0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.6100,  0.0871,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.4815,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 3,  0,  6,  2,  1,  4, -5, -6, -7,  1, -3, -1,  5,  1,  0,  5,  6, -1,\n",
            "        -1,  0,  1,  5,  6,  1,  5, -6, -6,  0, -5,  5,  1,  0,  4,  0, -7, -4,\n",
            "         0,  5, -9, -2,  2,  2, -3,  1, -5,  4, -4,  6,  7,  0, -1,  0, -6,  1,\n",
            "         6, -4,  3, -5,  7, -7,  0, -8,  0, -1,  4,  5, -4,  5,  2,  6,  3,  1,\n",
            "         5, -8, -2,  1, -4, -3,  2, -2,  7, -3,  1, -7,  1,  0,  2,  5,  0, -4,\n",
            "        -3, -1,  4, -9, -8,  0, -1,  0, -1,  1,  2, -4, -5, -1, -9, -2, -1, -6,\n",
            "        -5,  1,  2, -6, -1,  1, -1,  0, -2,  0,  4,  7,  1,  2, -8,  1, -6,  8,\n",
            "        -1,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -1.5070, -1.7981, -1.9522],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.8220, -0.8391, -1.1645],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.7877, -0.7706, -0.8391],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.4452,  1.0446,  1.0275],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.8228, -1.2080, -1.4706],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.0700,  0.0175, -0.4027],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.0875,  0.1050, -0.0350],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.1225,  1.5231,  1.2780],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.2614, -0.6972, -1.0458],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.7843,  0.7146,  0.1569],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  0.8017,  0.7669,  0.4880],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.6275,  1.5338,  1.1852],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-4, -9,  4,  5,  0,  5,  1, -6, -9,  3, -4, -1, -1,  5,  1,  4,  2, -1,\n",
            "        -1,  1,  0,  4,  6,  1,  5, -6, -1, -2, -1, -3,  8, -6,  7,  0, -7, -1,\n",
            "        -5,  4, -1,  4,  3,  1, -6,  4, -8,  2, -7,  4,  8,  2, -8, -2, -4, -6,\n",
            "        -1, -8,  8, -3,  3, -6,  4, -3, -6,  3,  1, -3, -4,  8,  6,  2,  3,  1,\n",
            "         7, -1, -1,  5, -6, -6, -4,  4,  3, -2, -4, -9,  2, -7, -4,  1,  7, -6,\n",
            "        -4, -5,  5, -5, -4, -2,  1, -2, -1,  1, -5, -5,  1, -1, -3, -5, -1, -4,\n",
            "        -2, -1,  3, -6,  2,  6, -2, -4, -6, -1,  1,  0, -4, -2, -2, -5, -4,  1,\n",
            "        -2,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.6954, -1.5412, -1.4899,  ..., -0.9247, -0.9590, -0.9076],\n",
            "         [-1.2844, -1.1131, -1.3186,  ..., -1.3871, -1.6611, -1.4214],\n",
            "         [-1.1645, -0.9932, -1.1987,  ..., -2.0036, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.0504, -0.7178, -0.8578,  ..., -0.5252, -0.5427, -0.5952],\n",
            "         [-0.5952, -0.2451, -0.5777,  ..., -0.8403, -1.0854, -0.9279],\n",
            "         [-0.6127, -0.3151, -0.5427,  ..., -1.5581, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.2963,  1.3943,  1.5512,  ...,  0.5403, -0.1220, -0.3486],\n",
            "         [ 0.2266,  1.2200,  1.4641,  ...,  0.0697, -0.5926, -0.5926],\n",
            "         [-0.0523,  0.3660,  0.4880,  ..., -0.8192, -1.4641, -1.2549]]])\n",
            "tensor([-3,  0,  1,  3,  1, -3, -5, -1, -5,  3,  0, -4, -1, -1,  1,  5,  3,  1,\n",
            "         3,  4, -3,  1,  3,  1,  3, -8, -2, -1,  2,  2,  1, -3,  3,  1, -6, -5,\n",
            "         3,  5, -6,  2,  2, -3,  1, -2, -2, -2,  1,  0, -1, -1, -1, -4,  0, -3,\n",
            "         2, -1, -1, -4,  5, -9,  5, -2, -2,  6,  4, -1, -3, -1,  6,  5,  1, -2,\n",
            "         2, -6,  0,  0,  1, -2, -2,  0, -1, -2,  3, -9,  5, -4, -1,  3, -1, -3,\n",
            "         0, -2,  7, -5, -2, -5,  1,  0,  2, -3, -6, -3,  1,  3, -1,  2, -1, -6,\n",
            "        -1,  5,  1,  1, -5,  2, -1, -5, -4,  1,  7,  0, -2, -2, -2, -6,  3,  5,\n",
            "         0,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421,  0.8562,  ..., -1.3357,  0.3767,  0.3767],\n",
            "         [-3.0825, -2.9626,  1.1302,  ..., -1.8837, -0.5309, -0.0514],\n",
            "         [-3.0311, -2.8770,  1.1987,  ..., -2.5173, -1.4214, -0.9761]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786,  1.4181,  ..., -0.5777,  1.1380,  0.9279],\n",
            "         [-2.4335, -2.1534,  1.8032,  ..., -1.0154,  0.3676,  0.6828],\n",
            "         [-2.5210, -2.2584,  1.7682,  ..., -1.7157, -0.5427, -0.2276]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  3.8170,  ...,  1.0980,  2.0392,  1.4641],\n",
            "         [-1.6732, -0.7320,  3.7996,  ...,  0.4183,  1.3769,  1.2898],\n",
            "         [-2.0218, -1.6383,  2.7015,  ..., -0.6972,  0.4009,  0.2789]]])\n",
            "tensor([-2, -6,  7,  4, -6,  5, -7, -8, -9,  7,  4, -6,  4, -3, -1,  5,  4,  2,\n",
            "        -2,  0, -2,  3,  4,  8,  7,  0, -3,  3,  1,  5, -1, -1,  0,  4, -5, -2,\n",
            "         3, -1, -1, -3, -1, -2,  0, -4, -8, -2, -3,  4,  7, -2, -4, -1,  3, -8,\n",
            "         0, -6,  4, -3,  6, -4,  4, -2,  0, -1,  4,  0, -4,  6,  4,  3, -1,  1,\n",
            "         2, -7, -1,  6, -4,  1,  4,  5,  5, -4, -4, -3,  5, -6, -2,  4,  6, -1,\n",
            "        -2, -1,  7, -5, -9, -6, -3, -1,  4, -1, -4, -9, -6,  2, -5, -4, -1, -5,\n",
            "        -8,  4,  2, -6,  3,  7,  1, -3,  1, -3,  2,  6, -2, -2, -8, -6, -5,  7,\n",
            "        -3, -2])\n",
            "tensor([[[-0.3425, -0.1199, -0.4110,  ..., -1.7467, -2.6030, -2.5345],\n",
            "         [ 0.1027,  0.3767,  0.1027,  ..., -1.0617, -2.0378, -2.0892],\n",
            "         [-0.1199,  0.0171, -0.4966,  ..., -0.9419, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-0.4027, -0.1751, -0.3852,  ..., -1.0329, -2.1884, -2.1709],\n",
            "         [ 0.0175,  0.3501,  0.2451,  ..., -0.1576, -1.4531, -1.5231],\n",
            "         [-0.1050,  0.1576, -0.1225,  ...,  0.0000, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.4357, -0.1917, -0.4183,  ..., -1.2549, -1.7778, -1.7952],\n",
            "         [ 0.1220,  0.4009,  0.3137,  ..., -0.2092, -0.8715, -0.9935],\n",
            "         [-0.0174,  0.2963,  0.1394,  ..., -0.0871, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -1,  8, -1, -7,  2, -7, -3, -7, -1, -1,  3,  4,  5, -2,  6,  4,  3,\n",
            "         5,  2,  2,  4, -1,  6,  1, -8,  1,  7, -3,  2,  5, -3,  8, -3,  1,  3,\n",
            "        -6, -3, -5, -1,  2,  4, -2,  1, -7,  2, -5,  6, -1,  3, -4, -7,  3, -8,\n",
            "         2, -2,  6,  2,  6, -3, -2, -5, -5,  0, -2, -1, -4, -1,  5,  5,  4,  0,\n",
            "         4, -4, -1, -2, -4,  2, -1, -3,  1, -4,  1, -3,  8, -6, -2,  3,  0, -6,\n",
            "         4,  1,  4, -4, -3, -3, -6,  1,  3, -2,  1,  0, -6,  8, -8,  2,  2, -8,\n",
            "        -6,  3,  1, -8,  1,  1, -5,  1,  1, -3, -1,  7,  1, -2, -7, -1,  1,  7,\n",
            "        -8,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.7021, -0.8734, -0.7877],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.4281, -0.6165, -0.6336],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.5480,  0.0000, -0.4452],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.2801, -0.3326, -0.3501],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.1050,  0.0175, -0.0175],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.0679,  0.6127,  0.1751],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.1917, -0.2092, -0.2092],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.4009,  0.3486,  0.2614],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.3943,  0.9586,  0.4357],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2,  0,  3,  2,  1,  0, -7,  0,  0,  2,  0, -2,  7, -1,  3,  9,  1,  3,\n",
            "         2, -3,  2,  2,  0,  5,  2, -2, -1,  1, -1, -3, -1,  0,  5,  0,  0, -3,\n",
            "        -6,  3, -4,  1,  3, -3, -2,  1, -7,  1, -3, -1,  6,  4, -5, -3,  2, -5,\n",
            "        -1, -5,  4, -4,  7, -4, -1, -5,  0,  6, -5,  5, -4,  5, -1,  4,  0,  1,\n",
            "         7, -4,  1,  6, -3, -4, -4,  0,  3, -3,  3, -4,  0, -4, -7, -2,  7, -3,\n",
            "        -3,  2,  7, -4, -3, -2,  0,  6, -1,  2, -6, -4,  3, -1, -4,  0, -1, -7,\n",
            "        -7,  5,  9, -1, -4,  1,  3, -4, -6, -6,  8,  7,  1,  0,  1, -2, -3,  2,\n",
            "         1,  2])\n",
            "tensor([[[ 1.7467,  2.0721,  2.0378,  ...,  0.4110,  0.4795,  0.4966],\n",
            "         [ 2.5002,  2.8256,  2.5858,  ...,  1.0104,  1.0446,  0.9419],\n",
            "         [ 2.3975,  2.5858,  2.0892,  ...,  1.1302,  1.0617,  1.1987],\n",
            "         ...,\n",
            "         [-0.8391, -0.6507, -0.5651,  ...,  0.3596,  2.0892,  2.1063],\n",
            "         [-0.4624, -0.2740, -0.4452,  ..., -0.1199,  1.3186,  1.5070],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.8032,  2.1183,  2.1359,  ...,  0.9454,  1.0329,  0.9979],\n",
            "         [ 2.5385,  2.9237,  2.8361,  ...,  1.7157,  1.7682,  1.6457],\n",
            "         [ 2.5035,  2.8361,  2.5385,  ...,  1.8207,  1.7857,  1.9258],\n",
            "         ...,\n",
            "         [-0.1751,  0.1926,  0.1225,  ...,  0.9104,  2.6436,  2.5560],\n",
            "         [ 0.2976,  0.6478,  0.3501,  ...,  0.5777,  2.0308,  2.1359],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.7603,  2.1264,  2.2484,  ...,  1.4118,  1.4815,  1.4118],\n",
            "         [ 2.5098,  2.9107,  2.9978,  ...,  2.3355,  2.3878,  2.2135],\n",
            "         [ 2.5272,  2.9455,  2.8932,  ...,  2.4575,  2.4227,  2.5098],\n",
            "         ...,\n",
            "         [ 1.1503,  2.3007,  2.4924,  ...,  2.0566,  3.1024,  2.8410],\n",
            "         [ 1.0806,  2.0741,  2.3529,  ...,  1.5686,  2.5621,  2.5098],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-4, -7,  8,  3, -4, -4, -3, -3, -4,  1,  4, -2,  5,  4,  5,  4,  0,  5,\n",
            "         0, -1,  1,  7,  2,  7,  2, -1, -2, -2,  0,  3,  1, -4,  5,  3, -3, -1,\n",
            "        -3, -1,  0, -3, -2, -1, -5,  1, -7,  3, -7,  8,  7, -1, -6, -2,  1, -7,\n",
            "         0, -8,  5, -5,  7, -7,  6, -4, -4,  6,  4, -4,  3,  0,  6,  6,  3,  1,\n",
            "        -2,  0, -4, -3,  2,  2, -4, -2,  3,  1, -3, -4,  2, -1, -2,  0,  2, -5,\n",
            "        -1,  1,  6, -3,  0,  1,  0,  3,  0,  0,  2,  0, -3, -1,  0, -5,  8, -7,\n",
            "         1,  0,  7, -5,  1, -1, -2, -6,  1,  3,  0,  1, -3,  3, -5, -5, -3,  2,\n",
            "        -3, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -0.5822, -0.6165,  ..., -0.3082,  1.5926,  1.6440],\n",
            "         [-3.0825, -0.0342, -0.3767,  ..., -0.6850,  0.8905,  1.1645],\n",
            "         [-3.0311,  0.0514, -0.2397,  ..., -1.1302,  0.0000,  0.4452]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762,  0.5077,  0.2626,  ...,  0.5077,  2.3459,  2.2584],\n",
            "         [-2.4335,  1.0154,  0.5252,  ...,  0.2101,  1.8032,  1.9608],\n",
            "         [-2.5210,  0.9454,  0.6127,  ..., -0.3326,  0.8578,  1.2605]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  0.9935,  1.1155,  ..., -0.1394,  1.2375,  1.1678],\n",
            "         [-1.6732,  0.9412,  1.0632,  ..., -0.4880,  0.7495,  0.8889],\n",
            "         [-2.0218,  0.1394,  0.1394,  ..., -1.0458, -0.1046,  0.1220]]])\n",
            "tensor([-2, -2, -1, -2, -2, -1, -7, -4,  0,  0,  5, -1,  0, -3, -4,  7,  6,  5,\n",
            "         3, -2, -2, -1,  7, -1,  1, -2,  1,  2,  1,  3, -1, -7,  2,  2,  1, -2,\n",
            "        -3,  2,  0,  1, -3,  0, -2, -3, -6,  3,  1,  5,  6,  4, -7, -5, -3, -2,\n",
            "        -1, -9,  2, -5,  3, -5,  1, -2,  3, -1, -3, -3, -1,  1,  5,  3,  5,  6,\n",
            "         6, -5,  4,  4, -7, -1, -1, -4,  2,  4, -6, -4,  7, -5,  2,  2,  4, -7,\n",
            "         3, -2, -1, -3, -6, -1, -5, -2, -2, -5,  0, -1, -6,  1,  0,  1, -1, -4,\n",
            "        -2,  1,  8, -2, -6,  5,  3, -1, -6, -6,  3,  5, -8,  2, -5, -7,  1,  5,\n",
            "        -1, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.7400,  2.9455,  2.4146,  ...,  0.9932,  0.9590, -1.8666],\n",
            "         ...,\n",
            "         [-0.7192, -0.2911, -0.2226,  ..., -0.0342,  1.7467, -0.9076],\n",
            "         [-0.0856,  0.1712, -0.1027,  ..., -0.4281,  1.0446, -1.4214],\n",
            "         [ 0.0856,  0.2397, -0.0856,  ..., -0.9419,  0.1712, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.7486,  3.0812,  2.7661,  ...,  1.5406,  1.5406, -1.2780],\n",
            "         ...,\n",
            "         [-0.1401,  0.4727,  0.3501,  ...,  0.3501,  2.1359, -0.5952],\n",
            "         [ 0.5602,  0.9804,  0.5427,  ...,  0.1050,  1.5931, -0.9279],\n",
            "         [ 0.5952,  0.8578,  0.4902,  ..., -0.4902,  0.6653, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.6492,  3.1198,  2.9804,  ...,  1.9346,  1.9346, -0.7320],\n",
            "         ...,\n",
            "         [ 1.0283,  2.4575,  2.5272,  ...,  1.3420,  2.4401, -0.3486],\n",
            "         [ 1.1852,  2.3007,  2.4052,  ...,  0.9412,  1.9695, -0.5926],\n",
            "         [ 0.9412,  1.3769,  1.3595,  ...,  0.1743,  1.1503, -1.2549]]])\n",
            "tensor([-1,  0,  1,  2, -7, -2, -2, -6, -3,  3,  2, -1,  2,  0, -2,  1,  0, -3,\n",
            "        -2,  4, -4, -1, -1,  1,  7,  0, -2,  4,  1,  5,  8, -6,  0, -3, -3, -5,\n",
            "        -1,  6, -7,  1, -1,  4, -6,  2, -3,  6,  0,  4,  7, -3,  0, -6,  2, -8,\n",
            "         0, -5,  7, -2,  0, -2, -1, -9,  3,  4,  0,  3, -2,  1,  2,  3,  0,  1,\n",
            "         6, -9,  0, -3, -5, -6,  2,  0,  0,  5, -4, -9,  4, -3,  2,  5,  2,  1,\n",
            "        -3,  1,  7, -3, -3, -2, -4, -1,  1,  4,  0, -7, -6,  6, -1, -5,  8, -6,\n",
            "        -2,  4,  9, -5, -3,  6, -5, -6, -5,  1,  1,  3, -8, -2, -5,  0, -3, -1,\n",
            "        -4,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -1.0789,  1.0789,  1.4042],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.3254,  1.7467,  1.9351],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.4624,  1.7638,  2.1235],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  1.4214,  3.1852,  3.2366],\n",
            "         [-3.0825, -2.9626, -3.1852,  ...,  1.0104,  2.5002,  2.7571],\n",
            "         [-3.0311, -2.8770, -3.0653,  ...,  0.4110,  1.4899,  2.0036]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.6478,  1.5581,  1.8908],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.2626,  2.4160,  2.6085],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.0854,  2.4510,  2.8186],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  1.8382,  3.6239,  3.5889],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  1.5931,  3.1162,  3.2913],\n",
            "         [-2.5210, -2.2584, -2.4860,  ...,  0.9104,  2.0308,  2.5035]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.3137,  1.9869,  2.3181],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.6449,  2.9978,  3.1895],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.3420,  3.0501,  3.4161],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  2.8410,  3.9564,  3.7473],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  2.3704,  3.4684,  3.5381],\n",
            "         [-2.0218, -1.6383, -1.5163,  ...,  1.5512,  2.4575,  2.7887]]])\n",
            "tensor([-5,  0,  1, -1, -3, -2, -3, -4, -6,  1,  1, -4,  3, -3, -4,  7,  4,  0,\n",
            "        -2,  3, -3,  1,  7,  2,  0, -9, -6,  0, -2,  5,  0, -3,  7,  2,  1,  1,\n",
            "         2,  1,  0,  1,  2,  0, -5, -2, -4,  3, -6,  8,  7, -1, -1, -5,  1, -4,\n",
            "         4, -9,  8, -6,  0, -9,  1, -4, -5,  6,  2,  3,  0,  2,  2,  4,  0,  1,\n",
            "         6, -8, -5,  6, -2,  1, -3,  0,  1,  3, -5, -9,  6, -4, -5, -2,  5, -1,\n",
            "        -2, -1,  1, -8, -9,  1, -2,  5,  6,  2,  2, -2, -5, -1, -9, -3,  6, -9,\n",
            "         0,  7,  6, -7,  0, -1,  3,  0,  2, -4,  2,  7,  0, -3, -4, -3, -5,  0,\n",
            "        -3,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.6679,  0.7364,  ..., -1.0446, -0.9932, -1.3700],\n",
            "         [-0.4281, -0.0342,  0.1884,  ..., -0.6165, -0.2226, -0.2397],\n",
            "         ...,\n",
            "         [-3.5619, -3.0825, -2.9283,  ..., -1.2159,  0.4795,  0.3425],\n",
            "         [-3.0825, -2.7228, -2.6715,  ..., -1.7125, -0.3254, -0.2569],\n",
            "         [-3.0311, -2.2776, -2.0207,  ..., -2.3290, -1.2844, -0.8562]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.7878,  1.2955,  ..., -0.0875,  0.0350, -0.3676],\n",
            "         [-0.4202,  0.3326,  0.9454,  ...,  0.3852,  0.8403,  0.8053],\n",
            "         ...,\n",
            "         [-2.9762, -2.5210, -2.5910,  ..., -0.4202,  1.2255,  1.0154],\n",
            "         [-2.4335, -2.0833, -2.2059,  ..., -0.8053,  0.5777,  0.5777],\n",
            "         [-2.5210, -1.8032, -1.5756,  ..., -1.5581, -0.4727, -0.0875]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.5403,  0.5752,  ..., -0.5577, -0.4357, -0.7495],\n",
            "         [-0.3660,  0.2440,  0.4880,  ..., -0.3137,  0.3137,  0.2789],\n",
            "         ...,\n",
            "         [-1.6732, -0.4880, -0.4009,  ..., -0.2092,  0.8715,  0.5054],\n",
            "         [-1.6732, -0.7320, -0.4009,  ..., -0.5577,  0.4009,  0.2614],\n",
            "         [-2.0218, -1.5163, -1.1155,  ..., -1.3769, -0.5054, -0.4357]]])\n",
            "tensor([ 0, -7,  3,  0, -1,  5,  1, -3, -4, -2,  5,  2,  2,  0, -4,  8,  1, -4,\n",
            "         3,  2,  1,  0,  5,  0,  2, -2, -4,  2,  2,  4,  2, -1,  0,  0, -1,  2,\n",
            "        -4, -3, -1,  1,  1, -1, -5, -3,  0, -1, -4,  7,  3,  1, -5,  0,  1, -1,\n",
            "        -1, -1,  2, -4,  6, -7, -1, -5,  3,  6,  1,  4,  3,  7,  6,  0,  4, -2,\n",
            "        -1, -7,  2,  0,  0, -6,  0,  1,  3, -3,  3, -1,  5, -3,  2,  2,  6, -5,\n",
            "         2,  4,  1, -6, -7,  1, -7,  5,  7,  0,  2, -1, -2,  2, -7,  3,  3, -4,\n",
            "        -1,  0,  2, -2, -5, -1, -2,  1, -5, -6,  5,  8, -3, -2, -7,  1,  1,  0,\n",
            "        -2,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.9590, -0.7192, -0.6336],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.2911, -0.0685, -0.1712],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.0856, -0.0342, -0.0342],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.0342,  1.5926,  1.9693],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.3767,  1.1987,  1.6611],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.6828, -0.4202, -0.3852],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.1576,  0.4027,  0.2801],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.3501,  0.4377,  0.4377],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.1401,  1.7682,  2.2584],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  0.1401,  1.7332,  2.0833],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.3834, -0.1394, -0.1046],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.6100,  0.8540,  0.6797],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  0.8192,  0.9063,  0.8540],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.9063,  1.6732,  2.2658],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.9063,  1.9869,  2.2309],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -7, -1,  7, -3, -1,  0, -6, -2,  6, -3,  0,  3, -2,  3,  6, -1, -2,\n",
            "         1,  2, -2, -1, -1,  7,  7, -7, -2, -2, -5, -1,  7, -6,  3, -1, -5,  1,\n",
            "         2,  4, -2,  5,  2,  4,  2,  5, -4,  2, -3,  2,  8,  5, -6,  0, -5,  1,\n",
            "         1, -2,  1, -6,  3, -1,  1, -8, -1,  1, -2, -2, -3,  1, -1,  1,  8, -1,\n",
            "         2, -3,  2,  6, -5,  1, -2,  3,  4, -2,  2,  0,  4, -5,  0, -2,  6,  1,\n",
            "         4,  3,  4,  0,  0,  0, -1,  0, -1,  0,  3, -8, -1,  8, -6, -6,  8, -3,\n",
            "        -7,  3,  6, -2,  1,  2,  2, -7, -5,  3,  5,  4, -2, -4, -7, -6, -2,  8,\n",
            "        -8,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -1.0617, -0.9419, -0.9247],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.3596, -0.2740, -0.3939],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.1370, -0.1712, -0.0514],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.5173, -0.6850, -0.5822],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.0700,  0.0525,  0.0525],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.7353,  0.8228,  0.7353],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.8403,  0.8403,  0.9979],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -1.6457,  0.1225,  0.0875],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  1.0632,  1.1852,  1.1503],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  2.0044,  2.0915,  1.9346],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  2.0392,  2.0392,  2.1438],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.0523,  1.1503,  0.9760],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 2, -5,  3,  4, -2,  1, -5, -9, -9,  5,  0, -5,  7,  6,  0,  4,  7,  4,\n",
            "        -3,  0, -3,  1,  7,  6,  5, -5,  2,  7, -4,  0,  2, -1,  6, -1, -1, -4,\n",
            "        -5, -1, -8,  4, -1, -4, -1,  2, -5,  5, -5, -1,  1, -2,  0, -7,  0, -1,\n",
            "         0, -1,  0, -3,  1, -1,  0, -3,  3,  2, -3,  5, -1, -1,  8, -1,  1,  4,\n",
            "         4,  0,  0,  2,  1, -5, -1, -2,  8,  4, -2, -3,  5,  1, -7,  0,  6, -4,\n",
            "        -1, -3,  5, -7, -7,  0,  0,  6,  1,  0, -1, -6,  0,  1, -9,  3,  8,  0,\n",
            "        -4,  0,  4, -4,  1,  0,  0, -6, -6,  2,  5,  5, -4,  2, -5,  0,  0,  1,\n",
            "        -2,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  1.6269,  3.4078,  3.4592],\n",
            "         [-3.0825, -2.9626, -3.1852,  ...,  1.2159,  2.7057,  2.9455],\n",
            "         [-3.0311, -2.8770, -3.0653,  ...,  0.6679,  1.7810,  2.2605]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  2.1359,  3.9216,  3.8690],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  1.8732,  3.3789,  3.5364],\n",
            "         [-2.5210, -2.2584, -2.4860,  ...,  1.2430,  2.3985,  2.8536]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  3.2244,  4.3224,  4.0959],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  2.8061,  3.8519,  3.8519],\n",
            "         [-2.0218, -1.6383, -1.5163,  ...,  2.0044,  2.9804,  3.1895]]])\n",
            "tensor([ 1, -6,  0,  6, -4, -4, -1, -1, -3, -1, -2, -2, -1, -2,  1,  3,  4, -4,\n",
            "         3,  4,  2,  5,  2, -1,  6,  0, -4, -2,  1,  3,  7, -1,  0, -3, -7, -4,\n",
            "        -6,  3,  0,  1,  2, -2,  2,  5, -4,  5, -6,  1,  4,  0, -3, -4,  0, -5,\n",
            "         1, -1,  3, -7,  0, -7, -2, -1, -6,  3, -1,  5,  0,  4,  2,  3, -1,  5,\n",
            "         2, -9,  1,  2, -1,  0, -5,  5,  2, -2, -2, -6,  8,  1,  2,  3,  3, -3,\n",
            "        -3, -2, -1, -9, -2, -8, -4,  6,  4, -5,  1, -7, -1,  0, -1, -2,  0, -3,\n",
            "         1,  3,  6, -8,  3,  2, -5, -2,  0,  0,  2,  6, -4, -5, -7,  1, -1,  0,\n",
            "         0,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.6715,  1.5755,  1.0617,  ...,  0.0514, -0.2397,  0.0514],\n",
            "         ...,\n",
            "         [-1.0960, -0.9076, -0.7364,  ..., -0.2911,  1.3871,  1.6611],\n",
            "         [-0.4795, -0.3939, -1.3186,  ..., -1.0789,  0.7877,  1.2501],\n",
            "         [-0.7192, -0.9247, -1.5755,  ..., -1.0104,  0.0342,  0.3596]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.4685,  1.4006,  1.0679,  ...,  0.5427,  0.2801,  0.5427],\n",
            "         ...,\n",
            "         [-0.6653, -0.2801, -0.2801,  ..., -0.1751,  1.5056,  1.6807],\n",
            "         [ 0.0175,  0.3151, -0.7528,  ..., -0.8403,  1.0679,  1.4531],\n",
            "         [-0.4202, -0.4552, -1.1029,  ..., -0.8403,  0.2626,  0.5602]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.1786,  1.2200,  1.1329,  ...,  0.6623,  0.5054,  0.8540],\n",
            "         ...,\n",
            "         [ 0.3834,  1.4292,  1.6209,  ...,  0.4532,  1.4292,  1.4466],\n",
            "         [ 0.3660,  1.2026,  0.6449,  ..., -0.3312,  1.0980,  1.3072],\n",
            "         [-0.3312, -0.2789, -0.6100,  ..., -0.4357,  0.4183,  0.4183]]])\n",
            "tensor([-2, -3,  4,  2, -2, -3, -4, -3, -5,  6,  3, -4, -2,  3, -3,  0,  0,  1,\n",
            "         5,  0, -3,  3,  4,  4,  2, -4,  2,  7,  2, -3,  4, -1,  6,  2, -6, -2,\n",
            "        -2,  0, -4, -3, -3,  2, -4,  1,  0,  4, -7,  8,  6, -1, -6,  0,  1,  0,\n",
            "         6, -8,  8, -5,  6, -3,  5, -4,  3,  4,  0,  0, -2,  8,  4,  0,  2,  2,\n",
            "         3, -6, -4,  2,  1,  2,  3,  0,  1,  4,  0, -2,  6, -5, -6,  6,  6, -6,\n",
            "        -3, -3,  8, -8, -1, -7, -2, -1,  1,  2,  3, -9,  1, -1, -3,  0,  5, -8,\n",
            "        -3,  0,  6, -7, -4,  6, -4, -2,  0, -3,  2,  4, -4,  1, -8, -4, -5,  4,\n",
            "        -2,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -1.9008, -1.7467,  ..., -0.6679,  1.1816,  1.3529],\n",
            "         [-3.0825, -1.4899, -1.6782,  ..., -1.0275,  0.4966,  0.7192],\n",
            "         [-3.0311, -1.4385, -1.5755,  ..., -1.5584, -0.4281,  0.0171]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -1.2605, -1.2605,  ..., -0.5252,  1.3305,  1.3655],\n",
            "         [-2.4335, -0.7528, -1.0329,  ..., -0.7178,  0.8053,  0.9804],\n",
            "         [-2.5210, -0.8403, -0.9804,  ..., -1.4006, -0.2451,  0.2626]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  0.8192,  1.1155,  ...,  0.5229,  1.6383,  1.5163],\n",
            "         [-1.6732,  0.6275,  0.9237,  ...,  0.1394,  1.1503,  1.1678],\n",
            "         [-2.0218, -0.2789, -0.0523,  ..., -0.7146,  0.2092,  0.4532]]])\n",
            "tensor([-6, -4,  2,  2, -8, -4, -3, -3, -6,  4,  1,  1,  2,  6,  5,  2,  1,  2,\n",
            "        -2,  5, -5,  4,  8,  2,  0, -5,  2,  0,  0,  5,  1,  1,  5,  4, -4,  3,\n",
            "        -3, -1, -8,  5,  0, -2, -1, -1, -4, -2, -1,  8,  5, -3, -4,  0, -4, -5,\n",
            "        -2, -6,  4, -1,  7, -2, -1, -2,  3,  2,  2, -1, -4,  0,  0,  6,  7,  2,\n",
            "         1, -3,  3,  6,  0, -5, -3,  0,  2,  3, -3, -5, -1, -3,  2, -1,  2, -6,\n",
            "         1,  3,  7, -8,  0, -8, -5, -1, -2,  4, -1, -5,  1,  8, -8, -1,  3, -8,\n",
            "        -3,  3,  4, -3,  1,  8, -5, -7,  0, -1,  6,  1,  1, -4, -4, -1, -5,  1,\n",
            "        -7, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.5516, -2.4660, -2.2091,  ..., -0.9076,  0.8049,  0.8049],\n",
            "         [-2.0550, -1.9522, -2.1063,  ..., -1.4214,  0.1199,  0.2911],\n",
            "         [-2.0036, -1.8495, -1.9522,  ..., -2.0550, -0.8562, -0.3082]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.0154, -0.7528, -0.6828,  ...,  0.5777,  2.3109,  2.2234],\n",
            "         [-0.4727, -0.2101, -0.5077,  ...,  0.1926,  1.7507,  1.9083],\n",
            "         [-0.6653, -0.4027, -0.5252,  ..., -0.5252,  0.6828,  1.2430]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.9237,  0.1394,  0.4880,  ...,  0.2266,  1.2200,  0.9586],\n",
            "         [-0.8715,  0.0523,  0.3312,  ..., -0.2266,  0.8192,  0.7843],\n",
            "         [-1.2200, -0.8540, -0.6449,  ..., -1.0458, -0.0174,  0.2789]]])\n",
            "tensor([-2,  0,  7,  0, -7,  0,  1, -9,  0, -1,  3, -3,  6,  6, -3,  6,  7, -4,\n",
            "         3,  3, -2,  5,  5, -1,  0,  0, -1,  4, -2, -1,  2, -2,  1, -3, -6, -3,\n",
            "         1,  5, -2, -1,  2,  0, -1,  4,  0,  4, -2,  5,  6, -4, -1, -3, -3, -3,\n",
            "         0,  0,  2, -6,  3, -3, -2, -2, -5,  3,  1,  3,  3,  7,  1,  1,  7,  1,\n",
            "         1, -8, -3, -3, -5, -6,  3,  0,  4,  3, -3, -4,  3, -6,  0,  6,  7, -5,\n",
            "        -4, -2,  6, -4, -4,  1, -6,  6,  4,  3, -4, -1, -5,  3, -2,  2,  8, -9,\n",
            "        -4,  6,  0,  1,  3,  3, -2, -3, -5, -1,  4,  1,  0, -1, -3, -8, -6,  8,\n",
            "        -4,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.6611,  1.9008,  1.5070,  ..., -0.1370, -2.0378, -2.0892],\n",
            "         [ 1.1816,  1.4042,  0.7364,  ..., -0.1370, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.5173, -2.2262, -2.1235,  ..., -0.5994, -0.9590, -0.9076],\n",
            "         [-1.9693, -2.0378, -2.2947,  ..., -1.0275, -1.6611, -1.4214],\n",
            "         [-2.0207, -1.9865, -2.2605,  ..., -1.5412, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 1.4006,  1.6457,  1.4181,  ...,  0.2276, -1.4531, -1.5231],\n",
            "         [ 1.1555,  1.4706,  1.0504,  ...,  0.1225, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.0308, -1.5756, -1.6632,  ..., -0.7178, -0.5427, -0.5952],\n",
            "         [-1.3831, -1.2605, -1.6807,  ..., -0.9454, -1.0854, -0.9279],\n",
            "         [-1.6632, -1.4181, -1.6282,  ..., -1.5406, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.6972,  0.9063,  0.8540,  ...,  0.0174, -0.8715, -0.9935],\n",
            "         [ 0.8192,  1.1329,  1.0632,  ..., -0.0174, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.0283,  0.1917,  0.5577,  ..., -0.3312, -0.1220, -0.3486],\n",
            "         [-1.0283, -0.1220,  0.1743,  ..., -0.5926, -0.5926, -0.5926],\n",
            "         [-1.3420, -0.9760, -0.7146,  ..., -1.3246, -1.4641, -1.2549]]])\n",
            "tensor([-1, -9, -1,  7, -5, -1,  2, -8, -3, -2,  4,  2,  2,  3,  3,  3,  1,  3,\n",
            "         6,  3,  2,  4,  5,  2,  6, -3, -5,  3, -1, -1,  1,  1,  4,  5, -2, -4,\n",
            "         2,  2, -2,  4,  3,  3, -3, -2, -1,  1, -5,  4, -1,  4, -4,  0,  1, -1,\n",
            "         3, -7, -1,  2,  0, -7,  7, -3,  3, -1, -4,  1,  2,  1,  7, -2,  5,  0,\n",
            "         1, -4,  3,  4, -3,  1,  1, -4,  8, -4, -6, -9,  6, -8, -7,  0,  8, -2,\n",
            "         3, -4,  2, -6, -7,  0, -5,  7,  1, -2,  2, -1,  2,  5, -5,  3,  1, -6,\n",
            "        -4,  0,  2, -6,  0,  2, -3,  1,  1, -6,  3,  1, -6,  2, -7, -5, -2, -1,\n",
            "        -8, -4])\n",
            "tensor([[[ 1.7981,  2.3632,  2.9626,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.5687,  3.3051,  3.8017,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.4831,  3.2880,  3.4250,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.2430,  1.9083,  2.8186,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.0133,  2.9587,  3.9391,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.9783,  3.0812,  3.8165,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.0283,  1.7778,  2.8235,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.8475,  2.8061,  3.9913,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.8998,  3.1198,  4.1481,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -2,  8,  7, -2,  3, -2, -8, -2,  0, -3, -5,  4,  4,  2,  0,  0,  4,\n",
            "         6, -1, -3,  4,  1,  5,  6, -7, -7, -1, -4, -3,  8, -5,  1,  4, -5, -3,\n",
            "        -5,  5, -8,  5,  0, -4, -1, -3,  0, -2,  0,  2,  6, -4, -4,  0, -3, -8,\n",
            "         7, -2,  8,  0,  5, -7,  6, -4, -1, -1,  2,  5, -1,  3,  5,  2,  2, -2,\n",
            "         5, -8,  4, -3, -6, -4, -4,  3,  7,  1, -1, -9,  5, -4, -7,  1,  7, -7,\n",
            "        -4,  3,  1, -2, -9,  1, -3,  6,  5, -2, -3, -5, -6,  3, -5,  3, -1, -3,\n",
            "         0,  1,  5, -2, -5,  7, -3, -3,  0, -1, -1,  7, -6, -3, -4, -6, -1,  0,\n",
            "        -7,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.5822,  1.0960,  1.4042],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.1199,  0.4281,  1.0789],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.4110,  1.3871,  1.3700],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.9761,  0.4795,  0.6850],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.6440, -0.5651, -0.0514]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.5056,  1.9958,  2.2409],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.8053,  1.3480,  1.9433],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.3852,  2.1534,  2.0833],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.1050,  1.4006,  1.5406],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.8053,  0.3501,  0.8403]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.7081,  2.5272,  2.8235],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  0.6972,  1.6035,  2.4401],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.2440,  0.9063,  0.5926],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -0.8017,  0.2789,  0.2614],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.6558, -0.7146, -0.5054]]])\n",
            "tensor([-6, -5,  5,  1, -6,  5,  0, -6, -6, -1, -2, -2,  6,  2, -4,  5,  2,  3,\n",
            "         1, -3,  4,  0,  2,  2,  4, -6, -5,  5, -1,  3,  1, -3,  1,  4, -7,  3,\n",
            "         0, -1, -5,  2,  5,  1,  2,  2, -6,  3, -1,  1,  8,  4, -2, -4, -5, -8,\n",
            "         3, -2,  6, -2,  4, -5, -1, -5,  2,  1,  4,  5, -2,  6, -1,  1,  3,  4,\n",
            "        -2, -9, -4, -2, -4,  3,  0,  2,  7, -2, -2, -1,  7, -5,  2,  2, -1,  2,\n",
            "        -4,  0, -1, -6, -8, -8, -3, -2, -2, -2, -5, -6,  3,  5, -5,  0,  7, -5,\n",
            "        -4,  4,  4, -7,  3,  1, -6, -1,  2, -2,  8,  1, -7,  0,  0,  1, -1,  2,\n",
            "        -5, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ...,  1.3186,  1.4042,  1.4556],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  1.8152,  1.8495,  1.7296],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  2.0550,  1.9180,  2.0378],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.8220,  0.8049,  0.7877],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -1.2159, -0.1712, -0.0856],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.8152, -1.1987, -0.8734]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  1.7332,  1.8382,  1.8557],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  2.3985,  2.4335,  2.3109],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  2.6436,  2.5385,  2.6436],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.0525,  1.7332,  1.6106],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.1751,  0.9279,  0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.8403, -0.1751,  0.1225]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  2.5272,  2.6318,  2.6318],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  3.4161,  3.5033,  3.3464],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  3.5381,  3.4858,  3.5730],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.2614,  1.2549,  0.9586],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -0.1394,  0.5229,  0.3486],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -0.9935, -0.4532, -0.3660]]])\n",
            "tensor([ 1, -3,  7, -1,  0,  3, -2, -3,  0,  6, -4, -3,  0, -1,  0,  0,  5,  0,\n",
            "         2, -3,  0, -1,  4,  8, -2, -4,  2,  2,  4, -3,  5, -2,  1,  4,  2,  3,\n",
            "         0,  5, -5,  4,  1,  5, -7,  1, -1,  1, -4,  6,  6, -4, -4, -8,  3, -2,\n",
            "         0, -4,  4, -3,  9, -7, -1, -7,  2,  6, -1, -3,  1,  0,  4,  0,  8, -1,\n",
            "         0,  0, -5,  3, -2,  2, -5,  4,  5, -4, -5,  0,  0, -5,  2,  5,  5, -5,\n",
            "        -3, -1,  8, -7, -1,  0, -3,  0,  5, -5, -1, -9, -4,  7, -8,  1, -1, -8,\n",
            "        -7,  6,  1, -3,  1,  2,  1,  2, -6, -6,  8,  5, -5, -5,  1,  0, -4,  6,\n",
            "        -1, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.0482, -3.0653, -3.1167,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-2.6030, -2.5516, -2.6886,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-2.5173, -1.9693, -1.6611,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.5385, -2.4860, -2.6611,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.0833, -1.9258, -1.9783,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.1884, -1.3831, -0.7003,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.2723, -0.5577, -0.4009,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.3769, -0.5926,  0.0871,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-1.7952, -0.6623,  0.6797,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -9,  3,  1, -2,  5, -5, -1,  0,  0, -2, -4,  5, -3,  4,  8,  0,  3,\n",
            "         5, -3,  1,  7,  1,  4,  6,  0, -7,  4, -3,  5,  5, -4, -1,  2,  2,  1,\n",
            "        -1,  6, -9,  2,  2, -3, -4, -1, -8, -1, -8,  2,  3, -4, -3, -9, -4, -6,\n",
            "         6, -2,  6, -4,  3, -7,  1,  0, -2,  4,  2,  0,  2,  0,  5,  4, -1, -2,\n",
            "         5, -7,  0, -1, -7, -5, -2,  0,  1,  3, -3,  0, -1, -4, -7, -2,  1, -2,\n",
            "         1, -4,  2, -8, -5,  1,  0,  2, -1, -1,  1,  0,  1,  6, -2,  0,  6, -4,\n",
            "        -8,  3,  5, -6, -4,  7,  2, -6,  2,  3,  5,  5, -5, -4, -6, -8, -3,  1,\n",
            "        -2,  0])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.6715, -2.4831, -2.4831,  ..., -1.8837, -0.1712, -0.1712],\n",
            "         [-1.8666, -1.6954, -1.8837,  ..., -2.1235, -0.7364, -0.5651],\n",
            "         [-1.7810, -1.7296, -1.8666,  ..., -2.7571, -1.7125, -1.2672]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.5035, -2.2234, -2.3985,  ..., -1.8557, -0.1576, -0.2626],\n",
            "         [-1.5756, -1.3305, -1.6807,  ..., -1.9783, -0.6127, -0.5252],\n",
            "         [-1.5756, -1.4706, -1.7157,  ..., -2.6085, -1.4881, -1.0679]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.0632, -0.0174,  0.0523,  ..., -0.6100,  0.4009,  0.1220],\n",
            "         [-0.6797,  0.2266,  0.4532,  ..., -0.8540,  0.0523,  0.0000],\n",
            "         [-0.9412, -0.6972, -0.5403,  ..., -1.6558, -0.7495, -0.5752]]])\n",
            "tensor([-5, -7,  4,  2,  1,  2, -7, -8, -3,  0, -3,  0,  7, -1,  0,  0,  8, -2,\n",
            "        -3, -3, -3, -1,  1,  0,  6, -2, -6,  3, -1,  3,  1, -6,  5, -4, -5, -2,\n",
            "         2,  1, -8,  2, -1,  4, -5, -4, -8,  6,  1,  1,  6, -1, -1, -3, -1,  1,\n",
            "         6, -1,  7,  2,  5, -6,  4, -7, -1,  5,  1,  1, -1,  3,  6,  6,  0,  5,\n",
            "         6, -6, -4,  1, -7,  2,  0, -1,  5,  2,  1, -6,  6, -4, -6, -3,  5, -7,\n",
            "         2, -2,  4, -8, -9, -5, -6,  1,  6, -4, -4, -4, -2,  8, -4, -5,  3, -7,\n",
            "        -7,  3,  6, -2, -6,  0, -2, -5, -2, -4,  1,  1, -5,  3, -1, -8, -1,  0,\n",
            "        -7, -2])\n",
            "tensor([[[-1.0104,  1.5926,  1.7296,  ..., -0.0171,  0.0856,  0.2397],\n",
            "         [-0.2740,  2.4146,  1.9180,  ...,  0.5137,  0.6336,  0.6336],\n",
            "         [-0.4281,  1.9865,  1.1645,  ...,  0.1199,  0.2055,  0.8049],\n",
            "         ...,\n",
            "         [-3.5619, -1.0789, -1.3700,  ...,  0.1884,  1.7296,  1.9008],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  1.6457,  1.8557,  ...,  0.4377,  0.4902,  0.5427],\n",
            "         [-0.3501,  2.5560,  2.2059,  ...,  1.1380,  1.2430,  1.1380],\n",
            "         [-0.4202,  2.2059,  1.6106,  ...,  0.6828,  0.8053,  1.3130],\n",
            "         ...,\n",
            "         [-2.9762, -0.3151, -0.8228,  ...,  0.4202,  1.9433,  1.9958],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  1.0458,  1.4118,  ...,  0.4880,  0.5229,  0.6972],\n",
            "         [-0.3486,  1.8998,  1.6906,  ...,  1.1329,  1.3420,  1.4641],\n",
            "         [-0.3660,  1.9346,  1.4815,  ...,  0.6449,  0.9063,  1.6209],\n",
            "         ...,\n",
            "         [-1.6732,  1.2026,  1.0109,  ...,  1.2723,  2.0392,  2.0044],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -2,  3,  4, -1, -4,  2, -2, -6,  4,  0, -5,  5,  6,  3,  6,  8,  4,\n",
            "         4,  4,  2,  3,  0, -1,  6, -4, -4,  6, -5,  0,  2,  0,  2,  4, -2, -1,\n",
            "        -6, -2, -7, -3,  1, -2,  2, -2, -3,  6, -5,  0,  4,  3, -8, -5, -4, -7,\n",
            "         3, -3,  7, -5,  3, -6, -1,  0, -1,  8, -1, -3,  1,  4,  2,  5, -1, -2,\n",
            "        -1, -4,  4,  0, -1,  3, -4,  0,  4, -4,  3, -6, -1, -7, -4, -3,  4, -2,\n",
            "        -3,  4,  5, -3, -3, -5,  1,  5,  1, -4, -6, -3, -4,  3, -7, -1,  2, -4,\n",
            "         1,  1,  7, -2, -3,  0, -1, -3, -2, -5,  0,  2,  1,  0, -4,  0, -4,  4,\n",
            "        -6, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.5994,  ..., -1.8152, -1.7981, -1.5926],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.7810,  ..., -0.9419,  0.7021,  0.6507],\n",
            "         [-3.0825, -2.9626, -1.0617,  ..., -1.3357,  0.1027,  0.2226],\n",
            "         [-3.0311, -2.8770, -1.0789,  ..., -2.1235, -0.6679, -0.2569]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225,  0.0000,  ..., -1.0679, -0.9629, -0.7878],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -0.9979,  ..., -0.2801,  1.3480,  1.1204],\n",
            "         [-2.4335, -2.1534, -0.0525,  ..., -0.4902,  0.9104,  0.9629],\n",
            "         [-2.5210, -2.2584, -0.1401,  ..., -1.3130,  0.0525,  0.5077]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000,  0.2963,  ..., -0.4706, -0.3834, -0.2266],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  1.2549,  ...,  0.6797,  1.6906,  1.2898],\n",
            "         [-1.6732, -0.7320,  1.8126,  ...,  0.3312,  1.2723,  1.2200],\n",
            "         [-2.0218, -1.6383,  0.6100,  ..., -0.6623,  0.5403,  0.7843]]])\n",
            "tensor([-2, -1,  6,  7, -8, -3, -6, -4, -4,  2, -4, -5,  6,  6,  2,  6,  1, -1,\n",
            "         4,  6, -3,  7,  8, -1,  3, -4, -5,  3, -2, -2,  4, -3,  0, -3, -5,  3,\n",
            "        -1,  4, -9,  4,  4, -2, -5,  1, -6,  0, -4,  8,  6, -1,  0, -3, -6, -8,\n",
            "         3, -1,  1, -6,  2, -9,  2, -6, -5,  2, -1, -1, -4, -1,  5,  7,  4,  1,\n",
            "         2, -7, -3,  2,  0, -4,  2, -4,  5,  0, -2,  0, -1, -7, -3, -3,  1, -5,\n",
            "        -2,  3,  5, -2, -2, -3, -2,  0, -1, -5, -1, -7,  3,  3, -1, -6,  4, -2,\n",
            "        -4,  5,  5, -3, -5,  7,  3, -4, -3, -4,  7,  0, -5, -1,  1, -5,  2,  7,\n",
            "        -5,  5])\n",
            "tensor([[[ 1.6440,  2.1406,  2.1920,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.1063,  2.5002,  2.4488,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.7981,  2.0207,  1.7125,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [ 0.2569,  0.6165,  0.6679,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.2740,  0.5137,  0.4452,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.9932,  1.2159,  1.0789,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.3634,  2.7661,  2.9412,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.6961,  3.0987,  3.2913,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.1359,  2.5210,  2.6961,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.1225,  0.5077,  0.5077,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.2276,  0.6828,  0.5252,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 1.5231,  1.8908,  1.7332,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.4183,  0.3660,  0.5926,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.3137,  0.8889,  0.9935,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.4183,  0.9412,  0.8540,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.0697,  1.2723,  1.4466,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 0.2092,  1.3246,  1.7255,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 1.9172,  2.4227,  2.6492,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -5,  2,  3, -8,  4, -4, -9, -3,  3,  0, -1, -1, -2, -3,  7,  2,  2,\n",
            "        -1,  3, -2,  0,  4,  3,  5, -6,  2, -1, -5,  4, -1,  0,  6,  2, -6,  0,\n",
            "         1, -1, -1,  3,  0,  3,  2,  4, -7,  1, -1,  8,  3, -1, -2, -6, -2,  1,\n",
            "         5, -9,  8,  0,  6,  0,  4, -3, -6, -1,  4, -4,  0,  8,  3,  0,  3,  5,\n",
            "         4, -1, -4,  5, -5, -1, -1,  3,  5, -2,  1, -7,  4,  1, -1, -3, -1, -1,\n",
            "        -4,  4,  1, -5, -3, -4, -5,  7, -2, -5,  2, -9, -2,  3, -2,  3,  1, -8,\n",
            "        -3, -1,  1, -3, -2, -1,  1, -5, -4,  2,  6,  0, -6, -2, -1, -3,  0,  1,\n",
            "        -3, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.7674,  4.0586,  3.7503,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 3.6818,  3.8359,  3.2880,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.8562, -1.4042,  0.0685,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-0.5651, -0.4624,  0.2397,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.3254,  0.5480,  0.7364,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 3.7815,  4.1492,  4.0091,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 3.7815,  4.0791,  3.7465,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.2626, -0.6303,  0.6303,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.0875,  0.2976,  0.8228,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.7353,  1.0329,  1.1380,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 3.7298,  4.0959,  4.0959,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 3.7821,  4.1481,  4.0261,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.4532,  0.8715,  2.3007,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-0.0174,  0.8366,  1.8824,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.3137,  0.7843,  1.2375,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -9,  4,  0, -8, -4, -7, -8, -3,  2,  1, -6,  5,  3, -1,  2,  5,  3,\n",
            "        -3,  5, -1,  4, -1,  7, -1, -7, -2, -1,  1,  5,  1, -4,  8, -2, -7,  0,\n",
            "        -5, -3, -1,  5,  2, -4,  0, -4, -3,  7, -6,  6,  0,  5, -6, -1, -6,  0,\n",
            "         2, -2,  1,  0,  2, -4,  5,  0, -3, -1,  0,  3, -3,  5,  3,  3,  2, -1,\n",
            "         6, -6, -3, -3, -3,  1,  1, -1,  8,  3,  3, -4,  1, -4,  1,  2,  2, -5,\n",
            "         0,  3,  7, -8, -9, -6, -5,  0,  0,  3,  1, -4, -1,  0,  0, -2,  8,  0,\n",
            "        -2,  7,  6, -2, -6,  1, -2, -3, -4, -4, -1,  4, -2, -5, -5,  1, -3,  2,\n",
            "        -2,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 0.5137,  0.7535,  0.5480,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.2911,  0.4110,  0.0685,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.3939, -0.2397, -0.2740,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.1027,  0.2740,  0.0000,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.0514,  0.2226,  0.0342,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 0.8228,  1.1555,  1.0329,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.5777,  0.8929,  0.6828,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.0700,  0.2801,  0.0525,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.4902,  0.8228,  0.4027,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.2976,  0.5777,  0.3501,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.4009,  0.7320,  0.7146,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.2266,  0.5926,  0.5752,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.9586,  2.0566,  2.0741,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 0.9935,  1.9521,  2.0915,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.5403,  0.9412,  1.0458,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -7, -1, -1,  0, -3, -1, -4, -5,  1, -3, -6,  0,  3, -2,  7,  1, -4,\n",
            "         5,  4, -4, -2,  1,  2,  7, -1, -7,  7,  4, -3,  1, -5,  8,  3, -3,  1,\n",
            "        -4,  1, -6,  6, -2,  5, -1,  0, -7,  4,  0,  2,  4,  5, -8, -9, -1, -4,\n",
            "         6, -2,  3, -6,  7, -7,  3,  0, -5,  2,  1, -4, -5,  7,  2,  7,  0,  5,\n",
            "         2, -1, -2,  5, -2,  3, -5,  5,  7,  2,  1, -3,  5, -1, -3,  0,  0,  0,\n",
            "        -2,  3,  3, -9, -9, -4, -2,  7, -1,  3,  1, -7,  0,  1, -1, -4,  5, -4,\n",
            "        -5,  5,  9, -7,  1,  0, -5, -6,  0,  1,  1,  2, -4, -3, -2, -3, -3,  2,\n",
            "        -8,  2])\n",
            "tensor([[[-0.3425, -0.0514, -0.1712,  ..., -2.0207, -2.6030, -2.5345],\n",
            "         [ 0.3082,  0.5994,  0.3425,  ..., -1.3357, -2.0378, -2.0892],\n",
            "         [ 0.1027,  0.3082, -0.2226,  ..., -1.3357, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.1406, -2.5858, -2.2091,  ..., -1.7296, -0.9590, -0.9076],\n",
            "         [-1.6954, -2.1235, -2.2605,  ..., -1.8152, -1.6611, -1.4214],\n",
            "         [-1.9693, -2.2262, -2.3118,  ..., -2.2776, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-0.4727, -0.1751, -0.2101,  ..., -1.6982, -2.1884, -2.1709],\n",
            "         [ 0.2276,  0.5602,  0.4552,  ..., -0.8578, -1.4531, -1.5231],\n",
            "         [ 0.1401,  0.4202,  0.0875,  ..., -0.7878, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.6982, -1.8382, -1.7157,  ..., -1.4006, -0.5427, -0.5952],\n",
            "         [-1.2080, -1.3305, -1.6807,  ..., -1.4356, -1.0854, -0.9279],\n",
            "         [-1.5931, -1.6106, -1.7507,  ..., -2.0308, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.5577, -0.2092, -0.1743,  ..., -1.3246, -1.7778, -1.7952],\n",
            "         [ 0.1569,  0.5054,  0.5403,  ..., -0.3312, -0.8715, -0.9935],\n",
            "         [ 0.1046,  0.4880,  0.3660,  ..., -0.2963, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.7320,  0.1569,  0.4357,  ..., -0.5054, -0.1220, -0.3486],\n",
            "         [-0.6972, -0.0523,  0.1569,  ..., -0.7669, -0.5926, -0.5926],\n",
            "         [-1.2898, -1.0283, -0.9063,  ..., -1.6035, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -6, -1, -1, -1,  2, -6, -9, -7,  3, -3,  1,  4, -3, -2,  0,  2,  2,\n",
            "        -1, -3,  3,  3,  3,  3, -2, -6, -7,  0,  3,  3,  5, -6,  8, -3, -1, -2,\n",
            "         2,  3, -5,  0, -2,  5,  1, -2, -5,  3, -1,  3,  5, -2, -6,  0, -1, -2,\n",
            "         2, -8,  2, -3,  6, -5,  7, -4,  2,  3, -4,  1,  3,  1,  2,  5,  7, -2,\n",
            "         5, -7, -3,  6,  2, -3, -4,  2,  5,  2, -2, -2,  4, -8, -3,  6,  5,  2,\n",
            "        -4, -3,  1, -5, -3, -7, -2,  7, -2, -2,  2, -6, -4,  3, -3,  3,  2, -8,\n",
            "        -2,  7,  7,  1,  0,  0,  2, -7,  0, -3,  1,  1, -5,  1, -1, -4, -5,  7,\n",
            "         1,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.4385,  ..., -1.4556,  0.4110,  0.6507],\n",
            "         [-3.0825, -2.9626, -1.2501,  ..., -1.5755, -0.0514,  0.2397],\n",
            "         [-3.0311, -2.8770, -0.9419,  ..., -1.8837, -0.7877, -0.3596]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -0.7878,  ..., -0.9979,  0.8403,  0.9804],\n",
            "         [-2.4335, -2.1534, -0.5077,  ..., -0.9454,  0.6478,  0.7878],\n",
            "         [-2.5210, -2.2584, -0.3326,  ..., -1.3655, -0.1926,  0.2101]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  1.9346,  ...,  0.0523,  1.3595,  1.1852],\n",
            "         [-1.6732, -0.7320,  1.8301,  ..., -0.0697,  0.9586,  0.9412],\n",
            "         [-2.0218, -1.6383,  0.9063,  ..., -0.5926,  0.3312,  0.5054]]])\n",
            "tensor([-5, -9,  2,  6, -4,  5,  1, -6, -1,  1,  4,  2,  7,  3, -4,  1,  1,  5,\n",
            "         6,  1, -3,  3,  7,  1,  6, -6, -4, -2, -1,  4,  7, -5,  5,  1,  1, -5,\n",
            "        -3,  4, -8,  4,  4,  0, -5, -1, -1,  1, -3,  7,  6, -2, -8,  0,  0, -8,\n",
            "         4, -8,  7, -3,  9, -6,  3,  0,  2,  7, -2, -2,  2,  7,  7,  5,  6,  6,\n",
            "         4, -6,  0,  4, -4, -5,  0,  3,  5,  0,  3, -9,  6, -2, -4,  3,  6, -5,\n",
            "         2,  4,  2, -2, -4, -1, -6,  5,  7, -3, -6,  0, -5,  6, -4,  0,  5, -5,\n",
            "        -7,  0,  9, -2, -5,  1, -5, -2, -3, -5,  4,  3,  0, -1, -8,  1, -4,  5,\n",
            "        -8, -2])\n",
            "tensor([[[ 2.8598,  3.0996,  2.9797,  ...,  1.0789,  1.1645,  1.2844],\n",
            "         [ 3.5448,  3.7846,  3.4763,  ...,  1.6440,  1.7125,  1.6954],\n",
            "         [ 3.4078,  3.5106,  2.9626,  ...,  1.7296,  1.6954,  1.9180],\n",
            "         ...,\n",
            "         [-1.2501, -1.2672, -1.6097,  ...,  0.0342,  1.8152,  1.8837],\n",
            "         [-0.7364, -0.5994, -0.9590,  ..., -0.7535,  0.7364,  0.9932],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.9062,  3.1513,  3.1338,  ...,  1.5581,  1.6632,  1.7332],\n",
            "         [ 3.6415,  3.9566,  3.8165,  ...,  2.2934,  2.3810,  2.3459],\n",
            "         [ 3.5539,  3.7990,  3.4489,  ...,  2.3634,  2.3634,  2.6085],\n",
            "         ...,\n",
            "         [-0.4377, -0.2801, -0.8228,  ...,  0.5777,  2.3634,  2.3284],\n",
            "         [ 0.1576,  0.4377, -0.0700,  ..., -0.0175,  1.4706,  1.6457],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.9630,  3.2418,  3.2941,  ...,  2.1089,  2.1961,  2.2309],\n",
            "         [ 3.7298,  4.0436,  4.0436,  ...,  2.9978,  3.0850,  2.9978],\n",
            "         [ 3.6776,  4.0087,  3.8693,  ...,  3.0675,  3.0501,  3.2418],\n",
            "         ...,\n",
            "         [ 1.0109,  1.9521,  1.6383,  ...,  1.7255,  2.8235,  2.6144],\n",
            "         [ 1.0980,  2.0566,  2.1089,  ...,  1.0283,  2.0566,  2.0741],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 2,  0,  8,  2, -7,  1, -4, -4, -8,  2,  4,  3,  6,  3,  5,  1,  5,  2,\n",
            "         5, -1, -1,  0, -1,  8,  7, -9, -4, -1,  3,  4,  2, -1,  3,  3,  2,  0,\n",
            "        -3, -3, -1,  3,  4, -3, -1, -3, -3, -2,  1,  7,  4,  2, -4, -5, -2, -5,\n",
            "         7, -7,  1, -5,  6, -5, -2, -9, -3, -1,  4, -3, -3,  7,  6,  6,  3, -2,\n",
            "         5, -5, -4,  5,  1, -5,  3,  2,  4,  0, -3, -3,  2,  0, -6,  0,  4,  1,\n",
            "         1, -5,  4, -2, -4, -1, -4,  1,  3,  2, -1, -5,  2,  8, -5, -3,  2, -2,\n",
            "        -6,  1,  8, -7, -2,  6,  3, -4, -3, -6,  3,  8, -2,  2, -2, -1, -1,  3,\n",
            "        -5, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000,  2.0207,  ..., -0.2911, -0.2226, -0.1199],\n",
            "         [-0.4281, -0.2740,  1.3700,  ..., -0.6507, -0.5651, -0.2226],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.5755,  ..., -0.3082,  1.1474,  0.8562],\n",
            "         [-3.0825, -2.9626, -1.2159,  ..., -1.1302,  0.3767,  0.4281],\n",
            "         [-3.0311, -2.8770, -1.1987,  ..., -1.3357, -0.4452, -0.1541]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000,  2.2759,  ...,  0.4902,  0.5777,  0.6478],\n",
            "         [-0.4202, -0.1225,  1.8557,  ...,  0.1050,  0.2276,  0.5777],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -0.7703,  ...,  0.2801,  1.7857,  1.4181],\n",
            "         [-2.4335, -2.1534, -0.4202,  ..., -0.4552,  1.1905,  1.2430],\n",
            "         [-2.5210, -2.2584, -0.5602,  ..., -0.7878,  0.2976,  0.6303]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.9760,  ..., -0.1743, -0.0871, -0.0697],\n",
            "         [-0.3660,  0.0000,  0.8017,  ..., -0.3660, -0.2266,  0.0523],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  0.2614,  ...,  0.2440,  0.9935,  0.3660],\n",
            "         [-1.6732, -0.7320,  0.2440,  ..., -0.7843,  0.3486,  0.1917],\n",
            "         [-2.0218, -1.6383, -0.9237,  ..., -1.3420, -0.4880, -0.4183]]])\n",
            "tensor([-2, -9,  1,  4, -4,  1, -1, -2, -2,  5,  4, -1,  2,  0,  1,  1,  1, -2,\n",
            "         4,  2, -4,  4,  8,  1,  5,  0, -3,  5,  4,  2, -1,  0,  0,  0, -5, -4,\n",
            "        -2,  5, -8,  6, -3,  2, -7, -4, -1,  5, -5, -1,  8, -4,  0, -8,  2, -2,\n",
            "         3, -8,  5, -7,  9, -1,  6, -5, -4,  6,  0, -1,  3,  4,  7, -2, -1, -3,\n",
            "         6, -9, -5,  6, -7, -2,  4,  5,  7,  0,  3, -5,  1, -2, -7, -3,  2, -6,\n",
            "         3, -5,  5, -4, -4, -8,  1,  2,  7, -2, -5, -5,  3,  2, -4,  0,  0, -4,\n",
            "        -1,  3,  0, -8,  2,  2,  0, -1, -1, -3,  2,  1,  1,  0, -3,  1, -2,  2,\n",
            "        -1,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740,  0.8049,  ..., -0.6507, -0.3939,  0.1370],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.1131,  ..., -0.1370,  1.5070,  1.5584],\n",
            "         [-3.0825, -2.9626, -0.7021,  ..., -0.4966,  1.1131,  1.3871],\n",
            "         [-3.0311, -2.8770, -0.4624,  ..., -1.1131,  0.1370,  0.6165]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225,  0.4202,  ..., -0.8754, -0.5252,  0.1401],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -1.2430,  ..., -0.5427,  1.1204,  1.1029],\n",
            "         [-2.4335, -2.1534, -0.7353,  ..., -0.6478,  0.9629,  1.1204],\n",
            "         [-2.5210, -2.2584, -0.5427,  ..., -1.3831, -0.0875,  0.3326]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000,  0.1569,  ..., -0.6797, -0.3137,  0.2789],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  0.2266,  ..., -0.2789,  0.6797,  0.3834],\n",
            "         [-1.6732, -0.7320,  0.3660,  ..., -0.5926,  0.5229,  0.4532],\n",
            "         [-2.0218, -1.6383, -0.4880,  ..., -1.5338, -0.4357, -0.2614]]])\n",
            "tensor([-3, -7,  6,  2, -4,  1,  2, -7, -7,  0, -4,  1,  7,  0,  2,  3,  0, -2,\n",
            "         6,  6,  1,  7,  6,  1,  6, -8, -7,  6,  3,  2,  6, -3,  4, -3, -3,  4,\n",
            "        -3,  6, -2,  4, -3, -1, -4, -1, -7, -2, -6,  2,  3,  1,  1, -5, -2, -5,\n",
            "         3, -6,  2, -7,  0, -8,  1, -9, -6,  0,  0, -2,  1,  2,  5, -2,  6, -3,\n",
            "         2, -5, -2,  6, -2,  3, -3, -4,  6,  5,  1, -8,  2, -1, -1,  5,  1, -4,\n",
            "         1,  2,  1, -8, -5, -5, -3,  1,  3,  1,  3, -3,  1,  2, -4,  3,  8, -3,\n",
            "        -1,  3,  7, -2, -4, -1, -2, -6, -3, -2,  5,  0, -8, -2,  1, -6,  1,  8,\n",
            "        -8, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.2226,  0.3082, -0.3082,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.4421, -3.4078, -3.3736,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-2.8770, -2.8770, -3.1510,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-2.7571, -2.6715, -2.9626,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.2451,  0.4727,  0.0700,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.8536, -2.6436, -2.8011,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.2234, -2.0658, -2.4860,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.2409, -2.0483, -2.3810,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.2614,  0.5577,  0.3660,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.5512, -0.5577, -0.4357,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.4641, -0.6449, -0.4880,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-1.7429, -1.4292, -1.4118,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5,  0, -1,  1, -3, -3,  0, -7, -3, -1,  4, -5, -1,  3,  3,  3,  0, -4,\n",
            "         4,  4, -3, -1,  5,  2,  3, -2, -3,  4, -5, -2,  1, -8,  1,  0, -2, -3,\n",
            "         0,  6, -2,  3,  1,  2, -3,  1, -2,  5, -4,  8,  0, -1, -6, -1, -3, -7,\n",
            "         5, -8,  2, -5,  7, -1,  6, -8, -1,  2,  0,  0,  3,  1,  4,  3,  7,  4,\n",
            "        -1, -7, -5,  5, -7, -6,  4, -3, -1,  4, -6, -4,  5, -6, -7,  5,  1, -5,\n",
            "         1, -3,  3, -4, -5, -6, -2,  2,  5,  4,  2,  0, -3,  5, -7, -4,  0, -9,\n",
            "        -2,  1,  0, -3, -1,  1,  2,  2, -1, -5,  4,  2, -6, -1, -5, -8,  0,  1,\n",
            "        -6,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 4.0586,  4.3497,  4.0586,  ...,  2.2433, -2.0378, -2.0892],\n",
            "         [ 3.9044,  4.0586,  3.4934,  ...,  2.3290, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [ 0.8049,  0.9247, -0.1027,  ..., -2.3632, -0.9590, -0.9076],\n",
            "         [ 1.2330,  1.3529,  1.1302,  ..., -2.1406, -1.6611, -1.4214],\n",
            "         [ 1.3015,  1.4556,  1.3015,  ..., -0.0171, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 4.0966,  4.4468,  4.2892,  ...,  2.9062, -1.4531, -1.5231],\n",
            "         [ 4.0091,  4.3067,  3.9566,  ...,  2.9762, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 1.4881,  1.7857,  0.4902,  ..., -1.9608, -0.5427, -0.5952],\n",
            "         [ 1.9783,  2.2584,  1.8557,  ..., -1.5581, -1.0854, -0.9279],\n",
            "         [ 1.9083,  2.1709,  1.9608,  ...,  0.5427, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 4.0784,  4.4270,  4.4270,  ...,  3.4684, -0.8715, -0.9935],\n",
            "         [ 4.0610,  4.3747,  4.2527,  ...,  3.5556, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 2.7712,  3.8519,  2.8584,  ..., -0.7669, -0.1220, -0.3486],\n",
            "         [ 2.7190,  3.6601,  3.8519,  ..., -0.5577, -0.5926, -0.5926],\n",
            "         [ 2.3878,  2.7712,  2.9107,  ...,  1.3072, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -9,  5,  5, -2,  1, -6,  0, -1, -1,  0,  3,  2,  1, -3,  5,  8,  4,\n",
            "         5,  2,  0,  0,  7,  2,  7, -8, -2,  0,  1,  3,  7,  1,  2, -2, -4,  3,\n",
            "        -5,  6, -6,  0,  5, -1, -6,  1, -1,  1, -7,  6,  1, -1, -2,  0, -5, -5,\n",
            "         5, -5, -1,  2,  5, -7,  3, -1, -2, -1, -4,  1, -2,  8,  8,  1,  1,  6,\n",
            "         5, -2, -1,  2,  2, -3,  4, -3, -1,  2,  1, -2,  0, -7, -2,  1,  1, -3,\n",
            "         0,  2,  0, -5, -4, -3, -1,  4,  5, -2, -5, -1,  3,  5, -8,  0,  6, -1,\n",
            "        -3,  0,  5, -2, -4,  6,  3, -6,  2, -6,  0, -1, -1,  1, -3,  1, -3,  4,\n",
            "        -6, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -1.8323, -1.7638,  ..., -1.2672,  0.4795,  0.4966],\n",
            "         [-3.0825, -1.3700, -1.5926,  ..., -1.7296, -0.2911, -0.1370],\n",
            "         [-3.0311, -1.3357, -1.5412,  ..., -2.3290, -1.2672, -0.8734]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -1.1029, -1.2080,  ..., -0.8403,  0.9279,  0.8228],\n",
            "         [-2.4335, -0.5777, -0.9454,  ..., -1.1380,  0.3151,  0.3852],\n",
            "         [-2.5210, -0.7353, -0.9629,  ..., -1.8207, -0.7178, -0.3501]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  0.7843,  0.9237,  ...,  0.1220,  1.2026,  0.9586],\n",
            "         [-1.6732,  0.6100,  0.8017,  ..., -0.3486,  0.6623,  0.6100],\n",
            "         [-2.0218, -0.3834, -0.2789,  ..., -1.2026, -0.2614, -0.1220]]])\n",
            "tensor([-6, -8,  0,  2, -3,  5, -4, -7, -8,  1, -1, -4,  6, -3,  2,  0,  6,  1,\n",
            "        -3,  0,  3,  5,  7,  4,  2, -2, -3,  3, -5, -3,  7,  1,  8,  1,  2,  4,\n",
            "         3,  2, -3,  3,  6, -2, -4,  5, -6,  0,  0,  0,  8,  0, -5, -1,  1,  0,\n",
            "        -2, -4,  1, -7,  3, -5,  4, -1, -6,  7,  0,  0, -5,  8,  1,  5,  5,  6,\n",
            "         2, -5, -3,  3,  2, -4,  4,  4,  2,  0, -4, -1,  0, -3, -6,  1,  0, -1,\n",
            "        -2,  2,  0,  0, -4, -4,  1,  5,  2,  1,  3, -9,  3,  8, -2,  0,  4, -8,\n",
            "        -3,  0,  4, -6, -5,  5, -5, -7, -5, -1,  4,  4, -1,  0, -6, -4, -6,  8,\n",
            "        -7, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.0514,  1.8152,  1.7296],\n",
            "         [-3.0825, -2.9626, -3.1852,  ...,  0.2226,  1.4727,  1.0446],\n",
            "         [-3.0311, -2.8770, -3.0653,  ...,  0.1712,  0.9076,  0.5137]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.2451,  1.9258,  1.7507],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  0.6828,  1.8032,  1.2255],\n",
            "         [-2.5210, -2.2584, -2.4860,  ...,  0.5427,  1.2080,  0.7178]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.4292,  2.4575,  2.0915],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  1.6383,  2.3355,  1.5861],\n",
            "         [-2.0218, -1.6383, -1.5163,  ...,  1.2723,  1.7952,  1.0806]]])\n",
            "tensor([-1, -1,  8,  5, -1,  3, -1, -5, -6,  6,  4,  2,  1, -1, -4,  2, -1,  0,\n",
            "        -2,  4, -4,  6,  5,  6,  2, -6,  0,  6,  2,  3,  2, -6, -1,  0, -6,  4,\n",
            "        -3,  0, -2,  4, -3, -4, -7, -1, -4,  2,  1,  6,  4,  4, -2, -2, -4, -3,\n",
            "         0, -8,  3, -6,  0, -5,  6, -9,  0,  6, -1, -1, -5,  5,  2,  4,  0,  5,\n",
            "        -2, -3,  0,  1, -2,  3,  3,  4, -1, -2,  0, -3,  6, -2, -7,  1,  4, -3,\n",
            "         4,  4,  6, -3, -1, -7, -8, -2,  7, -1, -1, -6,  0,  5, -4,  3, -1,  0,\n",
            "        -3,  2,  2, -7,  1,  1, -1, -7,  0, -1,  4,  8, -5, -3, -6, -7, -3,  3,\n",
            "        -8,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.1370,  ..., -1.9180, -1.7638, -1.8323],\n",
            "         [-0.4281, -0.2740, -0.7192,  ..., -1.7467, -1.8152, -1.6097],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.6782,  ..., -0.8049,  1.0617,  1.1131],\n",
            "         [-3.0825, -2.9626, -1.5070,  ..., -1.2844,  0.2055,  0.5309],\n",
            "         [-3.0311, -2.8770, -1.3871,  ..., -1.7810, -0.7021, -0.2226]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000,  0.0000,  ..., -1.3831, -1.2605, -1.3480],\n",
            "         [-0.4202, -0.1225, -0.3852,  ..., -1.2780, -1.2955, -1.0679],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -1.7157,  ..., -1.0854,  0.7878,  0.7353],\n",
            "         [-2.4335, -2.1534, -1.4531,  ..., -1.3831,  0.1225,  0.3501],\n",
            "         [-2.5210, -2.2584, -1.4006,  ..., -1.9433, -0.8403, -0.3676]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0871,  ..., -0.8889, -0.7669, -0.8715],\n",
            "         [-0.3660,  0.0000, -0.0871,  ..., -0.7495, -0.7495, -0.5926],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  0.0697,  ..., -0.6972,  0.4357,  0.2092],\n",
            "         [-1.6732, -0.7320,  0.0174,  ..., -1.1155, -0.0871, -0.0523],\n",
            "         [-2.0218, -1.6383, -0.9760,  ..., -1.8998, -0.9412, -0.7146]]])\n",
            "tensor([ 1, -9,  8,  5, -2, -1, -5, -2, -4,  1, -1,  2, -2,  0,  2,  4,  2,  0,\n",
            "         1, -2,  4,  0,  8, -1,  3, -6,  1,  5, -5,  2,  4, -1,  0,  5,  1,  0,\n",
            "         0, -1, -1, -2,  4, -3,  0,  4, -6,  2, -8,  6,  7, -1, -1, -7, -3, -3,\n",
            "         6, -4,  1, -4,  6,  0,  0, -7,  0,  7,  4,  2, -5,  3,  1,  1,  0,  3,\n",
            "         6,  0,  0,  3,  0,  1, -3,  0,  1, -4,  1, -9,  0,  0, -1, -2,  1, -6,\n",
            "         0,  1,  4, -5, -5, -6, -6,  7,  6,  2, -1, -6,  1,  2,  0,  0,  0, -9,\n",
            "        -3, -2,  0, -4, -6,  7, -1, -3, -1, -6,  1, -1, -3, -6, -1, -8,  1,  4,\n",
            "        -4,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ...,  0.0342,  0.1199,  0.1712],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.7192,  0.7706,  0.7021],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.8562,  0.8049,  0.9932],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -1.6611,  0.1541,  0.1884],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -2.0036, -0.4966, -0.2740],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -2.5173, -1.4214, -0.9590]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  1.3831,  1.4881,  1.4881],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  2.2409,  2.2934,  2.2234],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  2.3459,  2.3109,  2.4860],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.4727,  1.3480,  1.2605],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.6828,  0.8403,  0.9629],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.2780, -0.1401,  0.2801]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  2.1961,  2.2658,  2.2484],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  3.1895,  3.2418,  3.1373],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  3.2767,  3.2593,  3.3987],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.1155,  2.2484,  1.9521],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.7495,  1.7952,  1.7429],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -0.0349,  0.9237,  1.0806]]])\n",
            "tensor([ 2, -2,  5,  2, -1,  3, -2, -7, -2,  5,  3, -1,  3,  1,  3,  4,  2, -4,\n",
            "         0,  3,  2, -2,  2,  4,  1, -8, -2,  3, -4,  2,  5, -1,  0,  1, -1, -1,\n",
            "        -3, -1, -3,  6,  1, -3,  0, -4, -7,  4, -6,  6,  4, -3, -7, -6,  0, -6,\n",
            "         2, -7, -1, -7,  5, -7,  5, -9, -6,  2, -3,  4, -2, -1,  5,  5,  4,  5,\n",
            "         2,  0, -5,  1, -4, -3, -2, -1,  8, -2,  0, -2,  1, -5, -1,  6, -1, -3,\n",
            "        -4, -4,  6, -8, -6, -5, -8,  5,  3, -1,  2, -8, -4,  7, -3,  3,  7, -8,\n",
            "        -4,  4,  3, -8, -6,  1,  2, -7, -1,  3,  2,  1,  0,  2, -8, -8, -3,  4,\n",
            "        -4,  0])\n",
            "tensor([[[ 3.3565,  3.4421,  2.3975,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.4934,  2.7228,  1.9351,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.8666,  1.8837,  1.6269,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [ 0.8049,  0.8905,  0.9247,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 1.2844,  1.4042,  1.1645,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 1.3357,  1.4899,  1.3015,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 3.3789,  3.3789,  2.2934,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 3.4139,  2.4860,  1.6982,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.6282,  1.5931,  1.4706,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 1.4881,  1.7857,  1.5931,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 2.0308,  2.3109,  1.9258,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 1.9433,  2.2059,  1.9783,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 3.3464,  3.2941,  2.1961,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 3.2244,  2.1961,  1.4815,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.3420,  1.2723,  1.3246,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 2.7364,  3.8519,  3.9390,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 2.7538,  3.7124,  3.9041,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 2.4227,  2.8061,  2.9281,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-4,  0,  8,  2, -2,  1, -5, -9,  0,  5,  2,  3, -1, -3, -2,  1,  1,  0,\n",
            "         5,  6, -5, -1,  1, -1,  1, -9,  0, -2, -5,  4,  1, -7,  2, -2, -1, -5,\n",
            "        -6, -3, -7, -1,  6,  3, -5,  1, -7,  4, -1,  1,  0,  4, -5, -2, -3, -2,\n",
            "         2,  0,  1,  0,  5, -8,  2, -5, -5,  4, -3,  4,  3, -1,  0,  1,  0,  0,\n",
            "         4, -1, -1, -3, -3, -3,  2, -1,  1, -1, -6, -4,  2, -4, -5,  1,  6,  1,\n",
            "         1,  3,  4, -2, -2,  0, -8,  5,  3, -1,  3,  0, -1,  3, -1,  1,  5, -5,\n",
            "        -4,  4,  3, -5,  1,  1, -6,  0,  3,  2,  3,  3,  0,  0,  0,  1, -1,  5,\n",
            "        -1,  3])\n",
            "tensor([[[ 2.4317,  2.7400,  2.5858,  ..., -0.5651, -2.6030, -2.5345],\n",
            "         [ 3.1338,  3.3393,  3.0140,  ...,  0.2569, -2.0378, -2.0892],\n",
            "         [ 2.8598,  3.0311,  2.5687,  ...,  0.7021, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.8557,  2.1884,  2.1884,  ..., -0.3501, -2.1884, -2.1709],\n",
            "         [ 2.5735,  2.9062,  2.8186,  ...,  0.4902, -1.4531, -1.5231],\n",
            "         [ 2.4510,  2.7836,  2.5560,  ...,  0.7528, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.3595,  1.6558,  1.6558,  ..., -0.0174, -1.7778, -1.7952],\n",
            "         [ 2.0741,  2.3878,  2.4227,  ...,  0.8540, -0.8715, -0.9935],\n",
            "         [ 1.9695,  2.3878,  2.3878,  ...,  0.9237, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -2,  3,  1, -2, -1, -5, -3, -8,  7, -4,  1,  4, -2,  4,  9,  4,  0,\n",
            "         5,  4,  3,  3,  5,  0,  7, -3, -5,  1,  0,  0,  0, -7,  6, -3,  0, -5,\n",
            "         1,  3, -7, -2,  5, -4, -5, -4, -5,  4, -8,  8,  0, -1, -1, -2,  1, -2,\n",
            "        -2, -4,  2,  0,  5, -2, -1, -9,  2,  0, -5, -1, -1,  0,  6,  2,  7,  0,\n",
            "         0, -9,  3, -1,  0,  2, -5, -3, -1,  3,  3, -3,  3, -5, -1, -2,  6, -7,\n",
            "        -2, -5,  6, -1, -9,  0, -3,  3,  2,  2, -4, -4, -3,  3, -5,  1,  5, -3,\n",
            "        -1,  3,  0,  0,  1,  0, -1, -3, -5, -5,  0,  3,  0,  1, -7,  1,  0,  3,\n",
            "        -7, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ...,  0.0514,  0.2226,  0.3939],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  1.4899,  1.2501,  1.1474],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.8905,  0.3767,  1.5412],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  0.4377,  0.5777,  0.6653],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.9608,  1.7332,  1.6457],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.2605,  0.7878,  2.0833],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  0.8540,  1.0109,  1.0806],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  2.5272,  2.2658,  2.1612],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.7952,  1.2723,  2.5970],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1,  0,  7,  1,  0,  5, -6, -2,  0,  2, -3, -2,  5, -3,  3,  2,  7, -4,\n",
            "         3, -1, -2,  2,  3,  4,  2, -3, -1, -2,  4,  4,  4, -3,  2,  1, -2, -5,\n",
            "         1,  3, -3, -1, -3,  1,  2,  4, -5,  1, -2,  2, -1, -4,  0, -1,  3, -1,\n",
            "        -2, -8,  8, -5,  1,  0,  6, -8, -6, -1,  1,  2, -3,  7, -1, -2, -1,  2,\n",
            "        -2, -9,  0, -2,  1, -4, -1,  0,  1,  0, -2, -3,  6,  1,  0,  2,  2, -2,\n",
            "         4,  4,  2, -7, -4, -1, -8,  3,  7,  3,  0, -4,  2,  3, -5, -3,  8,  0,\n",
            "         1,  6,  8, -5,  2,  4, -2, -7,  1, -4,  2,  5, -3,  3, -1, -4,  0,  5,\n",
            "        -6, -2])\n",
            "tensor([[[-0.7021, -0.1370, -0.2226,  ..., -0.9247, -0.9590, -0.9247],\n",
            "         [-0.0342,  0.4452,  0.2569,  ..., -0.5137, -0.4795, -0.6679],\n",
            "         [-0.1541,  0.0685, -0.4110,  ..., -0.4795, -0.5994, -0.4452],\n",
            "         ...,\n",
            "         [-3.5448, -3.4250, -3.3736,  ..., -1.9008, -0.0856,  0.0000],\n",
            "         [-3.0653, -2.9455, -3.1681,  ..., -2.3975, -0.8734, -0.7021],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0154, -0.5777, -0.5427,  ..., -0.5777, -0.6653, -0.7178],\n",
            "         [-0.1576,  0.1401,  0.0525,  ...,  0.0175,  0.0175, -0.2451],\n",
            "         [ 0.0000,  0.2101, -0.1576,  ...,  0.0525, -0.0700,  0.0350],\n",
            "         ...,\n",
            "         [-2.9587, -2.6611, -2.8011,  ..., -1.4181,  0.3676,  0.3151],\n",
            "         [-2.4160, -2.1359, -2.5035,  ..., -1.7682, -0.2626, -0.2101],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.9586, -0.2092,  0.1046,  ..., -0.2440, -0.3312, -0.4009],\n",
            "         [-0.2092,  0.3312,  0.5926,  ...,  0.5403,  0.5403,  0.2614],\n",
            "         [-0.0349,  0.3486,  0.2963,  ...,  0.6100,  0.5054,  0.5752],\n",
            "         ...,\n",
            "         [-1.6558, -0.5752, -0.4357,  ...,  0.5577,  1.6906,  1.5686],\n",
            "         [-1.6558, -0.7146, -0.5054,  ...,  0.1569,  1.2026,  1.1329],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -3,  8,  3, -4,  5,  2, -7, -1,  4, -4, -1,  6,  5,  5,  7,  7,  1,\n",
            "         1, -3, -4,  6,  6,  2,  3, -2,  2,  3,  1, -1,  8, -8,  2, -4, -5,  2,\n",
            "         2,  2, -3, -1,  4,  4, -4,  4, -6,  1, -7,  4,  0, -1, -2, -7, -5,  0,\n",
            "         2, -5,  1, -5,  8, -7,  6, -3,  0,  0, -4,  0, -6,  7,  6,  6,  5,  2,\n",
            "         1,  0,  2,  3, -5,  2, -5, -4,  1,  5, -5,  0,  0, -5,  0, -2,  1, -2,\n",
            "         2, -4,  3, -8,  0,  0, -4,  2,  3, -5,  1,  0, -4,  4, -6,  1,  7, -4,\n",
            "        -2,  0,  7, -4,  1,  4, -5, -6,  3, -2, -1, -1, -6, -3, -7,  0,  2,  7,\n",
            "        -4,  2])\n",
            "tensor([[[ 0.7535,  1.6269,  2.0207,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.7742,  3.4763,  3.6476,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 3.4421,  3.8017,  3.1681,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.8053,  1.4006,  1.7332,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.2409,  2.8887,  3.1688,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.7661,  3.3964,  2.8361,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.0871,  0.7495,  1.3246,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 2.0741,  2.6144,  2.9978,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.6144,  3.0675,  2.5621,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -4,  8,  0,  1, -4, -1, -7, -9,  4,  3,  1,  6,  6, -3,  4,  7, -4,\n",
            "        -1, -3,  1, -1,  6, -1,  1, -2,  1,  3, -3, -1,  1, -2,  1,  5, -6, -3,\n",
            "        -1, -1, -9,  5,  3,  5, -2, -3, -3,  0, -8,  6, -1, -3, -4, -8, -6, -8,\n",
            "         3, -3,  3,  2,  0, -1,  4, -1, -5, -1, -3,  0, -4,  6,  0,  5,  3,  5,\n",
            "         3,  0,  4,  1,  0, -4,  1,  4,  7, -1, -5, -2,  5, -8, -4,  4,  6, -7,\n",
            "        -4,  3,  2,  0, -7, -6, -3,  6,  1,  0, -6, -4, -5,  7, -4, -6,  2, -9,\n",
            "        -3, -1,  2, -8, -2,  5, -5, -6,  3,  2,  0,  2, -5, -3, -7,  1, -3,  6,\n",
            "        -5, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.7228, -2.5345, -2.3975,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-2.0892, -2.2433, -2.4660,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-1.8837, -2.4317, -2.5858,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.9958, -1.6282, -1.6807,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-1.3130, -1.3305, -1.6632,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-1.2255, -1.7332, -1.9258,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.6100,  0.5403,  0.8017,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-0.4009,  0.1569,  0.3486,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-0.5054, -0.9935, -0.8540,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-6, -4,  0,  4, -4,  0, -1,  0, -7,  4,  4,  1,  2,  6, -4,  6,  4, -1,\n",
            "         3, -3, -3,  4,  6,  2,  3, -9, -6, -2, -2,  0,  7,  1,  6,  4, -1, -4,\n",
            "        -1,  4, -1,  2,  2, -1, -1, -1, -1,  2, -5,  6,  1, -2, -5, -3, -2, -3,\n",
            "        -1, -9,  3,  0,  4, -1, -2, -4, -6,  0, -1,  2, -4,  5,  7, -1,  7,  5,\n",
            "         2, -2, -1, -1,  0,  2,  3, -3,  5,  3,  0, -4,  8, -6,  2,  5,  0, -5,\n",
            "        -2,  1, -1, -2, -7, -2,  1, -2,  6, -1, -5, -3, -1,  8, -2, -3,  3, -3,\n",
            "        -5,  0,  2, -1,  3,  5, -6, -1, -6,  0,  3,  4, -4,  3,  1,  0,  1,  2,\n",
            "        -8, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [ 0.0000,  0.4110, -0.2226,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [ 0.4281,  0.7192,  0.0342,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [ 0.1712,  0.4110,  0.1027,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.4902,  0.8929,  0.3326,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [ 0.9629,  1.2255,  0.6653,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [ 0.7353,  1.0154,  0.7528,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.2026,  2.2309,  2.2309,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [ 1.1678,  1.9521,  2.2135,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [ 0.9063,  1.2375,  1.4292,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -8,  5,  2, -3,  5, -2, -9, -4,  7,  0, -5, -2,  5,  3,  1,  7, -2,\n",
            "         1,  4, -4,  2,  1,  4,  6, -1, -3,  1, -4, -2,  6,  0,  0,  4, -6,  0,\n",
            "         3,  1, -8, -3,  1, -1,  0,  5, -9,  1, -6,  5,  7,  3, -2, -1,  3, -1,\n",
            "        -1, -4,  4, -6,  1, -3,  4, -3, -1, -1,  4, -4,  2,  4,  3,  4,  4, -3,\n",
            "         4, -1, -4,  2, -2, -4,  3,  5,  5,  2,  0, -1,  8, -4,  1,  6,  0,  2,\n",
            "         1,  2,  3, -5, -8, -5, -5,  6,  3, -3,  1, -9, -1,  2, -4, -4,  6, -6,\n",
            "        -7,  4,  3, -4, -1,  2, -6,  1, -6,  2,  0,  2,  0, -4, -4, -6, -5,  3,\n",
            "        -4, -3])\n",
            "tensor([[[-1.0104,  2.0036,  1.9351,  ..., -0.4452, -0.3425, -0.3596],\n",
            "         [-0.2740,  2.7400,  2.4660,  ...,  0.0171,  0.2055,  0.0685],\n",
            "         [-0.4281,  1.7638,  1.1302,  ..., -0.1370, -0.2397, -0.0514],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  1.9958,  2.0483,  ..., -0.0875,  0.0175, -0.1050],\n",
            "         [-0.3501,  2.8361,  2.7311,  ...,  0.5777,  0.7353,  0.5777],\n",
            "         [-0.4202,  2.2759,  1.8557,  ...,  0.5427,  0.4377,  0.6828],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  1.7952,  1.8649,  ..., -0.0174,  0.0697, -0.1046],\n",
            "         [-0.3486,  2.5795,  2.5970,  ...,  0.8715,  0.9760,  0.7495],\n",
            "         [-0.3660,  2.1264,  1.9172,  ...,  0.9237,  0.7669,  0.9063],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -9,  7,  5,  1,  2,  0, -8, -2, -2,  2, -2,  3,  5,  5,  8,  8, -3,\n",
            "         1,  0, -2,  7,  6,  0,  4, -9,  0,  0, -1,  0,  2, -8,  4, -1, -6,  2,\n",
            "        -6,  5, -8,  0,  5,  0, -2,  3, -6,  4, -7,  5,  2,  5, -1, -9, -2, -1,\n",
            "         3, -5,  5,  1,  0, -7,  2, -3, -5,  4, -2, -4,  2,  3, -1,  0,  2,  1,\n",
            "         1,  0,  4,  5, -2, -4,  0, -2,  2,  2, -3, -5,  1, -3,  2,  0,  5, -5,\n",
            "         1, -1,  4, -5, -9, -2, -6,  2,  4,  3, -6, -6,  3,  7,  0,  1,  4, -7,\n",
            "        -2,  1,  8, -7, -3,  8, -6, -7,  0, -1,  6,  5,  0, -3, -2,  0, -2, -1,\n",
            "         0, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.4042,  1.4556,  0.8905,  ..., -0.4624, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.4934, -3.3222, -3.2195,  ..., -0.3939, -0.9590, -0.9076],\n",
            "         [-3.0311, -2.9112, -3.0996,  ..., -0.7877, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8598, -2.9797,  ..., -1.3700, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.3655,  1.5581,  1.1905,  ...,  0.0525, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9237, -2.5385, -2.5035,  ...,  0.1050, -0.5427, -0.5952],\n",
            "         [-2.3634, -2.1008, -2.5035,  ..., -0.1401, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2409, -2.4335,  ..., -0.8053, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.0806,  1.3420,  1.2898,  ...,  0.5926, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6558, -0.4532,  0.0000,  ...,  1.4118, -0.1220, -0.3486],\n",
            "         [-1.6035, -0.6797, -0.5054,  ...,  1.0109, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6035, -1.4815,  ...,  0.1743, -1.4641, -1.2549]]])\n",
            "tensor([-5,  0,  4,  7, -8, -1, -1, -5,  0, -2,  5, -4,  4,  4,  2,  2, -1,  2,\n",
            "        -2, -3, -3,  2,  1,  8,  5,  0, -2,  2,  1,  4,  4, -5,  7, -1, -4,  3,\n",
            "        -6,  4, -2,  5, -3,  3, -5, -4,  0,  5, -2,  5,  6,  4, -3, -3, -1,  1,\n",
            "         6, -6,  1, -6,  8,  0,  1, -6, -5,  4, -5,  0,  2,  6,  2, -1,  1,  4,\n",
            "         0, -9,  3,  6, -2,  3, -4,  4,  5, -4,  0, -4,  4, -4, -3,  1,  1, -4,\n",
            "        -1,  0,  0, -3, -3, -5, -1,  7, -2,  3, -3, -8,  2,  3, -2, -4,  3, -4,\n",
            "        -2, -2,  3, -5, -3, -1,  2,  0,  3, -1,  2,  5, -3,  0, -4, -5,  1,  2,\n",
            "        -5,  1])\n",
            "tensor([[[-1.0104,  0.0342, -0.0685,  ...,  0.9590,  0.4281, -0.7021],\n",
            "         [-0.2740,  0.7535,  0.4966,  ...,  0.6165,  0.7364,  0.0685],\n",
            "         [-0.4281,  0.4966, -0.0171,  ..., -0.6336, -0.6679, -0.3596],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  0.1751,  0.1576,  ...,  0.8754,  0.8403,  0.1225],\n",
            "         [-0.3501,  0.9629,  0.8929,  ...,  0.9454,  1.2255,  0.7878],\n",
            "         [-0.4202,  0.8578,  0.5252,  ..., -0.0350, -0.1225,  0.1751],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  0.1220,  0.1743,  ...,  0.8715,  1.1329,  0.3312],\n",
            "         [-0.3486,  0.8889,  0.9586,  ...,  1.2549,  1.8126,  1.0806],\n",
            "         [-0.3660,  0.9063,  0.8017,  ...,  0.6623,  0.6275,  0.8192],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -5, -1,  1, -1,  0,  2, -6, -4,  2, -2, -6,  5,  4,  0,  9,  7, -2,\n",
            "         1,  0,  1,  4,  5,  7, -1, -1, -4, -1, -5,  2,  7,  1,  5, -4, -3,  1,\n",
            "        -5,  3, -3,  5, -2,  0,  1, -1,  0, -2, -3,  6,  4,  0, -7, -2, -3, -4,\n",
            "         0, -9,  5, -2,  6, -3,  3, -4, -6,  6,  3, -2, -5,  2, -1,  0,  8,  5,\n",
            "        -1, -9, -3,  5, -3, -6,  2,  3,  1,  0, -5, -2,  5,  0,  1,  4,  0,  0,\n",
            "         2,  2, -1, -2, -2,  0,  1,  2,  5,  3, -5, -7,  1, -1, -6, -4,  8, -8,\n",
            "        -1, -1,  8, -7,  3, -1, -1,  1, -2, -1, -1,  2, -8, -3, -4, -7, -2,  3,\n",
            "        -5,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ...,  0.1199, -0.0342, -0.1884],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.6336,  0.5994,  0.1884],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.6507,  0.5994,  0.3939],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.2740,  1.5070,  1.5926],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.5480,  0.9247,  1.1645],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  0.5252,  0.3501,  0.1225],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.1730,  1.1380,  0.6653],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.1204,  1.1380,  0.9279],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.1225,  1.9258,  1.9258],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  0.0000,  1.5056,  1.6982],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  0.9935,  0.7146,  0.4009],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.7429,  1.6558,  1.0980],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.6209,  1.6035,  1.3595],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.9935,  2.1264,  1.9695],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.7843,  1.8649,  1.8998],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -2,  7,  6, -5, -4, -3, -2, -2,  4,  3,  1,  0, -1, -2,  0,  1, -1,\n",
            "         4,  3,  0,  6,  4,  8,  7, -6, -6,  4, -3, -1,  5,  1,  6, -4,  2,  4,\n",
            "        -1, -2, -1,  5,  5,  2, -6, -2, -8,  1,  0,  5,  3,  3, -3, -4, -2, -7,\n",
            "         6, -2,  0, -6,  7, -6,  2, -3,  2,  8, -3,  5,  2,  0,  7,  6,  2,  5,\n",
            "         0, -5,  1,  6, -7, -5, -2, -1,  8,  0,  1, -9, -1, -3, -5,  5,  2,  1,\n",
            "        -4, -4,  3, -7, -7, -7, -8,  6,  3,  2, -6, -9, -3,  2, -2,  0,  0, -4,\n",
            "        -7, -1,  7, -6,  1, -1, -3, -2, -4, -2,  4,  5, -4,  0,  1,  0, -3,  6,\n",
            "        -7, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.1199,  1.7296,  1.2159],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.6850,  0.6165,  0.2911],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.7296, -0.8391, -0.8049]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.0350,  1.5406,  1.1204],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.4727,  0.7878,  0.5602],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.6282, -0.7178, -0.5602]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.1394,  0.6100,  0.2440],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -0.4706,  0.1046, -0.0174],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.7081, -0.9935, -0.8366]]])\n",
            "tensor([ 2, -2,  1,  7, -5,  3, -3, -9, -2,  6,  5, -6,  0, -2, -1,  4,  8,  1,\n",
            "         5,  4,  3,  3,  4, -1,  6,  0,  2,  2,  1,  5,  7, -7,  4,  2, -2, -1,\n",
            "         2,  0, -1,  2,  3,  3, -5,  0, -1,  4, -4, -1,  6,  0, -7, -4, -5, -8,\n",
            "         0, -4, -1, -3,  3, -3, -1, -2, -6,  4, -1, -3, -5,  3,  1,  7,  6,  1,\n",
            "         1, -4,  2, -1, -4, -2,  3,  3, -1,  0, -3, -1,  1, -4,  2,  3,  5,  0,\n",
            "        -2, -3,  5,  0, -2, -8, -6, -2,  7,  1, -5, -4, -3,  5, -7,  1, -1, -9,\n",
            "        -7,  1,  8, -6,  1,  5, -1, -2, -4, -2,  5,  2, -2,  2,  1, -1,  3,  0,\n",
            "        -6,  1])\n",
            "tensor([[[ 2.7057,  2.8941,  2.6886,  ...,  0.4966, -2.6030, -2.5345],\n",
            "         [ 3.2537,  3.5106,  3.1510,  ...,  0.8220, -2.0378, -2.0892],\n",
            "         [ 3.0996,  3.1852,  2.5345,  ...,  1.0617, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.4860,  2.6436,  2.5210,  ...,  0.5427, -2.1884, -2.1709],\n",
            "         [ 3.0287,  3.3263,  3.1162,  ...,  1.1905, -1.4531, -1.5231],\n",
            "         [ 2.9237,  3.1863,  2.7486,  ...,  1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.4575,  2.6667,  2.6144,  ...,  0.9935, -1.7778, -1.7952],\n",
            "         [ 3.0153,  3.3115,  3.2767,  ...,  2.0218, -0.8715, -0.9935],\n",
            "         [ 3.0327,  3.3290,  3.1373,  ...,  2.3878, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -9,  2,  4,  1,  5, -2, -3, -8,  2,  2,  2,  7,  1, -3,  9,  5,  1,\n",
            "         6,  5,  4,  2, -1,  2,  2, -1,  1,  6, -3,  5,  6,  1, -1,  5,  2,  0,\n",
            "        -2, -3, -1,  2,  5,  4, -5,  3, -5,  7,  0,  2,  8, -4, -3, -4, -3,  0,\n",
            "        -2, -5,  2, -4,  2,  0,  6, -1, -3,  3, -2,  2, -2, -1,  8,  6,  0,  6,\n",
            "         3,  0, -1,  5, -7,  1,  1, -4,  1,  3, -3,  0,  5, -2, -6,  3,  2, -4,\n",
            "         1,  1,  2, -4, -7, -3, -6,  3, -2, -2, -5, -1, -5,  1, -5,  0,  7, -6,\n",
            "        -3,  0,  5, -3, -6,  2, -6,  0, -5, -1, -1,  1,  1,  3, -7, -6, -2,  4,\n",
            "        -2, -3])\n",
            "tensor([[[-1.0104,  0.5651,  0.2911,  ..., -0.8391, -1.0617, -0.6679],\n",
            "         [-0.2740,  1.3186,  0.7706,  ..., -0.4281,  0.2226,  0.1712],\n",
            "         [-0.4281,  0.9247,  0.1370,  ...,  0.3939,  0.4281,  0.8562],\n",
            "         ...,\n",
            "         [-3.5619, -1.6611, -0.9590,  ..., -0.9761,  1.9865,  1.5412],\n",
            "         [-3.0825, -0.8562, -0.8220,  ..., -0.7021,  1.0960,  0.7706],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  0.5427,  0.2276,  ..., -0.4027, -0.6478, -0.2976],\n",
            "         [-0.3501,  1.2605,  0.8929,  ...,  0.0525,  0.7353,  0.6653],\n",
            "         [-0.4202,  0.9104,  0.4902,  ...,  0.7353,  0.8053,  1.2255],\n",
            "         ...,\n",
            "         [-2.9762, -0.8578, -0.4027,  ..., -0.8053,  1.9783,  1.5756],\n",
            "         [-2.4335, -0.0700, -0.0700,  ..., -0.3326,  1.2955,  1.0154],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  0.8540,  0.6623,  ...,  0.0523, -0.2266,  0.0871],\n",
            "         [-0.3486,  1.5338,  1.3072,  ...,  0.7146,  1.3595,  1.2200],\n",
            "         [-0.3660,  1.3072,  1.0632,  ...,  1.1852,  1.4989,  1.8649],\n",
            "         ...,\n",
            "         [-1.6732,  1.1678,  1.6558,  ...,  0.3834,  2.6318,  2.0392],\n",
            "         [-1.6732,  1.1329,  1.4989,  ...,  0.6449,  1.8998,  1.4989],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -6,  4,  0,  0,  5, -5, -7,  0,  1,  0, -6, -1, -1,  1,  5,  0,  2,\n",
            "         1,  6,  2,  2,  7,  3,  6, -5, -2,  6,  2, -4,  5, -1,  4,  5, -2,  3,\n",
            "        -6,  4, -4, -3,  4,  5, -2, -4, -8, -2, -1,  6,  1, -2, -3,  0, -4, -3,\n",
            "         2, -8,  8, -6,  8, -9,  2,  0, -1,  8, -3, -3,  3,  1,  4, -1,  6,  0,\n",
            "         2, -7,  4,  4, -1, -1,  2,  2, -1, -1,  1, -9,  8, -6,  1,  4,  3, -6,\n",
            "         4, -4,  3, -6, -9,  1,  0,  1,  1,  0, -6,  0,  1,  4,  0, -2,  1, -1,\n",
            "        -3,  1,  5, -2, -5, -1, -4,  1,  1,  1,  6,  6, -7,  3,  0, -4,  3,  0,\n",
            "         1,  3])\n",
            "tensor([[[-1.0104, -0.7364,  0.0342,  ...,  0.0856,  0.6679,  0.6507],\n",
            "         [-0.2740,  0.0000,  0.5822,  ..., -0.2055,  0.5994,  0.8220],\n",
            "         [-0.4281, -0.2740,  0.0856,  ..., -0.5137, -0.3425,  0.2397],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.7638,  ..., -2.0550, -0.2740, -0.0171],\n",
            "         [-3.0825, -2.9626, -1.7125,  ..., -2.6030, -1.0789, -0.5822],\n",
            "         [-3.0311, -2.8770, -1.6782,  ..., -2.9797, -1.8152, -1.2501]],\n",
            "\n",
            "        [[-1.0854, -0.8053,  0.0000,  ...,  0.4552,  1.1029,  1.0154],\n",
            "         [-0.3501,  0.0000,  0.7003,  ...,  0.3326,  1.1555,  1.4006],\n",
            "         [-0.4202, -0.1225,  0.4377,  ...,  0.0350,  0.1751,  0.8053],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -1.3831,  ..., -1.5931,  0.1225, -0.0350],\n",
            "         [-2.4335, -2.1534, -1.2080,  ..., -1.9783, -0.5077, -0.3501],\n",
            "         [-2.5210, -2.2584, -1.1555,  ..., -2.4685, -1.2780, -0.7703]],\n",
            "\n",
            "        [[-1.0980, -0.7843,  0.0174,  ...,  0.2440,  0.7146,  0.5403],\n",
            "         [-0.3486,  0.0000,  0.7843,  ...,  0.5752,  1.1155,  1.1852],\n",
            "         [-0.3660,  0.0000,  0.7146,  ...,  0.6623,  0.6623,  1.0806],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  0.9935,  ..., -0.7320,  0.3486,  0.1569],\n",
            "         [-1.6732, -0.7320,  0.7669,  ..., -1.2549, -0.2092, -0.1046],\n",
            "         [-2.0218, -1.6383, -0.3312,  ..., -1.9172, -0.8889, -0.5926]]])\n",
            "tensor([ 0, -7,  6, -1, -1,  5, -1, -9,  0,  3, -3, -6, -1, -1, -1,  6,  0, -3,\n",
            "        -3,  6, -4,  2,  8,  8,  7, -5, -5,  5, -3,  5,  7,  1,  7,  5, -1, -4,\n",
            "         3,  2, -4, -1, -2,  0, -4,  1, -6,  2, -6,  8,  8, -2, -2,  0,  3, -6,\n",
            "         1, -9,  7, -4,  4, -3,  1, -4,  3,  2,  4,  2,  0, -1,  8, -2,  3,  6,\n",
            "         3, -7, -3, -3,  2, -3, -5, -2,  6,  2,  3, -5,  0, -3,  2,  5,  7, -4,\n",
            "        -1, -5,  5, -7, -8, -5, -8, -1, -1, -5,  2, -3, -3,  0, -8,  0, -1, -1,\n",
            "         0,  7,  8, -5, -1,  2,  0, -2, -3, -4, -1, -1, -6, -1,  1, -6, -2,  0,\n",
            "         1, -2])\n",
            "tensor([[[ 2.4488,  2.8256,  2.7571,  ..., -1.3529, -2.6030, -2.5345],\n",
            "         [ 3.0996,  3.4934,  3.2023,  ..., -0.9076, -2.0378, -2.0892],\n",
            "         [ 2.7913,  3.1338,  2.7057,  ..., -0.7535, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.4214, -1.6782, -1.9180,  ..., -1.5584, -0.9590, -0.9076],\n",
            "         [-0.8220, -0.6679, -1.2501,  ..., -1.9180, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.0483,  2.4510,  2.5035,  ..., -0.8578, -2.1884, -2.1709],\n",
            "         [ 2.6611,  3.1162,  3.0812,  ..., -0.2451, -1.4531, -1.5231],\n",
            "         [ 2.5210,  2.9762,  2.8186,  ..., -0.0875, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.7703, -0.8578, -1.2955,  ..., -1.1029, -0.5427, -0.5952],\n",
            "         [-0.1050,  0.2101, -0.5077,  ..., -1.3130, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.0915,  2.4924,  2.5795,  ..., -0.4357, -1.7778, -1.7952],\n",
            "         [ 2.7712,  3.1373,  3.1547,  ...,  0.3312, -0.8715, -0.9935],\n",
            "         [ 2.7190,  3.1373,  3.1198,  ...,  0.5229, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.4357,  1.1329,  0.9237,  ..., -0.1046, -0.1220, -0.3486],\n",
            "         [ 0.5577,  1.5163,  1.3072,  ..., -0.4532, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -1,  5,  4, -7, -1, -3, -7, -2,  2,  0, -2, -1,  4, -1,  2,  1, -3,\n",
            "         3,  6,  3, -2,  3, -1,  0, -2,  0,  4, -4, -3, -1, -3,  5,  2,  1,  3,\n",
            "        -3,  2, -8,  2, -1,  0, -1, -1, -6,  6, -2,  1,  8,  3, -2, -8,  1, -5,\n",
            "        -1, -7,  7,  2,  6, -9, -1, -3, -4,  7, -4,  4, -1,  0,  4,  5,  3,  0,\n",
            "         6, -7,  2, -1, -7, -5,  0,  5,  5, -2, -6, -1,  1, -1, -7,  1,  0,  0,\n",
            "         3, -1,  3,  0,  0, -2, -6,  2, -1,  1, -2,  0,  0,  5,  0,  0,  8,  0,\n",
            "        -3,  5,  1, -2,  2,  1,  3, -2,  3, -1,  2,  6, -7, -3, -5,  1, -5,  6,\n",
            "         0,  5])\n",
            "tensor([[[-1.0104,  2.8427,  2.8085,  ...,  1.0104,  0.7706,  0.8220],\n",
            "         [-0.2740,  3.5619,  3.2195,  ...,  1.4556,  1.3015,  1.3186],\n",
            "         [-0.4281,  3.0996,  2.5687,  ...,  1.4042,  1.3015,  1.4899],\n",
            "         ...,\n",
            "         [-3.5619, -1.4727, -1.3700,  ..., -0.2397,  1.5412,  1.5241],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  2.5910,  2.6436,  ...,  1.1730,  0.9629,  0.9804],\n",
            "         [-0.3501,  3.3789,  3.2213,  ...,  1.8032,  1.6632,  1.6807],\n",
            "         [-0.4202,  3.0462,  2.7661,  ...,  1.7332,  1.6807,  1.8732],\n",
            "         ...,\n",
            "         [-2.9762, -0.8053, -0.8929,  ...,  0.0000,  1.7857,  1.6632],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  1.8998,  1.9521,  ...,  1.2723,  1.0109,  0.9412],\n",
            "         [-0.3486,  2.7364,  2.6318,  ...,  2.0392,  1.8824,  1.7952],\n",
            "         [-0.3660,  2.6144,  2.4924,  ...,  1.9869,  1.8998,  2.0044],\n",
            "         ...,\n",
            "         [-1.6732,  0.9586,  1.1329,  ...,  0.6623,  1.7429,  1.4641],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -5,  2,  2,  1,  1,  1, -5,  0,  1,  2, -5, -1,  1, -1,  9,  8, -3,\n",
            "         6,  6, -3,  2,  2,  1,  2, -2, -6,  3, -4,  4,  5, -5,  5, -1, -3, -1,\n",
            "         3,  6,  0,  4, -3,  1,  0,  3, -5,  6,  0,  3,  1, -4, -3, -7, -2,  1,\n",
            "        -1, -2,  6, -3,  8, -1,  6, -9, -4,  2,  1, -1,  3, -1,  7,  0,  1,  1,\n",
            "         6, -1, -4,  5, -2,  1,  1, -4,  8, -3, -2, -1,  1, -7, -7,  2,  0, -1,\n",
            "         1, -2,  3,  0,  0, -8, -5,  3,  2, -3,  0, -6,  3,  3,  0,  2,  5, -5,\n",
            "        -7, -1,  9,  0, -4,  1, -5,  1, -1,  3,  6,  7, -7, -5, -4, -6, -2,  1,\n",
            "        -8,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -1.0446,  0.6165,  0.9247],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -1.0960,  0.4624,  0.7192],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.6782, -0.5480,  0.0514]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.6478,  1.0329,  1.2430],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.5252,  1.0679,  1.2430],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.1905, -0.0350,  0.5602]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.1220,  0.7843,  0.7843],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -0.2266,  0.8017,  0.7669],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.1678, -0.2266,  0.1220]]])\n",
            "tensor([ 2, -9, -1,  2, -5, -4,  0, -2, -3, -1,  2,  1, -1,  2,  2,  1,  6, -2,\n",
            "         1, -3, -3,  0,  4,  8,  1, -7, -6,  2, -4, -2,  8,  0,  4, -2, -4, -2,\n",
            "        -2,  6, -2, -1,  0,  5,  0,  2, -7,  7, -1,  4,  3, -4,  0, -2, -4, -8,\n",
            "         1, -4,  0, -4,  1, -2,  6, -2,  3,  7, -3,  5, -5,  3,  7,  4,  6,  0,\n",
            "         4,  0,  2,  6, -3, -1,  3, -4,  4,  2,  0, -9,  7, -4, -1, -2,  3, -5,\n",
            "        -2, -4,  7, -5, -5, -8, -6,  2, -1,  4,  2,  0, -6,  6, -8, -5,  5, -5,\n",
            "        -1,  2,  6,  1, -3,  1,  1, -4,  1, -2,  4,  0,  1,  0, -5,  1,  0, -1,\n",
            "        -1, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.1884, -0.0514,  0.0000],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.4624,  0.5480,  0.4624],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.5480,  0.5137,  0.6679],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  1.0329,  1.1730,  1.1905],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.8207,  1.9083,  1.8032],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.8557,  1.8557,  2.0133],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  1.8824,  2.0044,  1.9695],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  2.8235,  2.9107,  2.7538],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  2.8758,  2.8758,  2.9804],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-6, -9,  5,  4,  1,  3, -4, -7, -9,  4,  3, -1, -2, -1, -4,  3,  7,  2,\n",
            "         6,  6,  2, -1,  1, -1,  6, -1, -7,  6, -3, -4,  8, -3,  0, -1, -4, -5,\n",
            "         1,  3, -8, -3,  4,  5, -4, -3, -3,  1, -3,  0,  7, -1, -5, -7, -4, -3,\n",
            "         5, -5,  7, -3,  3, -9,  3,  0, -1,  3, -2, -2, -5,  8,  1,  6,  6,  5,\n",
            "         0,  0, -4,  5, -1, -2, -3, -1,  2, -4,  2, -7,  2, -3,  0, -2,  7,  1,\n",
            "        -3, -5,  8,  0,  0,  0, -1,  3, -2,  0, -6, -4, -5,  0, -5,  0,  6, -8,\n",
            "        -6,  6,  5, -7, -6,  2, -1,  1,  0,  0,  5,  6, -1, -2, -8, -3, -4,  7,\n",
            "        -5,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.1063, -1.9693, -1.8837,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-1.7810, -1.6782, -1.8837,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-1.8323, -1.6954, -1.8666,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.5581, -1.2430, -1.3655,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-1.1905, -0.9279, -1.2780,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-1.3831, -1.1380, -1.3480,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.2614,  0.8366,  1.0109,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-0.3660,  0.5752,  0.8017,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-0.7843, -0.4183, -0.2789,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-6, -8,  8,  7, -6, -3,  0, -3, -4,  1,  4, -3, -2,  4,  2,  2,  6,  3,\n",
            "         3,  2,  1,  2,  7,  4, -1, -5,  2, -1, -5,  1,  6, -8,  1,  1, -7,  3,\n",
            "        -5, -3, -5, -1, -1,  1, -2,  4, -9,  6,  1,  7,  3,  3, -2, -5,  2, -2,\n",
            "         5, -4, -1, -6,  3,  0,  7, -8, -2,  2,  3,  2,  0,  1, -1,  5,  0,  3,\n",
            "        -1, -3, -1,  1,  1, -6, -5, -2,  4,  0, -2, -5,  2, -1, -5, -3,  4, -6,\n",
            "         1,  2,  8, -2, -9, -3, -2,  0,  1,  3, -6, -3, -2,  5, -1, -2,  0, -4,\n",
            "        -8,  4,  3,  1, -2,  4, -2,  0,  3,  0,  1,  6, -2, -5, -1,  0, -1,  0,\n",
            "         1,  4])\n",
            "tensor([[[ 1.7296,  1.7981,  1.9693,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 2.5002,  2.7057,  2.4488,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 2.3118,  2.3975,  1.9865,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.5756,  1.7507,  2.1534,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.3634,  2.6786,  2.7311,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 2.2584,  2.5035,  2.4510,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 1.4118,  1.6732,  2.2832,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 2.2658,  2.5795,  2.8932,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 2.2309,  2.5272,  2.8061,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -6,  7,  5, -4,  4, -6, -7, -6,  5, -4, -3,  2, -3, -4,  4,  7,  0,\n",
            "        -2, -2,  4,  2,  7,  8,  3, -1, -6,  1,  2,  5,  7, -8,  4,  4, -3, -2,\n",
            "        -6,  3, -1, -2,  2,  2, -7, -4, -4, -2,  1,  1,  2, -2, -1, -3,  0, -4,\n",
            "         5, -5,  4, -6,  3, -9,  6, -2,  3, -1,  1,  4, -2,  2, -1,  6,  6,  1,\n",
            "        -2, -9, -1,  0, -7,  0, -5, -3,  0,  1, -2, -2,  3, -6, -2,  1,  5, -3,\n",
            "         2,  3,  2, -1, -6,  0, -1,  4,  2,  1,  3, -3, -5, -1, -9, -1,  3, -1,\n",
            "        -1,  4,  5, -6, -1,  6, -3,  1,  1,  3,  6,  7, -2, -1,  1,  1,  2,  1,\n",
            "         0, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -1.8323, -1.7296,  ..., -0.8049,  1.0446,  1.0960],\n",
            "         [-3.0825, -1.6269, -1.8837,  ..., -1.4214,  0.1884,  0.5137],\n",
            "         [-3.0311, -1.6954, -2.0207,  ..., -2.1748, -1.0104, -0.4452]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -0.7003, -0.7178,  ...,  0.0000,  1.8207,  1.7682],\n",
            "         [-2.4335, -0.4902, -0.7878,  ..., -0.4552,  1.1380,  1.3655],\n",
            "         [-2.5210, -0.7178, -0.9979,  ..., -1.2955, -0.1225,  0.4377]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  0.6972,  0.7495,  ...,  1.0109,  2.2135,  2.0566],\n",
            "         [-1.6732,  0.5752,  0.6797,  ...,  0.2963,  1.4815,  1.6209],\n",
            "         [-2.0218, -0.3137, -0.3137,  ..., -0.7669,  0.1917,  0.4357]]])\n",
            "tensor([-4, -5,  1,  2, -8,  3, -1,  0, -6, -1, -2,  0,  0,  2,  0,  9,  1,  1,\n",
            "         6,  4,  4,  4,  2,  1,  6,  0,  0,  5, -2, -4,  8,  1,  1, -1,  1, -1,\n",
            "        -6, -3,  0,  4,  4,  0, -7, -4, -3, -1,  1,  0, -1,  5, -3, -3, -3, -4,\n",
            "        -1, -8,  5, -7,  1, -4,  1, -4,  3,  5, -5,  0, -2,  4,  4,  3,  8,  0,\n",
            "         3,  0,  3,  2,  1,  3,  1,  1,  7,  4, -4, -7,  2,  0,  2, -1,  2, -3,\n",
            "         0,  4, -1, -3, -5, -3, -3,  7, -2, -1,  0, -1, -6, -1, -8, -3,  8, -3,\n",
            "         1,  3,  1, -7,  1,  6, -6,  2,  1,  1,  0,  5, -2,  3, -5, -4, -4,  6,\n",
            "        -8,  1])\n",
            "tensor([[[-1.0104,  2.2091,  2.0721,  ..., -1.1816, -1.0960, -1.3871],\n",
            "         [-0.2740,  2.8770,  2.5858,  ..., -0.5309, -0.5309, -0.9590],\n",
            "         [-0.4281,  2.6201,  2.0550,  ..., -0.3939, -0.5822, -0.7535],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  1.9608,  1.9083,  ..., -0.7703, -0.7003, -0.9979],\n",
            "         [-0.3501,  2.6786,  2.5735,  ...,  0.0525,  0.0350, -0.3676],\n",
            "         [-0.4202,  2.5735,  2.2409,  ...,  0.1751,  0.0350, -0.1050],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  1.4989,  1.5338,  ..., -0.8017, -0.8540, -1.1852],\n",
            "         [-0.3486,  2.1961,  2.2309,  ...,  0.1743,  0.0697, -0.3834],\n",
            "         [-0.3660,  2.2135,  2.0915,  ...,  0.2963,  0.0871, -0.1046],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-4,  0,  6,  2, -2,  3, -6, -6, -5,  0,  0, -6,  2,  6,  0,  3,  3, -2,\n",
            "         3, -3, -2,  5,  3,  8,  4, -4,  2,  5, -3, -4,  3, -5,  7, -2,  0, -5,\n",
            "        -1,  0, -6,  2, -3, -4, -1,  5, -9,  1, -3,  5,  0,  4, -2, -1,  0,  1,\n",
            "         6, -4,  2, -2,  0, -6,  5, -2,  0,  8, -5,  3,  3,  7,  1,  5,  8,  6,\n",
            "         3, -4, -5,  0, -5,  0, -5, -2,  6,  4, -3,  0,  6, -6, -5,  5,  5, -1,\n",
            "        -1,  4,  1, -1, -1, -2, -8,  4,  5,  4, -1, -5, -1,  0, -8, -2,  4, -1,\n",
            "        -3,  0,  7, -4, -1, -1,  0,  1,  3, -2,  2,  4, -2, -5, -5, -1, -6,  4,\n",
            "        -7,  2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.2911,  ..., -2.0892, -2.0207, -2.0721],\n",
            "         [-0.4281, -0.2740, -0.8220,  ..., -2.0036, -2.0378, -1.8495],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3736,  ..., -2.6886, -0.8905, -0.8905],\n",
            "         [-3.0825, -2.9626, -3.1681,  ..., -3.1338, -1.6440, -1.4042],\n",
            "         [-3.0311, -2.8770, -3.0482,  ..., -3.2880, -2.5858, -2.0721]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1225,  ..., -1.5231, -1.4356, -1.5056],\n",
            "         [-0.4202, -0.1225, -0.4552,  ..., -1.4531, -1.4531, -1.2605],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8011,  ..., -2.3284, -0.5077, -0.5777],\n",
            "         [-2.4335, -2.1534, -2.5035,  ..., -2.5910, -1.0679, -0.9104],\n",
            "         [-2.5210, -2.2584, -2.4685,  ..., -2.8011, -2.0658, -1.5756]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0174,  ..., -0.9412, -0.8540, -0.9760],\n",
            "         [-0.3660,  0.0000, -0.1220,  ..., -0.8540, -0.8540, -0.7146],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4357,  ..., -1.2200, -0.0697, -0.3312],\n",
            "         [-1.6732, -0.7320, -0.5054,  ..., -1.6383, -0.5752, -0.5752],\n",
            "         [-2.0218, -1.6383, -1.4989,  ..., -2.0218, -1.4641, -1.2200]]])\n",
            "tensor([-4, -7,  3,  6, -7, -2, -5, -8, -7,  3,  5, -5, -1,  0,  0,  2,  8,  3,\n",
            "        -3,  5, -3,  2,  5,  7, -2, -6,  0, -1,  3,  1,  7, -2,  4,  2, -5, -3,\n",
            "        -3,  2, -2,  4,  4,  3,  1,  0, -3,  2, -7,  1,  7,  2, -7,  0, -3, -5,\n",
            "         0, -8,  2, -6,  2, -9,  1, -6, -1,  3, -3,  1, -5,  2,  7,  4, -1,  1,\n",
            "        -1, -5,  0,  0, -7, -1, -5,  3, -1,  4,  2, -5,  8, -2, -2,  5,  7, -1,\n",
            "         3, -4,  6, -6,  0, -8, -4,  1,  2,  3, -3, -8, -1,  0, -3,  1,  2, -8,\n",
            "        -8,  3,  8, -6, -5, -1,  0, -6,  1, -2,  7,  8, -6, -1, -3,  0, -4,  1,\n",
            "        -4, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.6097,  1.6611,  1.4727,  ..., -0.5994, -0.7192, -2.0892],\n",
            "         [ 0.8220,  1.2330,  1.0446,  ..., -0.2569, -0.4452, -1.8666],\n",
            "         ...,\n",
            "         [-1.5926, -1.5412, -1.4214,  ..., -0.7021,  0.7364, -0.9076],\n",
            "         [-1.4385, -1.4556, -1.2501,  ..., -1.1474,  0.1199, -1.4214],\n",
            "         [-1.6954, -1.3357, -1.3529,  ..., -1.5755, -0.7877, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 2.7661,  2.9762,  2.9412,  ...,  0.6127,  0.5602, -1.5231],\n",
            "         [ 1.9433,  2.5385,  2.5035,  ...,  0.9804,  0.8403, -1.2780],\n",
            "         ...,\n",
            "         [ 0.7353,  0.9804,  0.9279,  ...,  0.7878,  2.2234, -0.5952],\n",
            "         [ 0.7703,  0.9104,  0.9629,  ...,  0.4902,  1.7332, -0.9279],\n",
            "         [ 0.1401,  0.5777,  0.4902,  ..., -0.0525,  0.7353, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.9237,  1.0283,  1.0632,  ..., -0.1743, -0.2963, -0.9935],\n",
            "         [ 0.5054,  0.8017,  0.6972,  ...,  0.1917, -0.0349, -0.7320],\n",
            "         ...,\n",
            "         [-0.7843,  0.0523,  0.1743,  ...,  0.1220,  0.8889, -0.3486],\n",
            "         [-1.1329, -0.4706,  0.0523,  ..., -0.3312,  0.4880, -0.5926],\n",
            "         [-1.7952, -1.2898, -1.1678,  ..., -1.0458, -0.4009, -1.2549]]])\n",
            "tensor([ 0,  0,  5, -1, -7,  0, -2, -2, -4,  2,  1, -1,  2,  1, -1,  4,  1, -3,\n",
            "         3,  1, -2,  6,  0,  6,  6, -4, -6,  3, -2, -1, -1, -3,  3, -4,  1,  2,\n",
            "        -6, -1,  0,  1,  6,  3, -2, -4, -1, -1, -5,  8,  3, -1, -7, -4,  0, -3,\n",
            "         4, -7,  2, -5,  0, -8,  2,  0,  1, -1, -5, -3, -3,  0,  8,  0,  2,  0,\n",
            "         4,  0, -1,  5, -7,  2,  4, -4,  2,  5, -5, -5,  7,  1, -5,  2,  4, -1,\n",
            "         0, -5,  6,  0, -6,  1, -2,  5,  2,  3,  0, -8, -1,  3, -8, -2,  1, -2,\n",
            "        -2,  5,  2, -6, -6,  7, -2, -1,  3,  1,  4,  7, -6, -5, -3, -6, -6,  2,\n",
            "        -6, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.5412,  2.1406,  1.5070,  ..., -1.6269, -1.7981, -1.8666],\n",
            "         ...,\n",
            "         [ 0.1712,  0.4452,  0.3425,  ..., -0.7877,  0.2740, -0.9076],\n",
            "         [ 0.5994,  0.7192,  0.3939,  ..., -0.7706,  0.2226, -1.4214],\n",
            "         [ 0.5309,  0.6679,  0.5822,  ..., -1.8323, -0.5480, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.5756,  2.2234,  1.7682,  ..., -1.0329, -1.1029, -1.2780],\n",
            "         ...,\n",
            "         [ 0.6303,  1.0329,  0.7003,  ..., -0.4552,  0.7703, -0.5952],\n",
            "         [ 1.1204,  1.3480,  0.8578,  ..., -0.3676,  0.7703, -0.9279],\n",
            "         [ 0.9454,  1.1555,  1.0154,  ..., -1.5056, -0.1401, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.0283,  1.3595,  1.0109,  ..., -0.4532, -0.5577, -0.7320],\n",
            "         ...,\n",
            "         [ 1.2026,  2.4749,  2.3181,  ...,  0.6275,  1.2026, -0.3486],\n",
            "         [ 1.1503,  2.1089,  2.0915,  ...,  0.4357,  1.1503, -0.5926],\n",
            "         [ 0.7146,  1.0458,  1.2375,  ..., -1.0632,  0.3660, -1.2549]]])\n",
            "tensor([-2, -8,  7,  2, -8,  1, -7, -7, -1,  6,  5,  1,  7,  0, -3,  2,  4, -4,\n",
            "        -3, -3, -3,  2, -1,  6,  6, -8, -7, -2,  4,  5,  3,  0,  0,  0,  1,  3,\n",
            "        -2,  5, -8, -1,  5, -3, -7, -3, -3,  7, -7,  3,  7,  0, -8,  0, -2, -5,\n",
            "         2, -6,  5, -5,  0, -3,  7, -9,  3,  0,  4,  4,  3,  3,  8,  1,  3,  3,\n",
            "         6, -8,  1, -2,  1,  0, -1, -2,  0,  2,  2, -7, -1, -4, -3,  3,  6, -3,\n",
            "        -1, -4,  1, -3,  0, -2, -6,  7,  6,  3,  3, -2, -1,  6, -3, -4,  0, -6,\n",
            "        -8,  0,  6, -2,  0,  8, -3, -1,  1, -4,  2,  7, -7, -2, -7, -3, -1,  5,\n",
            "        -6, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.5480, -0.3939, -0.6336,  ...,  0.1884, -0.9590, -0.9076],\n",
            "         [-0.3082, -0.3425, -0.7021,  ..., -0.3082, -1.6611, -1.4214],\n",
            "         [-0.4795, -0.6165, -0.9076,  ..., -0.6679, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.0875,  0.2276, -0.1751,  ...,  0.4727, -0.5427, -0.5952],\n",
            "         [ 0.1401,  0.2451, -0.2451,  ...,  0.0175, -1.0854, -0.9279],\n",
            "         [-0.1926, -0.2276, -0.5602,  ..., -0.4202, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.8192,  1.9172,  1.8301,  ...,  1.1155, -0.1220, -0.3486],\n",
            "         [ 0.4532,  1.2375,  1.3420,  ...,  0.6623, -0.5926, -0.5926],\n",
            "         [-0.1394, -0.0523, -0.0174,  ..., -0.1046, -1.4641, -1.2549]]])\n",
            "tensor([ 1, -9,  1,  5,  1, -1, -1, -6, -9,  1,  5, -6, -1,  6,  1,  9,  4,  5,\n",
            "        -1,  6, -2,  3,  7,  4, -1, -6,  0,  3, -2,  4,  5,  0,  4,  0, -6,  1,\n",
            "        -2,  4, -5,  6,  0, -2,  2,  2, -5,  1, -1,  8,  0,  3, -5,  0,  3, -1,\n",
            "        -1, -3,  4, -5,  4, -1, -1, -7, -1,  1,  4,  5, -6,  1,  2,  6, -1,  6,\n",
            "         5, -5,  2,  6, -2, -3, -3, -2,  7,  1,  1,  0,  2,  0,  0,  3, -1, -6,\n",
            "         4, -5,  8, -2, -2, -7,  0,  2,  3,  1,  0, -8, -3,  7, -3,  1,  7, -5,\n",
            "        -8,  2,  2, -7, -6, -1, -4, -5, -6, -3,  6,  0, -2,  2, -2, -8,  3,  1,\n",
            "        -6, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.2501,  1.4727,  0.8049,  ..., -1.1645, -1.5584, -1.4214],\n",
            "         [ 0.9076,  0.9932,  0.1027,  ..., -0.7021, -1.2672, -0.9076],\n",
            "         ...,\n",
            "         [-0.0342,  0.2055,  0.6336,  ...,  1.2672,  3.0996,  3.2023],\n",
            "         [ 0.2397,  0.6165,  0.7192,  ...,  0.9590,  2.4146,  2.5345],\n",
            "         [-0.0171,  0.7535,  0.8220,  ...,  0.5822,  1.7125,  2.1235]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 1.7157,  2.0483,  1.5406,  ..., -0.2101, -0.7178, -0.6303],\n",
            "         [ 1.4706,  1.7507,  1.0679,  ...,  0.2801, -0.3852, -0.0525],\n",
            "         ...,\n",
            "         [ 0.2276,  0.5952,  0.8578,  ...,  1.6982,  3.4314,  3.3088],\n",
            "         [ 0.5077,  1.0504,  0.9979,  ...,  1.5581,  2.9937,  3.0462],\n",
            "         [ 0.1926,  1.0504,  1.1730,  ...,  1.1204,  2.2759,  2.6786]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.0349,  0.4009,  0.3312,  ..., -0.3834, -0.6100, -0.5926],\n",
            "         [-0.2266,  0.2614,  0.1220,  ..., -0.0697, -0.4183, -0.0697],\n",
            "         ...,\n",
            "         [-0.4532,  0.5752,  0.9237,  ...,  1.1503,  2.2135,  2.0566],\n",
            "         [-0.8017,  0.2266,  0.5403,  ...,  1.2375,  2.1438,  2.0218],\n",
            "         [-1.0632, -0.2789,  0.0871,  ...,  0.7495,  1.6558,  1.7952]]])\n",
            "tensor([-5, -2,  1,  3, -2,  5, -4, -2, -4,  0,  0, -2, -2,  5, -1,  6,  1, -3,\n",
            "        -3,  3,  2,  1,  8,  8,  5, -6, -5,  3, -1,  1,  0,  1,  8,  5,  0,  3,\n",
            "         3,  5, -9,  0, -1, -1, -4,  2, -3,  5, -4,  0,  0, -1, -4, -5, -6, -7,\n",
            "         5, -6,  5, -6,  4, -4, -1, -3,  0,  8, -2,  2, -4,  8,  2,  7,  1,  5,\n",
            "         7, -6, -5,  1, -5,  0, -4, -2,  5,  2, -2, -1,  1, -8, -1,  2,  8, -1,\n",
            "        -2, -4,  4, -4, -4,  0, -7,  1,  6,  4,  2, -5, -6,  6, -8, -4,  2, -8,\n",
            "         1,  1,  4, -3,  1,  7, -5, -2, -6, -3,  3,  1,  1,  0, -5, -4,  1,  2,\n",
            "         0, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.2226, -0.0514, -0.6679,  ..., -1.7125, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [ 0.5651,  0.2740, -0.4452,  ..., -2.4317, -0.9590, -0.9076],\n",
            "         [ 0.5309, -0.2911, -1.7810,  ..., -2.8427, -1.6611, -1.4214],\n",
            "         [-1.3357, -0.9590, -1.0789,  ..., -3.3907, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.1926,  0.1225, -0.2801,  ..., -1.1555, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 1.2430,  1.1029,  0.1050,  ..., -1.8382, -0.5427, -0.5952],\n",
            "         [ 1.2080,  0.5077, -1.2080,  ..., -2.1359, -1.0854, -0.9279],\n",
            "         [-0.8754, -0.3852, -0.5952,  ..., -2.8186, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.0523,  0.3312,  0.1220,  ..., -0.3137, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 2.5621,  3.2070,  2.4227,  ..., -0.6100, -0.1220, -0.3486],\n",
            "         [ 2.0044,  1.9695,  0.7843,  ..., -0.9935, -0.5926, -0.5926],\n",
            "         [-0.3137,  0.2963,  0.4009,  ..., -1.7778, -1.4641, -1.2549]]])\n",
            "tensor([-6, -6,  0,  4, -6,  5, -7, -8, -5, -2,  0,  3,  6, -3,  5,  8,  7, -3,\n",
            "         1, -1,  2,  4,  5,  3,  5, -5,  0, -1, -5,  4,  8,  1,  5,  5,  2, -1,\n",
            "        -6, -2, -4,  0,  6,  2, -3, -3, -7,  4, -3,  7,  3,  0, -2, -9,  0, -7,\n",
            "        -1, -2,  5, -3,  3, -5,  5, -7, -4,  0, -5,  0, -5,  1,  5,  2,  6,  3,\n",
            "         4, -6,  3,  1,  1, -6,  3,  3,  4, -2, -6,  0,  5,  0, -6,  2,  3,  1,\n",
            "        -3, -1,  4, -2, -6, -7, -8,  3,  7, -1,  1, -8, -6,  4, -2, -3,  0, -3,\n",
            "        -3,  4,  1, -4,  2,  2,  2, -4, -4,  1,  4,  2, -3, -4, -2, -7,  1,  2,\n",
            "        -4,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.4899, -1.4727, -1.3700,  ..., -0.7364, -0.9590, -0.9076],\n",
            "         [-1.1816, -0.9761, -1.1302,  ..., -1.2501, -1.6611, -1.4214],\n",
            "         [-0.7021, -0.4624, -0.6679,  ..., -1.7810, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.6828, -0.5252, -0.5602,  ..., -0.1401, -0.5427, -0.5952],\n",
            "         [-0.2626,  0.0700, -0.1926,  ..., -0.5252, -1.0854, -0.9279],\n",
            "         [ 0.0000,  0.2801,  0.1401,  ..., -1.1555, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.4706,  0.3486,  0.6275,  ...,  0.1569, -0.1220, -0.3486],\n",
            "         [-0.6100,  0.3312,  0.7146,  ..., -0.3486, -0.5926, -0.5926],\n",
            "         [-0.6623, -0.2789, -0.1394,  ..., -1.0283, -1.4641, -1.2549]]])\n",
            "tensor([-1, -2,  7, -2,  0, -3, -4, -2, -8,  2, -1, -5, -2, -3, -3,  5,  7, -4,\n",
            "         6,  0, -4,  4,  3, -1,  3, -1,  0,  5, -5,  4,  6, -3,  0,  4, -7, -3,\n",
            "        -4,  6, -5, -1, -2,  4, -3,  5, -9,  6, -8,  1,  8,  0, -2, -1,  1, -1,\n",
            "         1, -2,  6, -1,  9,  0,  2, -4, -4,  6, -5,  1, -5,  6,  5, -2,  4,  3,\n",
            "        -2, -3, -3,  4, -3,  1, -5, -3,  7,  2, -2,  0,  4,  1, -3,  0,  6, -2,\n",
            "        -4, -1,  3, -9, -5, -4, -4,  2,  1, -2, -1, -1,  0,  2, -4,  2,  1, -1,\n",
            "        -4, -1,  7, -4,  3,  6, -6,  0, -5, -3,  6,  0, -8, -5, -1, -8, -2,  2,\n",
            "        -1, -4])\n",
            "tensor([[[-0.0856,  0.3767,  0.3939,  ..., -2.1406, -2.6030, -2.5345],\n",
            "         [ 0.7706,  1.2159,  1.0617,  ..., -1.3529, -2.0378, -2.0892],\n",
            "         [ 0.7364,  1.0275,  0.6165,  ..., -1.1131, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.7296, -1.4214, -1.1987,  ..., -1.4042, -0.9590, -0.9076],\n",
            "         [-1.2844, -0.9419, -0.9761,  ..., -1.8666, -1.6611, -1.4214],\n",
            "         [-1.2501, -0.8562, -0.8562,  ..., -2.4831, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-0.3501,  0.2451,  0.3852,  ..., -1.8908, -2.1884, -2.1709],\n",
            "         [ 0.6303,  1.2080,  1.2255,  ..., -0.8578, -1.4531, -1.5231],\n",
            "         [ 0.7528,  1.2255,  1.0154,  ..., -0.5777, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.1380, -0.6828, -0.6653,  ..., -0.8929, -0.5427, -0.5952],\n",
            "         [-0.5952, -0.1225, -0.3326,  ..., -1.2080, -1.0854, -0.9279],\n",
            "         [-0.6653, -0.2101, -0.2801,  ..., -1.9083, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.5403,  0.0697,  0.2614,  ..., -1.5686, -1.7778, -1.7952],\n",
            "         [ 0.4009,  1.0109,  1.1678,  ..., -0.5752, -0.8715, -0.9935],\n",
            "         [ 0.5926,  1.1329,  1.1329,  ..., -0.3486, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.0697,  1.1852,  1.4815,  ..., -0.0523, -0.1220, -0.3486],\n",
            "         [-0.0871,  1.0458,  1.4292,  ..., -0.5229, -0.5926, -0.5926],\n",
            "         [-0.4532,  0.1220,  0.4183,  ..., -1.3943, -1.4641, -1.2549]]])\n",
            "tensor([-4, -8, -1, -1, -6, -3, -6, -9, -1,  0, -1,  2, -2, -2,  2,  3,  2,  2,\n",
            "         0,  2,  4,  5,  3, -1, -2, -2, -7,  5, -3,  5,  4, -2,  0,  4, -5, -3,\n",
            "        -3,  5, -7,  6, -2,  0,  0,  5, -9,  7,  0,  2,  8,  5,  1, -9, -5, -2,\n",
            "         1, -8,  1,  1,  3, -4,  4, -6, -3,  3,  4,  2, -5,  5,  5,  0,  8,  1,\n",
            "         3, -9, -3,  1, -7, -4, -1,  4,  5, -1, -1, -7,  5,  0, -2,  5,  2, -4,\n",
            "        -3, -3,  0, -2, -1, -5, -5,  2,  6, -1,  1, -1, -2,  6, -9, -2,  7, -8,\n",
            "        -7,  4,  1,  1,  1,  5, -4, -7, -4,  1,  6, -1,  1,  3, -7, -7,  2,  5,\n",
            "        -4,  2])\n",
            "tensor([[[ 0.7535,  0.9076,  0.7706,  ..., -1.1987, -1.0617, -2.5345],\n",
            "         [ 1.3700,  1.6097,  1.4042,  ..., -0.6165, -0.4966, -2.0892],\n",
            "         [ 1.2159,  1.4042,  0.7877,  ..., -0.4795, -0.4281, -1.8666],\n",
            "         ...,\n",
            "         [-2.1063, -2.0550, -2.0721,  ..., -1.8323,  0.0856, -0.9076],\n",
            "         [-1.5926, -1.4727, -1.7125,  ..., -2.1063, -0.5480, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.4202,  0.5777,  0.5427,  ..., -0.9804, -0.8228, -2.1709],\n",
            "         [ 1.0329,  1.3480,  1.3480,  ..., -0.2276, -0.0875, -1.5231],\n",
            "         [ 0.9629,  1.2955,  0.9279,  ..., -0.1050, -0.0175, -1.2780],\n",
            "         ...,\n",
            "         [-1.6457, -1.4181, -1.6282,  ..., -1.5406,  0.3676, -0.5952],\n",
            "         [-1.0679, -0.7878, -1.1555,  ..., -1.6807, -0.1050, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.1220,  0.3137,  0.3312,  ..., -0.8366, -0.6623, -1.7952],\n",
            "         [ 0.7495,  1.0632,  1.1329,  ...,  0.0697,  0.2440, -0.9935],\n",
            "         [ 0.7320,  1.1503,  0.9237,  ...,  0.2092,  0.3312, -0.7320],\n",
            "         ...,\n",
            "         [-0.3312,  0.6797,  0.7146,  ..., -0.4009,  0.8366, -0.3486],\n",
            "         [-0.2963,  0.6449,  0.8192,  ..., -0.6797,  0.4357, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -2,  0,  2,  0, -1, -5, -7, -6,  1,  5, -4,  7,  2, -1,  2,  8,  1,\n",
            "         5,  4,  3,  3,  7,  0,  1, -3, -7,  1, -2,  2,  1, -2,  4,  4,  0, -5,\n",
            "        -4,  0, -5,  0,  5,  5, -7,  2, -6,  1, -4,  0,  7,  1, -1, -5, -2, -5,\n",
            "         3, -9,  3, -2,  9, -7, -2, -7,  0,  8,  3,  2,  2,  6,  6, -1,  4,  6,\n",
            "         1, -9,  4,  1, -6,  1, -3,  3,  4, -3,  2, -3,  7, -8, -3,  0,  3, -7,\n",
            "        -1,  2,  4, -3, -2, -2, -4, -1,  5, -5, -3, -8, -3,  3, -4, -1,  1, -7,\n",
            "        -3,  4,  2, -1, -2,  6, -4, -1, -1,  3,  8, -1, -4, -6,  1, -6,  0,  3,\n",
            "        -8,  1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.5755,  1.8666,  1.5584,  ..., -0.2397, -2.0378, -2.0892],\n",
            "         [ 1.4214,  1.6097,  1.0446,  ..., -0.1370, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.7467, -1.6097, -1.5584,  ..., -0.9076, -0.9590, -0.9076],\n",
            "         [-1.3015, -1.1474, -1.3700,  ..., -1.3186, -1.6611, -1.4214],\n",
            "         [-1.2501, -1.0789, -1.2672,  ..., -1.8837, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 1.8382,  2.2059,  2.0658,  ...,  0.6303, -1.4531, -1.5231],\n",
            "         [ 1.7682,  2.1008,  1.7507,  ...,  0.7178, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.9279, -0.6127, -0.7528,  ..., -0.2626, -0.5427, -0.5952],\n",
            "         [-0.4202, -0.1050, -0.4727,  ..., -0.5252, -1.0854, -0.9279],\n",
            "         [-0.5077, -0.2276, -0.4552,  ..., -1.1555, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 2.1612,  2.5447,  2.5447,  ...,  1.6209, -0.8715, -0.9935],\n",
            "         [ 2.1612,  2.5621,  2.4227,  ...,  1.7255, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.6797,  1.7778,  1.9172,  ...,  1.1678, -0.1220, -0.3486],\n",
            "         [ 0.6449,  1.6209,  1.8301,  ...,  0.7320, -0.5926, -0.5926],\n",
            "         [ 0.2963,  0.6972,  0.8192,  ..., -0.1046, -1.4641, -1.2549]]])\n",
            "tensor([-4, -8,  6,  3, -7,  3,  0, -8, -7, -1, -1,  3, -1,  4,  3,  9,  8, -1,\n",
            "        -2,  0, -3,  1,  2,  7,  4, -2, -3,  7,  4, -1,  3, -7,  1,  2, -3, -3,\n",
            "        -6,  6, -8, -3,  2, -1,  2, -2, -3,  7, -6,  8,  3,  4,  0, -8, -2, -8,\n",
            "        -2, -3,  1, -1,  5, -8,  0,  0, -1, -1, -2,  1, -1, -1,  1,  0,  7, -3,\n",
            "         5, -7,  1,  0, -6, -3, -5,  5,  2, -2, -1, -6,  5, -6, -4, -3,  1, -7,\n",
            "         4,  3, -1, -9, -6,  0,  1,  4, -1, -4, -1,  0,  3,  7, -5, -4,  4, -5,\n",
            "         0, -1,  5, -5,  3,  3, -6, -5, -5, -2,  1, -1, -2, -3, -6, -8,  2,  0,\n",
            "        -1, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.2569,  0.0514,  0.1370],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.5309,  0.5994,  0.6336],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.3254,  0.3596,  0.5651],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.1370,  1.0617,  0.5822],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.6828, -0.3326, -0.2276],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.2976,  0.3501,  0.4552],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -0.0350,  0.1401,  0.5077],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.2626,  1.2955,  0.7878],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.0283, -0.8017, -0.7320],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.0000, -0.0349,  0.0871],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.0697, -0.0697,  0.3660],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.1678,  1.6732,  1.0109],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -9,  4,  3,  1,  4,  2, -1, -9,  6,  2,  1,  1,  6,  4,  7,  4,  2,\n",
            "         0,  4, -4,  4,  8,  8,  7, -1,  1,  0,  2, -2,  7, -6, -1,  1, -2,  3,\n",
            "        -5,  1,  0,  6,  5, -3, -6, -4, -2,  2, -3,  0, -1,  5, -1, -3, -5, -3,\n",
            "        -1,  0,  3, -1,  0, -2,  6, -9, -4,  3,  2, -3,  3,  0,  5, -2,  3,  3,\n",
            "         2, -5, -1,  1,  1, -1, -4,  0, -1, -1,  1, -4,  7,  0,  2,  3,  1, -5,\n",
            "         3,  3, -1, -1, -3, -7, -5,  3,  5,  2,  0, -8, -2,  8, -4, -4,  8, -8,\n",
            "        -3,  7,  4, -5, -5,  6, -3,  0, -1, -3,  5,  7, -1, -6,  1, -3, -6,  2,\n",
            "        -1,  4])\n",
            "tensor([[[-0.7192, -0.6165, -0.6850,  ..., -1.6611, -2.6030, -2.5345],\n",
            "         [-0.0856,  0.1370,  0.1199,  ..., -1.1474, -2.0378, -2.0892],\n",
            "         [-0.2226, -0.0171, -0.2397,  ..., -1.4042, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-0.7528, -0.6127, -0.5252,  ..., -0.8228, -2.1884, -2.1709],\n",
            "         [-0.1751,  0.1751,  0.3852,  ..., -0.2276, -1.4531, -1.5231],\n",
            "         [-0.2801,  0.1225,  0.1751,  ..., -0.7003, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.8192, -0.6275, -0.6100,  ..., -1.1329, -1.7778, -1.7952],\n",
            "         [-0.1569,  0.2092,  0.4706,  ..., -0.1917, -0.8715, -0.9935],\n",
            "         [-0.0871,  0.3660,  0.5926,  ..., -0.3312, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1,  0,  2,  6, -8, -1, -1, -5, -9,  3,  3, -6,  4,  4, -1,  4,  4,  5,\n",
            "         4,  6,  3,  6,  6,  5, -1, -4, -5,  7,  0,  1,  3, -8, -1,  0, -6, -1,\n",
            "         0,  2,  0,  6,  0,  1, -6, -1, -7, -1,  1,  5,  5, -1, -4, -5,  3, -1,\n",
            "         7, -2,  0, -6,  7, -7,  0, -1, -4,  3,  0, -1, -2, -1,  5, -1,  0, -3,\n",
            "         5,  0, -1, -2, -1, -3, -5,  2,  0,  4, -4, -4,  8, -1,  2,  0, -1,  0,\n",
            "         5,  1,  5, -4,  0, -1, -7, -2,  5,  3, -4, -1,  1,  8, -7,  3,  2, -8,\n",
            "        -8,  6,  7, -5, -4,  2, -3, -5,  0,  1,  3,  3,  0, -4, -7, -3, -3, -1,\n",
            "        -3,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 0.2911,  0.7364,  0.9761,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.1541,  1.0960,  2.8427,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.9932, -0.5994, -0.9590,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-1.1131, -0.2226, -0.5822,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-1.5412, -0.8905, -0.8220,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 0.2801,  0.7528,  1.1555,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.2976,  1.3130,  3.2738,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.8228, -0.6127, -1.3305,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-0.7178, -0.0175, -0.7528,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-1.1029, -0.6828, -0.8578,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.3137,  0.7146,  1.0283,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.2789,  1.3072,  3.2767,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.2266,  0.9586,  0.4706,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-0.0697,  1.0109,  0.7146,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-0.5752, -0.2440, -0.2614,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -1,  0,  6, -1, -1,  1, -4, -6,  3,  4, -5,  1,  3,  2,  5,  3,  1,\n",
            "         4,  5, -1, -2,  4, -1,  3, -1, -4,  0,  0,  2,  3, -2,  4,  2, -2, -4,\n",
            "        -4,  2, -1,  1, -1,  1, -2, -3, -3,  1, -6, -1,  3, -1, -1, -6, -3, -3,\n",
            "         5, -6,  5, -3,  9, -2,  7, -1, -3, -1,  4,  5,  0, -1,  1,  4, -1,  3,\n",
            "         0, -9, -1,  5, -7, -6,  3,  0, -1,  2, -1, -1,  4, -6, -2,  5,  5, -4,\n",
            "         5, -5,  1, -9, -6,  1, -2,  5,  0,  2,  1, -9,  1,  8, -7, -4,  1, -5,\n",
            "        -5,  0,  1,  0,  1,  7,  0,  1,  1, -1,  5,  1, -2,  2, -8, -2, -1, -1,\n",
            "        -8,  1])\n",
            "tensor([[[-1.0104,  2.9626,  2.2433,  ...,  1.4556,  1.5926,  1.6782],\n",
            "         [-0.2740,  3.0311,  1.4385,  ...,  1.8837,  1.9351,  1.9008],\n",
            "         [-0.4281,  2.2605,  0.2911,  ...,  1.7810,  1.7467,  2.0036],\n",
            "         ...,\n",
            "         [-3.5619, -0.3596, -1.3186,  ..., -1.0960,  0.6679,  0.9761],\n",
            "         [-3.0825,  0.3254, -0.7192,  ..., -1.0275,  0.4452,  0.8905],\n",
            "         [-3.0311,  0.8734,  0.1884,  ..., -0.6507,  0.4624,  1.0275]],\n",
            "\n",
            "        [[-1.0854,  2.9762,  2.3634,  ...,  2.0133,  2.1534,  2.1534],\n",
            "         [-0.3501,  3.0987,  1.6982,  ...,  2.5910,  2.6786,  2.6085],\n",
            "         [-0.4202,  2.5035,  0.7703,  ...,  2.4860,  2.5035,  2.7311],\n",
            "         ...,\n",
            "         [-2.9762,  0.4377, -0.7703,  ..., -0.6478,  1.1204,  1.3305],\n",
            "         [-2.4335,  1.1730, -0.0700,  ..., -0.4202,  1.0679,  1.4356],\n",
            "         [-2.5210,  1.5406,  0.7703,  ..., -0.1050,  1.0504,  1.5931]],\n",
            "\n",
            "        [[-1.0980,  2.9630,  2.3704,  ...,  2.4749,  2.5969,  2.5621],\n",
            "         [-0.3486,  3.0327,  1.7255,  ...,  3.2767,  3.3638,  3.2070],\n",
            "         [-0.3660,  2.5272,  0.9760,  ...,  3.2418,  3.2767,  3.3987],\n",
            "         ...,\n",
            "         [-1.6732,  2.5272,  1.6209,  ...,  0.4532,  1.5338,  1.5686],\n",
            "         [-1.6732,  2.5970,  1.9346,  ...,  0.5229,  1.5512,  1.7603],\n",
            "         [-2.0218,  2.1612,  1.7429,  ...,  0.6623,  1.6383,  1.9346]]])\n",
            "tensor([ 3, -8,  6,  7, -8,  0, -7, -5,  0,  1,  3, -1,  2, -2,  5,  2,  3,  2,\n",
            "         2,  1, -5,  3,  2,  2,  0, -9, -4,  5, -4,  0,  2, -7,  3, -1, -6,  2,\n",
            "         0, -1, -5, -2,  1,  0, -6,  5, -2, -1, -5,  5,  8, -3, -8, -5,  2,  0,\n",
            "        -2, -3,  6,  2,  3, -1, -1, -2, -1,  1, -5,  5,  0,  4,  7,  6,  1,  1,\n",
            "         0, -8,  0,  5, -3,  2, -1,  5,  8,  4, -4, -8,  2, -7, -6, -3,  5,  1,\n",
            "         5, -4,  3, -7, -5, -1,  0,  3, -2,  1,  0, -5, -5,  8, -4, -2,  7, -4,\n",
            "        -2,  7,  1, -7, -3,  2, -3,  2, -3,  3,  5,  1, -5, -1, -6, -1,  0,  7,\n",
            "        -4,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.1370,  0.6507,  0.8734],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.5137,  1.2159,  1.2501],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.6850,  1.2330,  1.4385],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.2226,  2.0036,  2.0721],\n",
            "         [-3.0825, -2.9626, -3.1852,  ...,  0.0342,  1.4727,  1.7125],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  0.2976,  1.5406,  1.8557],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.1555,  2.3109,  2.4510],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.3480,  2.3459,  2.6611],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  1.1730,  2.9937,  2.9762],\n",
            "         [-2.4335, -2.1534, -2.5210,  ...,  1.2430,  2.7311,  2.8536],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ...,  0.0697,  1.1678,  1.3943],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.0632,  2.1089,  2.1612],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.2723,  2.1438,  2.3529],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.4989,  2.6144,  2.4052],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  1.2898,  2.3007,  2.2832],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -8, -1, -2, -2, -1, -6, -9, -5, -1,  2,  2,  7, -1, -1,  9,  7,  0,\n",
            "         5,  4,  0,  1,  1,  1,  3, -3, -1,  1,  1,  4,  4, -5, -1, -3, -7, -2,\n",
            "         1,  0, -3,  3, -1, -3,  1,  0, -9, -2, -3,  7,  2, -1, -7, -7, -6, -7,\n",
            "         1, -1, -1, -3,  8, -5,  5, -4,  1,  1,  2,  1, -1,  5,  4,  0,  7, -2,\n",
            "         7, -6,  3,  0, -2, -6, -1,  3,  1, -3, -5, -1,  5, -5, -2, -1,  7,  2,\n",
            "         4,  4, -1, -6, -4,  0, -6,  2,  4,  4, -6, -6, -4,  8, -7,  3,  5, -3,\n",
            "        -7,  0,  5, -4,  2,  2, -1, -7,  1, -6,  7,  3, -2,  0, -1,  1,  0,  0,\n",
            "        -8, -3])\n",
            "tensor([[[ 0.2397,  0.5994,  0.5309,  ..., -0.0171,  0.1884, -2.5345],\n",
            "         [ 0.9932,  1.4385,  1.1474,  ...,  1.5241,  1.6782, -2.0892],\n",
            "         [ 0.5822,  1.1302,  0.6679,  ...,  1.6269,  1.6611, -1.8666],\n",
            "         ...,\n",
            "         [-1.8837, -1.7638, -1.7467,  ..., -0.8905,  0.7877, -0.9076],\n",
            "         [-2.3118, -2.1920, -2.4488,  ..., -1.2844,  0.0856, -1.4214],\n",
            "         [-2.6372, -2.5002, -2.6543,  ..., -1.8323, -0.8220, -2.1063]],\n",
            "\n",
            "        [[-0.0700,  0.2976,  0.2801,  ...,  0.0700,  0.2801, -2.1709],\n",
            "         [ 0.6653,  1.2605,  1.1204,  ...,  1.6982,  1.8732, -1.5231],\n",
            "         [ 0.4202,  0.9804,  0.7528,  ...,  1.7682,  1.8382, -1.2780],\n",
            "         ...,\n",
            "         [-1.6807, -1.4006, -1.5756,  ..., -0.6828,  0.9104, -0.5952],\n",
            "         [-1.9783, -1.6982, -2.1534,  ..., -0.9279,  0.3852, -0.9279],\n",
            "         [-2.4860, -2.2234, -2.4685,  ..., -1.5581, -0.5952, -1.6106]],\n",
            "\n",
            "        [[-0.4183, -0.0174,  0.0523,  ..., -0.1743,  0.0349, -1.7952],\n",
            "         [ 0.3486,  0.9237,  0.9237,  ...,  1.8475,  1.9695, -0.9935],\n",
            "         [ 0.1569,  0.8366,  0.8192,  ...,  1.8824,  1.9695, -0.7320],\n",
            "         ...,\n",
            "         [-0.8192,  0.2614,  0.4009,  ..., -0.3834,  0.4183, -0.3486],\n",
            "         [-1.4292, -0.4880, -0.3137,  ..., -0.7146,  0.0000, -0.5926],\n",
            "         [-2.0044, -1.6209, -1.5163,  ..., -1.5163, -0.8540, -1.2549]]])\n",
            "tensor([-3, -3,  2, -2,  0,  5,  0, -2, -3,  0,  5, -4, -2,  3, -2,  7,  5,  4,\n",
            "         5, -2,  4,  4,  4,  3,  2,  0, -1,  2, -5,  3,  3,  1,  0, -3, -4, -1,\n",
            "        -5,  5, -8,  2,  0, -3,  1,  2, -4, -1,  0,  0,  1, -4, -5, -9, -1,  0,\n",
            "        -2, -8,  6, -2,  4, -2,  3,  0, -4,  4,  3, -4, -1,  1,  5,  1,  4,  4,\n",
            "        -2, -6, -4,  4, -1,  2, -4,  3,  5,  1,  1, -1,  6,  0, -2,  1,  5, -3,\n",
            "        -1,  4, -1,  0, -5, -2, -7,  2, -1, -1,  0,  0, -2,  3, -9, -2,  1, -4,\n",
            "        -6,  3,  2, -7, -6,  1,  3, -5,  1, -3, -1,  2, -3,  2, -8,  0,  0,  4,\n",
            "        -7,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -1.8323,  ..., -0.3254,  1.5412,  1.7296],\n",
            "         [-3.0825, -2.9626, -1.5926,  ..., -0.3425,  1.1474,  1.3357],\n",
            "         [-3.0311, -2.8770, -1.7125,  ..., -0.5822,  0.4624,  0.8905]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -0.4552,  ...,  0.1576,  2.0483,  2.1884],\n",
            "         [-2.4335, -2.1534, -0.0525,  ...,  0.2801,  1.8032,  1.9433],\n",
            "         [-2.5210, -2.2584, -0.2451,  ..., -0.0350,  1.0504,  1.4706]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926,  0.8192,  ...,  1.2200,  2.4575,  2.4575],\n",
            "         [-1.6732, -0.7320,  0.8715,  ...,  1.1503,  2.2658,  2.3355],\n",
            "         [-2.0218, -1.6383, -0.2614,  ...,  0.6449,  1.6035,  1.8475]]])\n",
            "tensor([ 3, -4,  1,  3, -8, -1,  1, -8,  0,  1, -1, -4, -1,  3,  2,  6,  3,  1,\n",
            "         3,  3,  1,  6,  3,  3,  2, -5, -4,  6,  2, -4,  4,  1,  2, -2,  2, -2,\n",
            "        -2, -3, -1,  5,  5,  0, -6,  3, -6,  3, -8,  3,  4, -2, -3, -7,  1, -7,\n",
            "         7, -4,  0, -4,  7,  0, -1, -3,  0,  4, -2, -4,  0,  4, -1, -2,  1,  4,\n",
            "         0,  0, -3,  4, -4, -5,  3,  1,  4, -3, -2, -2, -1, -3,  0, -1,  7, -3,\n",
            "        -3, -3,  7, -1, -8, -3,  1,  1,  0,  0,  3, -1, -3,  5,  0,  2,  5, -4,\n",
            "        -6,  4,  2,  0, -1,  8, -4, -4,  1, -6,  2,  1, -1, -1, -1,  1, -5,  4,\n",
            "        -6, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  1.0789,  0.6850,  ..., -1.0104, -0.8220, -0.9590],\n",
            "         [-0.4281,  0.7021,  0.1541,  ..., -1.0960, -0.9590, -0.7706],\n",
            "         ...,\n",
            "         [-3.5619, -1.6097, -1.5755,  ..., -1.0275,  0.7535,  0.8391],\n",
            "         [-3.0825, -1.1302, -1.3871,  ..., -1.4385,  0.0514,  0.3082],\n",
            "         [-3.0311, -0.9932, -1.2501,  ..., -1.9865, -0.8562, -0.3767]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  1.2605,  1.0154,  ..., -0.3151, -0.0525, -0.1926],\n",
            "         [-0.4202,  1.0504,  0.7353,  ..., -0.3676, -0.1926, -0.0175],\n",
            "         ...,\n",
            "         [-2.9762, -0.6653, -0.8228,  ..., -0.4552,  1.3305,  1.3130],\n",
            "         [-2.4335, -0.1401, -0.5427,  ..., -0.7003,  0.8053,  0.9804],\n",
            "         [-2.5210, -0.1926, -0.4902,  ..., -1.3305, -0.1576,  0.2976]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.9063,  0.8192,  ..., -0.2614, -0.0871, -0.2789],\n",
            "         [-0.3660,  0.8889,  0.8017,  ..., -0.1569, -0.1569, -0.1569],\n",
            "         ...,\n",
            "         [-1.6732,  1.6035,  1.7255,  ...,  0.8540,  1.9521,  1.7429],\n",
            "         [-1.6732,  1.4641,  1.6383,  ...,  0.4357,  1.4815,  1.4989],\n",
            "         [-2.0218,  0.6100,  0.6623,  ..., -0.3486,  0.6275,  0.8366]]])\n",
            "tensor([ 2,  0, -1,  2, -3,  5, -2, -3, -2, -1,  0, -2, -1,  5,  3,  4,  6,  1,\n",
            "        -1,  3,  0,  4,  6,  7, -2, -1, -5,  5, -4,  5,  5, -7,  6,  5, -3,  4,\n",
            "        -2, -1, -3,  5,  6, -2, -6, -2, -5,  3, -2,  7, -1,  3, -7, -6, -2, -3,\n",
            "         0, -1,  1,  0,  0, -8,  2,  0,  2,  8,  4,  2, -2,  3,  2, -1,  0, -2,\n",
            "         3, -4,  1, -3,  1, -1, -4,  3,  1, -3,  3, -9, -1, -3, -3,  4,  3, -2,\n",
            "         5,  2,  3, -2, -2, -4, -7,  1,  2, -4,  0, -6, -2,  3, -8, -2,  6, -6,\n",
            "        -6, -2,  7, -1, -1,  4,  0, -6, -6, -6,  2,  6, -1, -3, -2, -3,  1,  1,\n",
            "        -5, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.0856,  0.0342, -0.0171],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.4966,  0.1884,  0.2397],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.6507,  0.5651,  0.0685],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.6850,  0.2911, -0.2740],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -0.9932, -0.2397, -0.5994]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.6828,  0.7353,  0.6478],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  0.9804,  0.8403,  0.9279],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -0.1576,  1.3130,  1.1029],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.2626,  0.9979,  0.8053],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.8053,  0.2626,  0.3676]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.2898,  1.4118,  1.3595],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.5338,  1.5163,  1.6383],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  0.9412,  1.9521,  1.8649],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.4357,  1.4466,  1.4641],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -0.4183,  0.6100,  0.8715]]])\n",
            "tensor([ 0,  0, -1, -1, -2,  2,  0,  0, -2,  3, -3, -1,  2, -3,  4,  0,  0,  5,\n",
            "         6,  0,  1,  0,  1,  1, -2, -9, -6,  6, -5,  3,  3, -5,  3, -3, -1, -1,\n",
            "        -4, -3, -3, -3,  1,  3, -3,  1, -3,  5,  1,  8,  1, -1, -6, -4,  1, -7,\n",
            "         1, -7,  6, -2,  7, -4, -2,  0,  3,  3, -2,  5, -1,  1,  8,  7,  7,  1,\n",
            "         1, -5,  0,  0, -2,  3, -5,  3,  7,  1,  3, -4,  7, -5,  1,  1,  2, -1,\n",
            "         5,  2,  0, -1, -2, -7, -1,  0,  6, -5, -2, -7, -1,  1, -9, -5,  6, -8,\n",
            "        -7,  4,  4, -8,  3,  8, -6, -1,  0,  2,  2,  7,  1, -3, -4, -3,  0,  5,\n",
            "        -2,  0])\n",
            "tensor([[[ 2.7913,  3.1167,  3.0311,  ...,  1.1474,  1.2844, -2.5345],\n",
            "         [ 3.4763,  3.7674,  3.4592,  ...,  1.7467,  1.8323, -2.0892],\n",
            "         [ 3.3393,  3.5106,  2.9455,  ...,  1.7981,  1.7810, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 2.7311,  3.0637,  3.0462,  ...,  1.6106,  1.7857, -2.1709],\n",
            "         [ 3.4489,  3.8165,  3.6590,  ...,  2.3985,  2.5210, -1.5231],\n",
            "         [ 3.4139,  3.7290,  3.3613,  ...,  2.4510,  2.4685, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.8061,  3.1721,  3.2244,  ...,  2.1089,  2.2484, -1.7952],\n",
            "         [ 3.5556,  3.9041,  3.8867,  ...,  3.0501,  3.1547, -0.9935],\n",
            "         [ 3.5556,  3.9390,  3.7821,  ...,  3.1024,  3.1198, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -2,  0,  3, -1,  1, -6, -4, -1,  7, -1, -6,  3,  6,  5,  6,  7,  4,\n",
            "         1,  0,  0, -1,  5,  5, -1, -8, -2,  7, -1, -2,  1,  0,  5,  1, -1, -5,\n",
            "        -3,  5, -6,  3,  5,  1, -1,  3, -2,  7, -5,  5,  2, -2, -8, -7,  2, -1,\n",
            "         3, -8,  3,  0,  2, -9,  4, -8, -6,  4, -2, -3, -6,  5,  3,  4,  2,  4,\n",
            "         1, -7,  0,  2, -2, -4, -1,  4,  2,  0,  3, -9,  1, -2, -5, -3,  0, -1,\n",
            "         2, -5,  1, -1, -5, -5, -2,  6,  1,  2,  2, -6, -2,  4,  0, -3,  6, -5,\n",
            "        -5,  4,  4, -6, -6,  7, -2, -5, -6, -2,  6,  8, -1, -4, -7, -1, -2,  3,\n",
            "        -7, -3])\n",
            "tensor([[[-1.0104, -0.1370, -0.1884,  ..., -2.0207, -2.0892, -2.1235],\n",
            "         [-0.2740,  0.6336,  0.3767,  ..., -1.6097, -1.5412, -1.6440],\n",
            "         [-0.4281,  0.3254, -0.2226,  ..., -1.5412, -1.2330, -1.2672],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.2976, -0.3676,  ..., -1.7507, -1.8032, -1.8908],\n",
            "         [-0.3501,  0.5427,  0.3151,  ..., -1.0854, -1.0329, -1.1555],\n",
            "         [-0.4202,  0.4027, -0.0700,  ..., -1.0154, -0.8403, -0.7528],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.3312, -0.2614,  ..., -1.2723, -1.3595, -1.5163],\n",
            "         [-0.3486,  0.4706,  0.4532,  ..., -0.4706, -0.3834, -0.6275],\n",
            "         [-0.3660,  0.4880,  0.1917,  ..., -0.2789,  0.1569, -0.1220],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-5, -3,  8,  0, -4, -4, -2, -7, -2,  2,  3,  1,  5,  0,  4,  5,  6,  3,\n",
            "         3,  6,  3,  0, -1,  4,  5, -7, -1, -1, -5,  0,  3, -5,  8,  0, -4, -4,\n",
            "         1,  1,  0,  6,  6,  2, -4,  0, -9,  1,  1,  3,  6, -2, -6, -7, -4, -5,\n",
            "         0, -5,  2, -1,  9, -2,  5, -3, -2,  0, -1, -3, -2,  6,  5,  0,  4,  6,\n",
            "        -2, -7,  1,  1,  2,  0, -2,  1,  0,  2, -5, -3,  8, -7, -4, -1,  0, -6,\n",
            "        -4,  1,  7, -4,  0, -3,  1,  5, -1,  4, -2, -5,  2,  7, -3,  1,  2, -5,\n",
            "        -8,  5,  3, -6, -2,  2,  3,  2, -5, -2,  0, -1, -4, -5,  1, -1,  2,  5,\n",
            "        -7,  3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -0.8220, -0.7535,  ...,  0.1370,  1.9351,  2.0036],\n",
            "         [-3.0825, -0.4281, -0.6507,  ..., -0.3596,  1.1302,  1.3871],\n",
            "         [-3.0311, -0.4966, -0.6850,  ..., -0.9419,  0.1370,  0.6336]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -0.1751, -0.2801,  ...,  0.4552,  2.3284,  2.2934],\n",
            "         [-2.4335,  0.3151, -0.0350,  ...,  0.1401,  1.6457,  1.8207],\n",
            "         [-2.5210,  0.0525, -0.1751,  ..., -0.5077,  0.5952,  1.0504]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  1.7255,  1.8301,  ...,  1.3420,  2.4401,  2.2658],\n",
            "         [-1.6732,  1.5338,  1.7255,  ...,  0.9586,  2.0218,  2.0044],\n",
            "         [-2.0218,  0.5054,  0.6100,  ...,  0.1394,  1.0980,  1.2898]]])\n",
            "tensor([ 3, -9,  3,  3,  1,  1, -2, -8, -2, -2, -2, -6, -2,  3,  4,  6, -1, -2,\n",
            "         3, -3, -4, -1, -1,  6,  0,  0,  1,  6,  1,  3,  7,  1,  1,  0, -3, -1,\n",
            "        -5,  0, -4, -2,  3,  5, -4,  2, -7,  3, -4,  8,  8,  2, -8, -1, -6, -5,\n",
            "         0, -4,  2,  1,  7, -6, -2, -9, -5,  0,  1,  4, -2,  1,  2,  5,  3,  6,\n",
            "         1,  0, -2,  4,  0,  2, -2, -4,  0,  5, -3, -9,  3, -5, -1,  0,  5, -4,\n",
            "        -2,  4, -1, -5, -6, -4, -5,  0,  2, -3,  1, -6, -6,  2, -6, -1, -1, -1,\n",
            "        -4, -2,  4, -3,  3,  5, -3, -5,  0, -2,  1,  2, -1,  0, -3, -8,  2,  2,\n",
            "        -1, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.5173, -2.0721, -1.4727],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -0.9247, -0.6850, -0.4110],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -0.5309, -0.4281, -0.1027],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7228, -0.8562, -0.8562],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1338, -1.5755, -1.3700],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -1.6106, -1.2780, -1.1029],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -0.4202, -0.2801, -0.2626],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -0.3676, -0.3151,  0.0350],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -1.7157,  0.1751,  0.0525],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -1.9958, -0.3852, -0.2626],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.0980, -0.8366, -0.7843],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.1569,  0.1046, -0.0697],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  0.0349, -0.0523,  0.3137],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -0.3312,  0.8715,  0.5752],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -0.7843,  0.3660,  0.3486],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -9,  3,  1,  1, -2,  0, -9, -6,  7, -3,  3,  7, -2,  1,  7,  6,  0,\n",
            "         5, -1,  0, -1,  4,  2,  1, -4,  2,  7, -4,  4,  0,  0,  3,  1, -3, -5,\n",
            "        -3,  0, -9,  6,  2,  4, -6,  1, -3,  7, -6,  1,  8, -3, -6, -6, -6, -8,\n",
            "         5, -9,  2,  0,  4, -5,  1, -3, -6,  8,  0, -1, -1,  2,  6,  1,  7, -2,\n",
            "         5, -9, -2,  2, -2, -5, -4,  4,  1,  0,  2, -8,  3, -7, -1,  6,  5, -6,\n",
            "         1, -5,  4, -3, -5, -2,  0,  0,  2, -5,  1, -6, -1,  4, -3, -1,  8, -1,\n",
            "        -1,  2,  5, -2,  0,  7, -1, -6,  1, -5,  6,  7, -1, -2, -6,  0,  3,  8,\n",
            "        -1,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.1027,  ..., -0.4452, -0.4110, -0.2397],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -2.7057,  ..., -1.7981, -0.0514, -0.0685],\n",
            "         [-3.0825, -2.9626, -2.5002,  ..., -2.3461, -0.9076, -0.7192],\n",
            "         [-3.0311, -2.8770, -2.3632,  ..., -3.0140, -1.9693, -1.5241]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225,  0.3151,  ..., -0.5427, -0.5427, -0.4027],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.1183,  ..., -1.3655,  0.4027,  0.2976],\n",
            "         [-2.4335, -2.1534, -1.8207,  ..., -1.7682, -0.2801, -0.1576],\n",
            "         [-2.5210, -2.2584, -1.7682,  ..., -2.4860, -1.4006, -0.9629]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000,  0.2092,  ..., -0.4357, -0.4706, -0.4183],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.1569,  ..., -0.9935,  0.1046, -0.1569],\n",
            "         [-1.6732, -0.7320, -0.2614,  ..., -1.4292, -0.3660, -0.3834],\n",
            "         [-2.0218, -1.6383, -1.2723,  ..., -2.1786, -1.2026, -0.9760]]])\n",
            "tensor([-4, -3,  6, -2, -7, -4,  2, -8, -3,  6,  0,  1,  6,  3, -3,  7,  4,  5,\n",
            "         0,  1, -5,  2,  3,  3,  2, -2, -5,  5, -4,  2,  3, -7,  6,  2,  2,  3,\n",
            "         1,  4,  0,  6, -3, -4,  2, -4, -8, -2, -4, -1,  6, -1, -7, -6, -1, -4,\n",
            "         4, -7,  8, -5,  5, -9,  5, -9, -4, -1, -2, -3, -4,  2,  0,  4,  3,  1,\n",
            "        -1, -3, -2,  2, -7, -3,  3,  0,  0,  4,  0, -7,  0, -8, -5,  2, -1, -7,\n",
            "         1,  1, -1,  0, -9, -3, -1, -2,  2, -1, -5, -7,  1,  3, -7,  3,  8, -3,\n",
            "        -2,  7,  6, -2,  1, -1,  0, -6, -3, -1,  7,  5, -3,  1, -7, -8,  0,  2,\n",
            "        -7, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.7877, -0.8220, -0.8734],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.0514, -0.3596, -0.5137],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.7364,  0.1884,  0.2055],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -0.1926, -0.1926, -0.2626],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  0.8403,  0.4902,  0.3852],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.3831,  0.7878,  0.7878],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.1743, -0.2440, -0.4706],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  0.8889,  0.4880,  0.2092],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.4466,  0.8017,  0.6797],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-2, -4,  0, -2,  1, -4, -5, -6, -5,  4,  3,  2,  2, -3,  1,  3,  1,  3,\n",
            "         6, -2,  4,  0,  7,  5,  7, -1,  2,  6,  3,  0,  5, -1,  6,  5,  2, -5,\n",
            "         0,  3, -7,  1,  6,  2, -7, -2, -2,  4, -2,  3,  0,  3, -8, -1, -3, -3,\n",
            "         5, -4,  8, -2,  2, -2,  6, -6, -1,  3,  1, -3,  1,  5,  7,  5,  0,  0,\n",
            "         3, -2,  0, -2, -4, -5, -2,  3,  4, -2,  2, -2, -1, -7, -2,  2,  2, -5,\n",
            "        -4,  2,  2, -5, -9, -5, -2,  6,  5,  1,  3, -2, -5,  6, -9,  2,  0, -6,\n",
            "        -6, -2,  3, -8, -3, -1, -5, -3, -6, -6,  4,  6, -5, -3, -3,  1,  2,  8,\n",
            "        -7, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -0.8220, -0.8734, -0.7877],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  0.1027, -0.2569, -0.3596],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  0.3767, -0.2226, -0.1541],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ...,  0.0175,  0.0875,  0.1225],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.0854,  0.8754,  0.7528],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.3480,  0.8403,  0.9804],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -0.3486, -0.4532, -0.4532],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.0283,  0.5229,  0.3312],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.3943,  0.5926,  0.5403],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -7,  3,  4, -3, -4,  0, -9, -8,  5, -4, -2,  2,  0, -2,  0, -1, -2,\n",
            "        -3,  2,  3,  0,  8,  0,  4, -8, -5,  4,  4,  1,  8, -5,  2, -2, -1, -3,\n",
            "        -1,  0, -8,  5,  1,  5, -3, -2, -8,  6,  1,  1,  1,  2, -7, -4,  3,  0,\n",
            "         3, -5,  1, -6,  2, -6, -1, -2,  1,  4,  1, -3, -3,  4,  6,  3,  1, -2,\n",
            "         1, -6, -2, -2,  1, -1, -3,  2,  8, -2,  3, -9,  5, -8, -6,  6,  5,  0,\n",
            "         2,  1,  2, -7, -5, -2, -6, -2,  4,  1, -1, -2,  1,  2,  0, -1,  0, -6,\n",
            "        -3, -1,  4,  0, -5,  0,  0, -7, -3, -4,  6,  1,  1, -5, -7, -4, -1,  7,\n",
            "        -1,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.0617, -0.7364, -0.6679,  ..., -0.0514,  1.6954,  1.7125],\n",
            "         [-0.3767, -0.1712, -0.4110,  ..., -0.3939,  0.8734,  1.0275],\n",
            "         [-0.1541, -0.0514, -0.2397,  ..., -1.0960,  0.1370,  0.5309]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-0.2276,  0.2276,  0.0700,  ...,  0.6303,  2.4160,  2.3634],\n",
            "         [ 0.2276,  0.5252,  0.1225,  ...,  0.4202,  1.7857,  1.9083],\n",
            "         [ 0.0350,  0.2101, -0.0350,  ..., -0.3151,  0.9804,  1.3480]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.3137,  1.6383,  1.8126,  ...,  0.7320,  1.8824,  1.7081],\n",
            "         [ 0.6623,  1.7255,  1.9521,  ...,  0.3137,  1.2375,  1.2200],\n",
            "         [ 0.5577,  0.9412,  1.0980,  ..., -0.6100,  0.4532,  0.5403]]])\n",
            "tensor([-4,  0,  4,  2, -1,  1, -7, -2, -9,  5, -4, -6,  3,  2, -2,  0,  7, -2,\n",
            "         2, -3, -4, -1,  7,  0,  3, -6, -1,  7, -1,  4,  2, -8, -1,  5, -4, -2,\n",
            "        -2,  2, -2, -1, -1,  2, -2,  4,  0,  6, -6,  1,  3,  1, -1,  0, -4, -7,\n",
            "         4, -4,  1, -2,  6, -2,  5, -4, -6,  1,  4,  3, -3,  2,  6,  6,  4, -2,\n",
            "         2, -4,  3,  4,  1,  0,  2, -3,  3, -4, -2, -4,  8,  0,  0,  5,  2, -2,\n",
            "         3,  1,  8, -6, -1, -5,  0, -1,  5,  1, -1, -5, -3,  8, -7,  0,  8, -8,\n",
            "        -3,  2,  7, -5, -2,  8,  0, -2,  3,  0,  6,  4, -5, -3,  1, -7, -5,  6,\n",
            "         0, -3])\n",
            "tensor([[[-0.2911, -0.4795, -0.7192,  ..., -1.9351, -2.1920, -1.7125],\n",
            "         [ 0.1884,  0.6165,  0.1884,  ...,  0.7192, -0.3082, -1.0104],\n",
            "         [ 0.2397,  0.4795, -0.2569,  ...,  0.4966, -0.6336, -1.3871],\n",
            "         ...,\n",
            "         [-2.9283, -1.1987,  0.0342,  ...,  0.8562,  2.3975,  2.0892],\n",
            "         [-1.7125, -0.0171, -0.0514,  ...,  0.4110,  1.5241,  1.6440],\n",
            "         [-0.7021,  0.0342, -0.3082,  ..., -0.2569,  0.5822,  1.3186]],\n",
            "\n",
            "        [[-0.2276, -0.5077, -0.4377,  ..., -1.5756, -1.4181, -0.1050],\n",
            "         [-0.0350,  0.4027,  0.3501,  ...,  1.1204,  0.4027,  0.4027],\n",
            "         [-0.0350,  0.2626, -0.0525,  ...,  0.7003, -0.1751, -0.3501],\n",
            "         ...,\n",
            "         [-2.5385, -0.5777,  0.7178,  ...,  1.2780,  3.1338,  2.9937],\n",
            "         [-1.0504,  0.9279,  0.9979,  ...,  0.9979,  2.4685,  2.6261],\n",
            "         [ 0.1576,  1.1730,  1.0329,  ...,  0.4902,  1.5931,  2.2234]],\n",
            "\n",
            "        [[ 0.1743, -0.1394, -0.0697,  ..., -0.8889, -0.5403,  1.1155],\n",
            "         [ 0.3834,  0.7320,  0.7669,  ...,  1.9172,  1.3595,  1.6383],\n",
            "         [ 0.4009,  0.6972,  0.5229,  ...,  1.4989,  0.7146,  0.7320],\n",
            "         ...,\n",
            "         [-0.6972,  1.9695,  3.3464,  ...,  2.5447,  3.8519,  3.4510],\n",
            "         [ 0.2440,  2.8584,  3.3290,  ...,  2.1786,  3.2418,  3.1024],\n",
            "         [ 1.1155,  2.3355,  2.4575,  ...,  1.6035,  2.5098,  2.6667]]])\n",
            "tensor([ 2, -3,  0,  1,  0,  1, -1, -8, -2, -1, -4, -4,  4,  0, -4,  6,  4, -3,\n",
            "         2,  2, -4,  3,  7,  4,  5, -4,  1, -2,  3,  1,  3, -1,  0,  5, -3, -2,\n",
            "        -1,  4, -9,  0,  0,  3,  1,  1,  0, -2,  1, -1, -1,  0, -7, -7, -6,  1,\n",
            "        -2, -2,  0, -3,  1, -8, -2, -1,  0,  8, -3, -1, -4,  0, -1,  3,  5, -2,\n",
            "        -1, -3,  3, -1, -1,  1, -5, -2,  4,  1, -6, -8, -1,  0,  2,  1,  7,  0,\n",
            "         1, -2, -1, -2, -7,  1, -6,  1,  5,  4, -2, -5,  0,  3, -8,  3,  5, -7,\n",
            "        -7,  2,  1,  0,  2,  0, -3,  0, -1,  0,  7,  1, -2, -5, -3, -6, -4,  0,\n",
            "        -6, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.0036, -1.9351, -1.8495,  ..., -0.6165,  1.4042,  1.9351],\n",
            "         [-1.5241, -1.3529, -1.6782,  ..., -0.5480,  0.9076,  1.3871],\n",
            "         [-1.4556, -1.3700, -1.6269,  ..., -0.7021,  0.1884,  0.7021]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.1884, -1.8382, -2.0133,  ...,  0.0000,  1.9783,  2.3109],\n",
            "         [-1.5756, -1.2605, -1.7332,  ...,  0.3676,  1.8207,  2.0483],\n",
            "         [-1.6807, -1.5056, -1.7507,  ..., -0.0700,  0.9804,  1.3831]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.4466, -0.3312, -0.2440,  ..., -0.4183,  0.8540,  1.0632],\n",
            "         [-1.3769, -0.4183, -0.2789,  ..., -0.0697,  0.8192,  0.8366],\n",
            "         [-1.7778, -1.4641, -1.3072,  ..., -0.5403,  0.2614,  0.3486]]])\n",
            "tensor([-4, -9, -1,  7,  1, -3, -4, -4, -9,  2,  5, -1,  6, -2,  0,  7,  6,  1,\n",
            "         0,  1,  4,  2,  5,  6, -2, -1,  1, -1, -1,  2,  1, -7,  2,  4,  0,  1,\n",
            "        -6,  2, -9,  2, -1, -1, -3,  0,  0, -1, -2,  0, -1,  1,  0, -7, -1, -2,\n",
            "        -1, -9,  5, -6,  6, -4,  5,  0,  3,  6, -3, -4,  1,  5,  5, -1,  3,  5,\n",
            "        -2, -2, -5, -3, -6, -3, -3,  1, -1, -3,  0, -7,  2, -4, -7,  0,  4,  1,\n",
            "        -2, -2,  2, -9,  0, -2,  1,  2,  5, -4, -4, -9,  3,  0, -7,  2,  5,  0,\n",
            "        -6, -2,  6, -1, -2,  2, -4,  1, -4,  1, -1,  2, -1, -1, -8, -3, -6,  8,\n",
            "        -6, -3])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  2.0721,  2.1063,  2.3290],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -0.2397,  1.6954,  1.7981],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.7021,  0.8905,  1.1987],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.2159, -0.0856,  0.4966]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  2.7136,  2.7836,  3.0112],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  0.0875,  2.0308,  2.0308],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.2976,  1.3130,  1.5406],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -0.8754,  0.2801,  0.8228]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  3.2593,  3.3290,  3.5033],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  1.1678,  2.4052,  2.2309],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.6449,  1.7952,  1.8649],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -0.1046,  0.8540,  1.1329]]])\n",
            "tensor([ 2, -6,  0,  4, -3, -1,  2, -8, -9,  6,  5,  3,  4,  5, -3,  5,  1,  0,\n",
            "         5,  5,  4,  1,  6,  5,  4, -5,  2,  7,  0,  4,  6, -4,  2,  0, -2, -2,\n",
            "         0, -3,  0,  5,  3,  5,  2, -4,  0,  2, -1, -1,  7, -4,  1,  0,  2, -7,\n",
            "        -1, -1,  7, -3,  2, -9,  0, -3, -1,  5, -3,  0, -1,  1,  4, -2, -1, -3,\n",
            "         7, -6, -5,  1, -2, -6, -1, -3,  3,  3, -5,  0,  4, -7, -5,  5,  3, -3,\n",
            "         3,  3, -1, -7, -7, -7, -5,  5, -2, -5,  0, -9, -4,  8, -7, -5, -1, -3,\n",
            "        -4,  3,  8, -6,  1, -1,  3,  1, -1,  1,  3,  4, -3,  3, -8, -2,  1,  1,\n",
            "        -1, -4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -0.6507, -0.8562,  ..., -1.2330,  0.3254,  0.4624],\n",
            "         [-3.0825, -0.7192, -0.3767,  ..., -1.8495, -0.4795, -0.2911],\n",
            "         [-3.0311, -0.9932, -0.5994,  ..., -2.2947, -1.0275, -0.4966]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -0.0175, -0.4552,  ..., -0.9104,  0.6653,  0.7178],\n",
            "         [-2.4335, -0.0700,  0.0700,  ..., -1.3831,  0.0175,  0.1225],\n",
            "         [-2.5210, -0.6127, -0.2976,  ..., -1.9958, -0.6828, -0.1926]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  1.8824,  1.7255,  ...,  0.0174,  0.9063,  0.7843],\n",
            "         [-1.6732,  1.1852,  1.9172,  ..., -0.6275,  0.3137,  0.2440],\n",
            "         [-2.0218, -0.1394,  0.5229,  ..., -1.4466, -0.3137, -0.0523]]])\n",
            "tensor([-3, -2,  2,  2, -4, -4, -7, -1, -6,  7,  5,  2,  7,  2,  2,  7,  3,  2,\n",
            "        -2,  0, -5,  3,  7,  4, -1, -3,  0, -1, -4,  5, -1, -6,  4,  4, -3, -4,\n",
            "         3, -1, -4,  4,  1, -3, -5,  3, -5,  5, -4,  4,  4, -4, -4, -5, -3, -3,\n",
            "        -1, -6,  2, -3,  7, -6,  7, -7,  0,  3, -5, -1, -6,  1,  2, -1,  0,  6,\n",
            "         4,  0,  2, -3, -6,  2,  2, -2,  7,  5,  2, -3, -1, -6, -6, -3,  7, -6,\n",
            "         4, -1,  6, -6, -5,  1, -5,  1,  1,  0, -2,  0,  3,  4, -4, -4, -1, -1,\n",
            "         1,  0,  2, -1,  3,  2, -1, -1, -2,  0,  1,  8,  1,  1, -1, -1, -6, -1,\n",
            "        -5,  4])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ...,  1.4042,  1.4727,  1.3529],\n",
            "         [-0.4281, -0.2740, -0.8391,  ...,  1.5755,  1.4385,  1.5755],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -1.9351, -0.0342,  0.7877],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -1.8666,  0.1541,  0.8049],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.4727, -0.3082,  0.2055]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ...,  1.1380,  1.1905,  1.0679],\n",
            "         [-0.4202, -0.1225, -0.4727,  ...,  1.1730,  1.1730,  1.3130],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -1.9258, -0.0875,  0.4202],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -1.8732,  0.1225,  0.6478],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.6632, -0.4727,  0.0350]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ...,  1.0458,  1.0458,  0.8715],\n",
            "         [-0.3660,  0.0000, -0.1394,  ...,  1.1678,  1.0632,  1.1503],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.0806,  0.0174,  0.0349],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.3769,  0.0523,  0.2092],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -1.6558, -0.6797, -0.4706]]])\n",
            "tensor([ 3, -4, -1,  3, -8,  1, -2, -2, -8,  6, -2, -2, -2,  1,  0,  3,  2,  5,\n",
            "        -3,  6, -3,  7,  2,  7,  7, -6, -6,  3, -5, -1,  6, -2,  0, -4, -3, -1,\n",
            "         3, -2, -8, -3, -3,  5, -7, -3, -3,  1, -4, -1,  2,  0, -4, -4, -3,  0,\n",
            "         5,  0,  7, -1,  7, -6,  3, -3, -1,  0, -4, -3, -2,  1,  8, -1,  4,  3,\n",
            "         5, -8, -1, -2, -7, -4, -3,  1,  8,  2, -6, -8,  2, -7, -6, -1,  0,  1,\n",
            "         1,  1,  8, -7, -5, -6,  0,  6,  1,  2, -5, -2, -5,  6, -4,  3,  6, -9,\n",
            "        -1,  6,  3, -2, -5,  1, -4, -2,  3,  2,  3,  7,  1, -2, -6, -3,  3,  2,\n",
            "        -1,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 3.8531,  4.0243,  3.4592,  ...,  2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-1.6954, -1.5755, -1.5412,  ..., -1.1816, -0.9590, -0.9076],\n",
            "         [-1.2330, -1.0960, -1.3186,  ..., -1.4556, -1.6611, -1.4214],\n",
            "         [-1.2159, -1.0275, -1.1816,  ..., -1.9351, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 3.9916,  4.2892,  3.9391,  ...,  2.6261, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.0854, -0.7878, -0.9454,  ..., -0.7528, -0.5427, -0.5952],\n",
            "         [-0.5602, -0.2626, -0.6303,  ..., -0.8754, -1.0854, -0.9279],\n",
            "         [-0.6828, -0.3852, -0.5777,  ..., -1.4356, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 4.0261,  4.3922,  4.2702,  ...,  3.3290, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 0.1394,  1.2200,  1.3420,  ...,  0.3137, -0.1220, -0.3486],\n",
            "         [ 0.1394,  1.0980,  1.3072,  ...,  0.0349, -0.5926, -0.5926],\n",
            "         [-0.2266,  0.1917,  0.3486,  ..., -0.6972, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -6,  6,  7, -6,  3,  1,  0, -9,  5, -1, -6, -1,  4, -3,  4,  5,  4,\n",
            "        -1,  0, -1, -1,  5,  1,  4, -9, -4,  3, -4, -4,  8, -1, -1, -4,  2, -2,\n",
            "        -2,  6, -1,  5, -3, -3, -1, -3, -5,  7, -8,  6,  7, -3, -8,  0, -1, -6,\n",
            "         2, -5,  6, -7,  4, -1,  7, -6, -5,  1, -3,  0, -2, -1,  3,  3,  0,  4,\n",
            "         1, -4,  2, -1, -3, -2,  0,  2,  3, -1,  2, -5,  5, -2, -2, -3,  3,  2,\n",
            "         5,  2,  7, -3, -2, -5, -5,  3,  7,  1, -6, -8, -3,  7,  0,  2,  7, -9,\n",
            "        -2,  4,  0,  1,  2,  5, -2,  0,  1,  3,  8,  2, -8,  3, -3, -3,  0,  4,\n",
            "        -8,  5])\n",
            "tensor([[[ 0.7192,  1.2330,  1.1131,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 1.2330,  3.2708,  2.5345,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 1.5412,  4.0757,  2.6886,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.8403,  1.3831,  1.4006,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 1.2080,  3.3613,  2.7836,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 1.5931,  4.2892,  3.0987,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.4880,  1.0283,  1.0806,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 1.1155,  3.3115,  2.8061,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 1.5163,  4.4270,  3.4684,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -1, -1, -1, -8,  0, -7, -6, -6,  0, -3, -5,  2,  6,  5,  4,  1, -3,\n",
            "        -2, -1,  2,  2,  4,  4,  4, -6, -6,  1, -4, -2,  0, -3,  2, -1, -5,  0,\n",
            "         0,  1, -4, -1,  4,  2, -3,  2, -8,  0, -1,  8, -1, -4, -4,  0, -3, -3,\n",
            "         0, -5,  7,  2,  0, -1,  0, -4, -5,  5,  1, -1, -3,  2,  1,  4,  5,  1,\n",
            "         5, -4, -4, -3, -2,  3, -5,  0, -1, -4,  0, -3,  0, -3, -7,  2,  1, -2,\n",
            "         0, -2,  6, -7, -4, -4, -2,  5,  1, -4,  3, -2, -2,  8, -9, -5,  4, -8,\n",
            "        -1,  2,  7, -2, -5, -1,  0, -3,  0, -6,  8,  1, -8,  0, -6,  1, -5,  7,\n",
            "        -5,  0])\n",
            "tensor([[[-1.0104,  2.0550,  1.8837,  ...,  0.0514,  0.1541,  0.2055],\n",
            "         [-0.2740,  2.7400,  2.4488,  ...,  0.6850,  0.7364,  0.6850],\n",
            "         [-0.4281,  2.4146,  1.8837,  ...,  0.8391,  0.7706,  0.9247],\n",
            "         ...,\n",
            "         [-3.5619, -0.7877, -0.7021,  ..., -0.0342,  1.6440,  1.6611],\n",
            "         [-3.0825, -0.2911, -0.4795,  ..., -0.4452,  0.9761,  1.2159],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854,  2.0483,  1.9608,  ...,  0.4727,  0.6303,  0.6303],\n",
            "         [-0.3501,  2.8011,  2.6786,  ...,  1.2955,  1.3831,  1.3130],\n",
            "         [-0.4202,  2.6261,  2.3109,  ...,  1.4181,  1.4181,  1.5756],\n",
            "         ...,\n",
            "         [-2.9762,  0.0000, -0.1050,  ...,  0.4027,  2.1183,  2.0308],\n",
            "         [-2.4335,  0.5427,  0.2101,  ...,  0.1401,  1.6106,  1.7682],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980,  2.0218,  2.0044,  ...,  0.9063,  1.0458,  1.0109],\n",
            "         [-0.3486,  2.7538,  2.7712,  ...,  1.8824,  1.9521,  1.8126],\n",
            "         [-0.3660,  2.7015,  2.5970,  ...,  2.0044,  1.9521,  2.0392],\n",
            "         ...,\n",
            "         [-1.6732,  2.0915,  2.2658,  ...,  1.5512,  2.5272,  2.2658],\n",
            "         [-1.6732,  1.9695,  2.2135,  ...,  1.1155,  2.0915,  2.0915],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -9, -1,  2, -7, -3,  1, -6, -7,  3, -3, -2, -1,  5,  1,  8,  1, -3,\n",
            "         0,  2, -4,  2,  1,  1,  5, -2,  1, -2, -1, -2,  1, -1, -1,  1, -5, -5,\n",
            "        -6, -3, -3,  0,  4,  3,  1,  2,  0, -2, -8,  1,  1,  2, -5, -5, -6, -1,\n",
            "         4, -7,  4, -1,  1, -1,  6, -9, -1,  7,  2, -3,  3,  2,  2,  2,  2,  3,\n",
            "        -2, -3,  3, -3,  2, -3, -5, -4,  5, -4,  1, -6,  3, -4,  1, -2,  3,  2,\n",
            "        -1, -4,  1,  0, -8, -1, -4,  5,  2, -3, -6, -9, -4,  2, -2, -1,  8, -9,\n",
            "        -2,  3,  5, -8, -4,  0,  3, -6, -4,  0,  5,  8, -1, -3, -7,  1, -1,  1,\n",
            "        -8, -1])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.5858, -2.3290, -1.3015,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-2.0207, -1.6269, -1.7296,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-2.0036, -1.1302, -2.0207,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.2059, -1.8557, -1.1730,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-1.5581, -1.0679, -1.4356,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-1.7157, -0.7703, -1.7157,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.1678, -0.1394,  0.5577,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.1155, -0.1220,  0.1046,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-1.5163, -0.7495, -1.0806,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-1, -5,  7,  4, -7, -4,  1, -1, -4,  5,  1, -4,  4,  3, -1,  1,  2, -4,\n",
            "         0,  4, -3,  6,  7,  8,  7,  0, -6,  1,  2,  1,  0, -5,  8,  1, -1, -4,\n",
            "         1,  2, -1,  0,  4,  1, -2, -3, -9, -1,  0,  0,  0, -2, -1, -5, -1, -1,\n",
            "         0,  0,  3,  2,  9, -4, -1, -4, -3,  4,  0,  3,  1, -1,  2, -2,  3,  2,\n",
            "         2, -9, -2,  1, -7, -2,  1,  1,  8, -2, -4, -2,  5, -8, -1,  4,  8, -5,\n",
            "         3,  0,  1, -7, -3, -7,  0,  6,  7,  4,  1, -5, -3,  5, -4, -1, -1,  0,\n",
            "        -7,  5,  2,  0, -3,  4, -1, -3, -6, -1,  1, -1,  1,  1,  1, -1, -6,  0,\n",
            "        -4, -3])\n",
            "tensor([[[ 0.1370,  0.4110,  0.2740,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 0.7021,  1.3186,  0.9590,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 0.8049,  1.6782,  1.1816,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-2.2776, -2.3461, -2.1406,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-1.8666, -1.9693, -2.1406,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-1.8666, -1.8152, -2.2091,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 0.2451,  0.4902,  0.4202,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 0.7528,  1.3480,  1.0854,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 0.8578,  1.7857,  1.4531,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-1.6457, -1.5056, -1.4706,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-1.1555, -1.0854, -1.4006,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-1.2955, -1.1204, -1.5581,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.2963,  0.5752,  0.5403,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 0.8192,  1.4118,  1.2723,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 0.9586,  1.8998,  1.6906,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-0.3834,  0.5054,  0.7669,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-0.4532,  0.2614,  0.4880,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-0.8540, -0.5752, -0.6797,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([-3, -8,  1,  2, -1,  5, -6, -3, -2,  7,  4, -1, -1,  2,  2,  5,  0,  2,\n",
            "         1,  2, -3,  5,  2,  3,  4,  0, -7,  2,  2,  2,  3, -7,  0, -4, -6, -1,\n",
            "        -6,  1, -7,  3,  5,  0,  0,  0,  0, -1,  1, -1,  3,  4, -4, -1,  2, -8,\n",
            "         4, -5,  3, -4,  8, -4, -1, -4,  1,  2, -2,  1, -3,  7,  4,  4,  5, -1,\n",
            "         7, -6, -4,  4, -3, -2, -2, -2,  7,  4,  2, -1, -1, -3, -5, -2, -1,  0,\n",
            "         5,  3,  1, -6, -7, -2, -3,  2, -2, -2, -2, -2, -3,  8, -8,  3,  4, -8,\n",
            "        -2, -1,  5, -8,  2,  1,  1, -3,  3, -5,  5,  0, -4, -2, -4, -5, -4,  5,\n",
            "        -4,  4])\n",
            "tensor([[[ 3.1167,  3.3907,  3.0311,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [ 3.8702,  3.9558,  3.3222,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [ 3.7503,  3.5277,  2.1063,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-0.2226, -0.0856, -0.2397,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 3.1338,  3.4489,  3.2388,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [ 3.9041,  4.1492,  3.7290,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [ 3.8515,  3.8866,  2.8011,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [ 0.2276,  0.6478,  0.4377,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 2.7887,  3.3290,  3.2767,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [ 3.4161,  3.9564,  3.9041,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [ 3.4510,  3.9216,  3.2244,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [ 1.1329,  2.5098,  2.7364,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 3, -1,  4,  5, -5,  0, -3, -7, -8,  3,  4,  3,  5, -1, -2,  3,  3, -2,\n",
            "        -2, -2, -5,  6,  7,  5,  4, -5,  0,  2,  2,  3,  7,  0,  4,  0, -1,  0,\n",
            "        -3,  5,  0,  6, -2, -4, -5,  2,  0,  4, -3,  7,  1,  3, -3, -2, -4, -4,\n",
            "         4, -3,  2, -1,  3,  0,  2, -6,  1,  5,  3, -2, -1, -1,  3,  4,  8,  1,\n",
            "        -1, -8,  1,  5, -2, -2,  1,  0,  7,  4, -3, -1,  0,  1, -3,  0,  7, -5,\n",
            "         0, -4,  7, -6, -2,  1,  1,  4,  4, -3,  0,  0,  3,  5, -9, -2,  8, -7,\n",
            "        -4,  0,  6,  0,  1,  7, -5, -3, -2,  3,  4,  7, -5,  1, -6, -3, -2,  0,\n",
            "        -6,  2])\n",
            "tensor([[[-0.7192, -0.2911, -0.4966,  ..., -0.0514,  0.1370, -2.5345],\n",
            "         [ 0.0000,  0.2740,  0.0171,  ...,  0.5651,  0.7021, -2.0892],\n",
            "         [-0.0856,  0.2397, -0.4110,  ...,  0.6679,  0.6336, -1.8666],\n",
            "         ...,\n",
            "         [-0.0514,  0.0000, -0.0856,  ...,  0.7706,  2.7057, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[-0.6127, -0.1050, -0.2101,  ...,  0.7003,  0.8754, -2.1709],\n",
            "         [ 0.0875,  0.4902,  0.4377,  ...,  1.4706,  1.6457, -1.5231],\n",
            "         [ 0.0525,  0.5777,  0.1576,  ...,  1.5581,  1.5931, -1.2780],\n",
            "         ...,\n",
            "         [ 0.5777,  0.8053,  0.5952,  ...,  1.3130,  3.2388, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[-0.6972, -0.1743, -0.2092,  ...,  1.5163,  1.6906, -1.7952],\n",
            "         [-0.0174,  0.3660,  0.5229,  ...,  2.4401,  2.6144, -0.9935],\n",
            "         [-0.0174,  0.5403,  0.4706,  ...,  2.5447,  2.5447, -0.7320],\n",
            "         ...,\n",
            "         [ 2.2309,  3.3115,  3.3638,  ...,  2.7712,  4.0610, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 1,  0,  5, -2, -3,  5,  1, -4,  0,  5, -3,  0,  4, -1, -2,  8,  8,  5,\n",
            "         6,  4,  4, -2,  6,  1,  7, -7, -4,  5, -3,  4,  2, -6,  0, -1,  0, -4,\n",
            "         3,  4, -3,  3, -1,  5, -6,  0, -1,  2, -3,  8,  7,  0, -1, -7,  3, -7,\n",
            "         4, -1,  8, -3,  5, -2,  3, -6, -6,  4,  2,  2,  2,  4,  7,  0,  3,  3,\n",
            "         1, -6,  3, -2, -1, -3, -1,  1, -1,  5,  3, -1,  2, -5, -6,  1,  7, -3,\n",
            "        -1,  0,  6, -1, -3, -8,  1,  1,  7, -4,  2, -3, -4, -1, -6,  1,  3, -6,\n",
            "        -1,  1,  7, -4, -6,  7, -2,  0, -4, -1,  4,  8, -2,  1, -7, -5, -3,  2,\n",
            "        -1, -2])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.7057, -2.6030, -2.5345],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -2.1063, -2.0378, -2.0892],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -2.0207, -2.0550, -1.8666],\n",
            "         ...,\n",
            "         [-3.5619, -0.8220, -0.7535,  ...,  0.2911,  2.0550,  2.0892],\n",
            "         [-3.0825, -0.2569, -0.3939,  ..., -0.0342,  1.4899,  1.7125],\n",
            "         [-3.0311, -0.0171, -0.2397,  ..., -0.4452,  0.6679,  1.1645]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.3109, -2.1884, -2.1709],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.5406, -1.4531, -1.5231],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -1.4706, -1.4706, -1.2780],\n",
            "         ...,\n",
            "         [-2.9762, -0.4027, -0.4902,  ...,  0.3151,  2.0833,  2.0133],\n",
            "         [-2.4335,  0.1926, -0.1401,  ...,  0.1401,  1.6807,  1.8382],\n",
            "         [-2.5210,  0.2801,  0.0700,  ..., -0.3676,  0.7878,  1.2780]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8824, -1.7778, -1.7952],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.9586, -0.8715, -0.9935],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.8715, -0.8715, -0.7320],\n",
            "         ...,\n",
            "         [-1.6732,  1.5861,  1.7952,  ...,  1.3943,  2.4749,  2.2309],\n",
            "         [-1.6732,  1.5163,  1.6906,  ...,  1.0632,  2.1438,  2.1438],\n",
            "         [-2.0218,  0.8017,  0.9412,  ...,  0.3660,  1.3420,  1.5686]]])\n",
            "tensor([-1, -7,  7,  5, -8,  0, -1,  0, -4,  7,  0, -1,  4,  1,  4,  2,  0, -2,\n",
            "         3,  3, -4,  3,  7,  8,  4, -8, -1, -1, -3,  0, -1, -3,  8,  4, -2, -1,\n",
            "        -5,  2, -7,  2,  2,  5,  1,  3,  0,  0, -6,  6,  1,  4, -5, -8,  3, -6,\n",
            "         3, -3,  7, -1,  5, -5,  6, -6, -4,  7, -3,  1,  1, -1,  7,  2,  4,  0,\n",
            "         7, -2, -4,  4,  1,  2,  0,  4,  6,  4,  1, -4,  3, -5,  1,  6,  6, -7,\n",
            "         2,  1,  7, -2, -9, -4,  0,  2,  4, -4,  0, -7, -6,  6, -6, -1,  2,  0,\n",
            "        -7,  2,  3, -3,  1,  4,  0,  0,  2, -1,  2,  4,  1,  0, -3, -2, -2,  3,\n",
            "        -7,  5])\n",
            "tensor([[[-1.0104, -0.7364, -0.8562,  ..., -2.6543, -2.5516, -2.3290],\n",
            "         [-0.2740,  0.0000, -0.3082,  ..., -1.9351, -1.5926, -1.6611],\n",
            "         [-0.4281, -0.2740, -0.8391,  ..., -1.3186, -0.6336, -0.8905],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ...,  0.7877,  2.0550,  1.4042],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -0.2397,  0.9247,  0.9761],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -1.0446,  0.0000,  0.4281]],\n",
            "\n",
            "        [[-1.0854, -0.8053, -0.8403,  ..., -2.2584, -2.1359, -1.9783],\n",
            "         [-0.3501,  0.0000, -0.1401,  ..., -1.3655, -0.9979, -1.0679],\n",
            "         [-0.4202, -0.1225, -0.4727,  ..., -0.7528, -0.0175, -0.2276],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ...,  1.0504,  2.3109,  1.4531],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -0.0525,  1.1204,  1.0329],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -1.0854,  0.0000,  0.3676]],\n",
            "\n",
            "        [[-1.0980, -0.7843, -0.7495,  ..., -1.8301, -1.7255, -1.6558],\n",
            "         [-0.3486,  0.0000,  0.0000,  ..., -0.8017, -0.4357, -0.5752],\n",
            "         [-0.3660,  0.0000, -0.1394,  ..., -0.2092,  0.5403,  0.2963],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ...,  2.0044,  2.5795,  1.5512],\n",
            "         [-1.6732, -0.7320, -0.5229,  ...,  0.7146,  1.4466,  1.2375],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -0.4706,  0.4357,  0.5926]]])\n",
            "tensor([-3, -6,  1,  3, -3, -1, -7, -4, -7,  0,  5,  2,  4,  1, -1,  5,  4,  3,\n",
            "         4, -2,  3,  6,  7,  6,  7, -3, -5, -1, -5, -1,  2,  0,  4, -4, -5, -3,\n",
            "        -6,  4, -7,  1,  5, -1, -1,  2, -3,  0, -2,  0,  7,  3, -3,  0, -2, -4,\n",
            "         4, -8,  2, -7,  8, -1, -1, -6,  2,  4,  1,  0, -1,  0, -1,  1,  3,  5,\n",
            "         6, -5,  2,  0, -1, -3,  2, -4,  2,  3, -5,  0,  3, -3,  2,  4,  6, -2,\n",
            "         2,  4,  6, -8, -5,  0,  0, -1,  7, -3,  0,  0, -3,  1, -2, -2,  8, -4,\n",
            "        -2,  7,  9, -2, -6,  5, -5,  2, -3,  2,  2,  7, -8, -1, -6, -3, -3,  2,\n",
            "        -7, -3])\n",
            "tensor([[[ 2.1577,  3.0140,  3.4078,  ...,  0.2226,  0.7021,  1.0617],\n",
            "         [ 3.0825,  3.9387,  3.8873,  ...,  0.8562,  1.4214,  1.6097],\n",
            "         [ 3.5448,  3.6304,  3.2023,  ...,  1.4556,  1.5755,  1.8152],\n",
            "         ...,\n",
            "         [-3.5619, -3.4421, -3.3907,  ..., -2.7400, -0.9590, -0.9076],\n",
            "         [-3.0825, -2.9626, -3.1852,  ..., -3.1510, -1.6611, -1.4214],\n",
            "         [-3.0311, -2.8770, -3.0653,  ..., -3.6989, -2.5858, -2.1063]],\n",
            "\n",
            "        [[ 1.2955,  2.1359,  2.6085,  ..., -0.0700,  0.3852,  0.7703],\n",
            "         [ 2.1884,  3.0637,  3.1338,  ...,  0.7703,  1.3305,  1.5406],\n",
            "         [ 2.6786,  2.8361,  2.5385,  ...,  1.3655,  1.5231,  1.7507],\n",
            "         ...,\n",
            "         [-2.9762, -2.6786, -2.8186,  ..., -2.3284, -0.5427, -0.5952],\n",
            "         [-2.4335, -2.1534, -2.5210,  ..., -2.5910, -1.0854, -0.9279],\n",
            "         [-2.5210, -2.2584, -2.4860,  ..., -3.2213, -2.0658, -1.6106]],\n",
            "\n",
            "        [[ 0.7495,  1.5686,  2.0915,  ..., -0.1220,  0.2789,  0.6972],\n",
            "         [ 1.6209,  2.4401,  2.6667,  ...,  0.9586,  1.4815,  1.6906],\n",
            "         [ 2.0915,  2.2658,  2.2484,  ...,  1.6558,  1.8301,  2.0044],\n",
            "         ...,\n",
            "         [-1.6732, -0.5926, -0.4532,  ..., -1.2200, -0.1220, -0.3486],\n",
            "         [-1.6732, -0.7320, -0.5229,  ..., -1.6383, -0.5926, -0.5926],\n",
            "         [-2.0218, -1.6383, -1.5163,  ..., -2.4401, -1.4641, -1.2549]]])\n",
            "tensor([ 0, -4,  7,  5,  0, -3, -5, -6, -7,  3,  0, -5,  2,  6,  1,  8, -1,  5,\n",
            "         0,  6,  2,  1,  2,  2,  1, -8, -3,  0,  4, -3,  0, -3,  0,  0, -5,  4,\n",
            "        -4,  1, -4,  2,  5,  4, -3,  0, -3,  2, -6,  3,  5,  2, -6, -8,  2, -6,\n",
            "         1, -3,  4, -4,  7, -4, -1,  0,  1,  5,  3,  2, -1,  4,  3, -2,  8, -2,\n",
            "         7, -8,  2,  4, -5, -2, -2, -4,  4,  1, -3, -7,  7, -6, -7,  0,  8, -7,\n",
            "         5, -3,  6, -2,  0, -8, -7,  7,  4,  1, -3, -1, -1,  5, -3,  2,  7, -7,\n",
            "        -3,  4,  5, -4,  0,  3,  0, -6, -6, -3,  1,  8, -7, -1, -7, -4, -5,  3,\n",
            "        -2,  5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-a8aeddbb4670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open_lth/datasets/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_image_transforms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_composed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_tensor_transforms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDNPNUBkzDaG"
      },
      "source": [
        "from datasets import registry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PJiUzCJ8htC"
      },
      "source": [
        "from datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "goI9Qwma8x80",
        "outputId": "67d72d7a-664d-45a8-8188-0dad4136f5c2"
      },
      "source": [
        "cifar10.Dataset(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c5b4c0484fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ6YjMb1yOV6",
        "outputId": "c52dd4b4-51c6-402b-d303-b2c8912d53bc"
      },
      "source": [
        "a, ab = next(iter(data_loader_2))\n",
        "print(a[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.1075, -1.3815, -1.2617,  ...,  0.5878,  0.4851,  0.4166],\n",
            "         [-1.8439, -2.1179, -1.8097,  ..., -0.0116, -0.0801, -0.0287],\n",
            "         [-1.6898, -1.8439, -1.2788,  ..., -0.0972, -0.0629, -0.2513],\n",
            "         ...,\n",
            "         [ 1.4440,  1.3242,  1.2728,  ...,  0.6221, -1.1589, -1.2103],\n",
            "         [ 0.9646,  0.8447,  1.0673,  ...,  1.0331, -0.4568, -0.6965],\n",
            "         [ 0.9132,  0.7591,  0.9474,  ...,  1.5810,  0.4679, -0.0116]],\n",
            "\n",
            "        [[-0.9503, -1.2304, -1.1954,  ...,  0.2752,  0.1527,  0.1352],\n",
            "         [-1.6856, -2.0357, -1.8957,  ..., -0.4951, -0.5826, -0.5126],\n",
            "         [-1.6155, -1.9132, -1.5630,  ..., -0.5651, -0.5651, -0.7577],\n",
            "         ...,\n",
            "         [ 0.9405,  0.6429,  0.7829,  ...,  0.2927, -1.4930, -1.4405],\n",
            "         [ 0.3978,  0.1176,  0.4853,  ...,  0.5553, -0.9503, -1.1078],\n",
            "         [ 0.4853,  0.2227,  0.4503,  ...,  1.1856,  0.0301, -0.4251]],\n",
            "\n",
            "        [[-0.7064, -1.0201, -1.0550,  ...,  0.0779, -0.0267, -0.0092],\n",
            "         [-1.4559, -1.8044, -1.8044,  ..., -0.8458, -0.9330, -0.8110],\n",
            "         [-1.4384, -1.8044, -1.6650,  ..., -0.9330, -0.9330, -1.0724],\n",
            "         ...,\n",
            "         [-0.1312, -1.2119, -1.3513,  ..., -0.5844, -1.6824, -1.4559],\n",
            "         [-0.1312, -1.0724, -1.2816,  ..., -0.1661, -1.2119, -1.2119],\n",
            "         [ 0.2173, -0.1661, -0.2881,  ...,  0.6356, -0.3404, -0.5495]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OT8kg4Ad-1M"
      },
      "source": [
        "t, tb = next(iter(train_set))\n",
        "a, ab = next(iter(data_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_EPcc8gcmj4",
        "outputId": "ad5093a0-4931-4631-ebfa-fb191dc24b2c"
      },
      "source": [
        "print(t[0])\n",
        "print(a[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.1597,  0.1426,  0.1254,  ..., -2.1179, -2.1179, -2.1179],\n",
            "         [ 0.3138,  0.1083,  0.0569,  ..., -2.1179, -2.1179, -2.1179],\n",
            "         [ 0.1939,  0.1426,  0.1083,  ..., -2.1179, -2.1179, -2.1179],\n",
            "         ...,\n",
            "         [-1.5357, -1.1247, -0.3369,  ..., -2.1179, -2.1179, -2.1179],\n",
            "         [-0.3541, -0.5424, -0.0629,  ..., -2.1179, -2.1179, -2.1179],\n",
            "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "        [[-0.3901, -0.4426, -0.4776,  ..., -2.0357, -2.0357, -2.0357],\n",
            "         [-0.1975, -0.4251, -0.4776,  ..., -2.0357, -2.0357, -2.0357],\n",
            "         [-0.4076, -0.4601, -0.4426,  ..., -2.0357, -2.0357, -2.0357],\n",
            "         ...,\n",
            "         [-1.9482, -1.4930, -0.8627,  ..., -2.0357, -2.0357, -2.0357],\n",
            "         [-0.6702, -0.8627, -0.5126,  ..., -2.0357, -2.0357, -2.0357],\n",
            "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "        [[-0.7936, -0.8807, -0.9678,  ..., -1.8044, -1.8044, -1.8044],\n",
            "         [-0.6193, -0.8807, -0.9330,  ..., -1.8044, -1.8044, -1.8044],\n",
            "         [-0.9156, -0.9504, -0.9330,  ..., -1.8044, -1.8044, -1.8044],\n",
            "         ...,\n",
            "         [-1.8044, -1.6127, -1.2816,  ..., -1.8044, -1.8044, -1.8044],\n",
            "         [-0.8110, -1.0027, -0.7761,  ..., -1.8044, -1.8044, -1.8044],\n",
            "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])\n",
            "tensor([[[-1.1075, -1.3815, -1.2617,  ...,  0.5878,  0.4851,  0.4166],\n",
            "         [-1.8439, -2.1179, -1.8097,  ..., -0.0116, -0.0801, -0.0287],\n",
            "         [-1.6898, -1.8439, -1.2788,  ..., -0.0972, -0.0629, -0.2513],\n",
            "         ...,\n",
            "         [ 1.4440,  1.3242,  1.2728,  ...,  0.6221, -1.1589, -1.2103],\n",
            "         [ 0.9646,  0.8447,  1.0673,  ...,  1.0331, -0.4568, -0.6965],\n",
            "         [ 0.9132,  0.7591,  0.9474,  ...,  1.5810,  0.4679, -0.0116]],\n",
            "\n",
            "        [[-0.9503, -1.2304, -1.1954,  ...,  0.2752,  0.1527,  0.1352],\n",
            "         [-1.6856, -2.0357, -1.8957,  ..., -0.4951, -0.5826, -0.5126],\n",
            "         [-1.6155, -1.9132, -1.5630,  ..., -0.5651, -0.5651, -0.7577],\n",
            "         ...,\n",
            "         [ 0.9405,  0.6429,  0.7829,  ...,  0.2927, -1.4930, -1.4405],\n",
            "         [ 0.3978,  0.1176,  0.4853,  ...,  0.5553, -0.9503, -1.1078],\n",
            "         [ 0.4853,  0.2227,  0.4503,  ...,  1.1856,  0.0301, -0.4251]],\n",
            "\n",
            "        [[-0.7064, -1.0201, -1.0550,  ...,  0.0779, -0.0267, -0.0092],\n",
            "         [-1.4559, -1.8044, -1.8044,  ..., -0.8458, -0.9330, -0.8110],\n",
            "         [-1.4384, -1.8044, -1.6650,  ..., -0.9330, -0.9330, -1.0724],\n",
            "         ...,\n",
            "         [-0.1312, -1.2119, -1.3513,  ..., -0.5844, -1.6824, -1.4559],\n",
            "         [-0.1312, -1.0724, -1.2816,  ..., -0.1661, -1.2119, -1.2119],\n",
            "         [ 0.2173, -0.1661, -0.2881,  ...,  0.6356, -0.3404, -0.5495]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQhbbAHRhbRQ",
        "outputId": "1f140d64-5e72-47da-97eb-97b2dd8f7e69"
      },
      "source": [
        "2.6400/1.8044"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4630902238971404"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCHfxRwXQ2m"
      },
      "source": [
        "class helperFunctions:\n",
        "    def __init__(self):\n",
        "        self.num_batches = len(train_set)\n",
        "        self.batch_size = next(iter(train_set))[0].size()[0]\n",
        "        self.zeroTensor = torch.zeros(())\n",
        "\n",
        "        self.batchsizes=[]\n",
        "        for batch in train_set:\n",
        "            self.batchsizes.append(batch[0].size()[0])\n",
        "\n",
        "    def getZeroTensor(self): #returns a tensor or zeroes with dimension equal to # of batches*# of examples in each batch\n",
        "        for batch in train_set:\n",
        "            batchsize = batch[0].size()[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MMWdRf3XDKk",
        "outputId": "9b6ba82d-c2da-4e45-9bb6-fa8f4e680cf5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A: 2.06 accuracy A: 0.23 \n",
            "\n",
            "Epoch 2, train loss A: 1.81 accuracy A: 0.33 \n",
            "\n",
            "Epoch 3, train loss A: 1.70 accuracy A: 0.37 \n",
            "\n",
            "Epoch 4, train loss A: 1.62 accuracy A: 0.40 \n",
            "\n",
            "Epoch 5, train loss A: 1.55 accuracy A: 0.43 \n",
            "\n",
            "Epoch 6, train loss A: 1.49 accuracy A: 0.45 \n",
            "\n",
            "Epoch 7, train loss A: 1.44 accuracy A: 0.47 \n",
            "\n",
            "Epoch 8, train loss A: 1.38 accuracy A: 0.49 \n",
            "\n",
            "Epoch 9, train loss A: 1.33 accuracy A: 0.52 \n",
            "\n",
            "Epoch 10, train loss A: 1.29 accuracy A: 0.53 \n",
            "\n",
            "Epoch 11, train loss A: 1.25 accuracy A: 0.55 \n",
            "\n",
            "Epoch 12, train loss A: 1.22 accuracy A: 0.56 \n",
            "\n",
            "Epoch 13, train loss A: 1.19 accuracy A: 0.57 \n",
            "\n",
            "Epoch 14, train loss A: 1.16 accuracy A: 0.58 \n",
            "\n",
            "Epoch 15, train loss A: 1.13 accuracy A: 0.59 \n",
            "\n",
            "Epoch 16, train loss A: 1.11 accuracy A: 0.60 \n",
            "\n",
            "Epoch 17, train loss A: 1.09 accuracy A: 0.61 \n",
            "\n",
            "Epoch 18, train loss A: 1.06 accuracy A: 0.62 \n",
            "\n",
            "Epoch 19, train loss A: 1.04 accuracy A: 0.62 \n",
            "\n",
            "Epoch 20, train loss A: 1.03 accuracy A: 0.63 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUl6OXj1ZFZW",
        "outputId": "dbd451dc-8b34-4061-cc69-2dbb3e64715a"
      },
      "source": [
        "model_A_msrments.forgetStatistics[0,0].size()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp2WEiKCYsog",
        "outputId": "c516e74c-b462-46af-ac0c-dc1b155bafe3"
      },
      "source": [
        "manageData=manageForgetDataset(model_A_msrments)\n",
        "forgotten_set = manageData.get_forgotten_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "vHzDZWvPZQFG",
        "outputId": "0773de53-dd1b-4254-9d98-e29798083cdd"
      },
      "source": [
        "next(iter(forgotten_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-873b7e80b792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforgotten_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \"\"\"\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEXIGSDapMLM"
      },
      "source": [
        "tot = torch.sum(model_A_msrments.forgetStatistics,0)\n",
        "totFlat = torch.flatten(tot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zbWvSIyphE6",
        "outputId": "eba8f9ee-2a94-42d7-ca3e-955bc1a8e546"
      },
      "source": [
        "totFlat.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50048])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "unDf3iCBjrNu",
        "outputId": "2b480de5-4b07-45f5-a81d-e0b8b0827c35"
      },
      "source": [
        "process = processMeasurements(model_A_msrments)\n",
        "process.plotForgetHist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xrgRNC48kGc",
        "outputId": "7f7aa7d6-56df-4f20-d4c6-10efe158f2cd"
      },
      "source": [
        "# from foundations import hparams\n",
        "# from models import registry\n",
        "\n",
        "# model_hparams = hparams.ModelHparams(\n",
        "#     'cifar_resnet_20',\n",
        "#     'kaiming_uniform',\n",
        "#     'uniform'\n",
        "# )\n",
        "# testCopy = registry.get(model_hparams).cuda()\n",
        "# testCopy.load_state_dict(model_A_state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsHIyZgK-ztj"
      },
      "source": [
        "# testv=[]\n",
        "# testv.append(testCopy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1w-JRvDGJu5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7sGHmLhGB80",
        "outputId": "bfab2ff0-2049-4dfb-e31f-d4789fa9e548"
      },
      "source": [
        "epsilons[24]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdGqyQu8kUqa"
      },
      "source": [
        "ps=model_A.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_6wJpLpnNDo",
        "outputId": "5e2597e4-f948-469f-c488-e100f3d1143d"
      },
      "source": [
        "next(ps).size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oLbYQiokCeq",
        "outputId": "c5a2b25c-8db1-484e-d405-db371408ee5b"
      },
      "source": [
        "matrix=torch.rand(torch.Size([2, 2]))\n",
        "print(matrix)\n",
        "print(matrix*0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5460, 0.5551],\n",
            "        [0.1765, 0.1428]])\n",
            "tensor([[0.0546, 0.0555],\n",
            "        [0.0176, 0.0143]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3f80Ldoq8Gd"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55u7lz8irAfQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pWZrpfjj0Wp",
        "outputId": "afa2e352-bfad-49ea-ebfe-bc7d8e379181"
      },
      "source": [
        "with torch.no_grad():\n",
        "    for param in model_A.parameters():\n",
        "        print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.3359,  0.2122,  0.2936],\n",
            "          [-0.4357,  0.3791,  0.1035],\n",
            "          [-0.5486,  0.1932,  0.3652]],\n",
            "\n",
            "         [[-0.1027,  0.0941, -0.1432],\n",
            "          [-0.2385, -0.0900,  0.3772],\n",
            "          [-0.0411, -0.4218,  0.2456]],\n",
            "\n",
            "         [[-0.1773,  0.1650,  0.2649],\n",
            "          [ 0.0690, -0.0498, -0.2736],\n",
            "          [-0.4439,  0.1397,  0.1371]]],\n",
            "\n",
            "\n",
            "        [[[-0.2607,  0.1767,  0.2951],\n",
            "          [ 0.2390,  0.2469, -0.0896],\n",
            "          [-0.1899, -0.1489, -0.1903]],\n",
            "\n",
            "         [[ 0.1545, -0.0686, -0.4377],\n",
            "          [ 0.0773,  0.1521,  0.2717],\n",
            "          [-0.3863,  0.3285, -0.2309]],\n",
            "\n",
            "         [[-0.2326,  0.2835, -0.3001],\n",
            "          [-0.2229,  0.3890, -0.1359],\n",
            "          [-0.3316, -0.3786, -0.1551]]],\n",
            "\n",
            "\n",
            "        [[[-0.0923, -0.4500, -0.4989],\n",
            "          [-0.1435,  0.1315,  0.0653],\n",
            "          [ 0.0923,  0.3156, -0.5831]],\n",
            "\n",
            "         [[-0.2606,  0.4134,  0.3242],\n",
            "          [ 0.0508, -0.2533, -0.0357],\n",
            "          [ 0.0058, -0.2484, -0.4557]],\n",
            "\n",
            "         [[-0.3571,  0.5439,  0.2873],\n",
            "          [ 0.3826,  0.2796, -0.2194],\n",
            "          [ 0.2402,  0.3055,  0.4598]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2658,  0.1710,  0.3963],\n",
            "          [-0.2531, -0.0221, -0.1055],\n",
            "          [-0.0454, -0.1551, -0.2790]],\n",
            "\n",
            "         [[-0.2101,  0.4743,  0.1718],\n",
            "          [-0.3785, -0.3448,  0.0167],\n",
            "          [ 0.3013,  0.2548,  0.0634]],\n",
            "\n",
            "         [[-0.0049, -0.1505, -0.4347],\n",
            "          [-0.0464,  0.3319, -0.1551],\n",
            "          [ 0.1982,  0.3261, -0.3539]]],\n",
            "\n",
            "\n",
            "        [[[-0.3726, -0.4333, -0.3438],\n",
            "          [ 0.1117,  0.0664,  0.2056],\n",
            "          [ 0.0654, -0.0422, -0.2432]],\n",
            "\n",
            "         [[-0.1832, -0.2271, -0.0155],\n",
            "          [-0.1008, -0.4928,  0.2685],\n",
            "          [ 0.2596,  0.1454,  0.4185]],\n",
            "\n",
            "         [[-0.3162,  0.4614,  0.1847],\n",
            "          [ 0.5220, -0.0939,  0.2917],\n",
            "          [-0.2040, -0.0200,  0.0884]]],\n",
            "\n",
            "\n",
            "        [[[-0.3026, -0.1174,  0.3084],\n",
            "          [-0.4281,  0.4554,  0.1280],\n",
            "          [ 0.0742,  0.4259, -0.4303]],\n",
            "\n",
            "         [[ 0.1036,  0.3645, -0.4354],\n",
            "          [-0.4630,  0.2805,  0.1399],\n",
            "          [ 0.3458,  0.4363, -0.4345]],\n",
            "\n",
            "         [[-0.2912,  0.4624,  0.0209],\n",
            "          [ 0.2334,  0.0159, -0.4162],\n",
            "          [-0.0135,  0.0975, -0.3560]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2232, -0.5116,  0.1567],\n",
            "          [ 0.1161, -0.4964, -0.0475],\n",
            "          [-0.1589, -0.3751,  0.1857]],\n",
            "\n",
            "         [[ 0.1998, -0.0168,  0.2062],\n",
            "          [-0.3986,  0.1503,  0.1795],\n",
            "          [ 0.1537, -0.2353,  0.0510]],\n",
            "\n",
            "         [[ 0.0493, -0.1854,  0.5110],\n",
            "          [-0.2613, -0.0443, -0.2155],\n",
            "          [ 0.5809,  0.2967, -0.3380]]],\n",
            "\n",
            "\n",
            "        [[[-0.3081, -0.1452, -0.2867],\n",
            "          [ 0.2173, -0.1471,  0.2328],\n",
            "          [-0.1701, -0.0191,  0.3276]],\n",
            "\n",
            "         [[-0.1027,  0.3751, -0.2241],\n",
            "          [ 0.0343,  0.3254, -0.2355],\n",
            "          [ 0.3898, -0.0583, -0.1107]],\n",
            "\n",
            "         [[-0.5430, -0.0557, -0.1062],\n",
            "          [-0.0793,  0.5387, -0.0532],\n",
            "          [ 0.2883,  0.1420, -0.2932]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2824, -0.2524, -0.2102],\n",
            "          [-0.0410,  0.3289, -0.1545],\n",
            "          [-0.1332,  0.2842, -0.0234]],\n",
            "\n",
            "         [[-0.0048, -0.1818, -0.0864],\n",
            "          [-0.1033, -0.1511,  0.2420],\n",
            "          [ 0.0733, -0.0600,  0.3369]],\n",
            "\n",
            "         [[ 0.0798, -0.0096, -0.5389],\n",
            "          [-0.3802,  0.4793, -0.3345],\n",
            "          [-0.0747,  0.1846,  0.1044]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1905,  0.2302,  0.2233],\n",
            "          [ 0.0651, -0.4384, -0.5147],\n",
            "          [ 0.2803,  0.1991, -0.3655]],\n",
            "\n",
            "         [[-0.0869, -0.0548,  0.5057],\n",
            "          [-0.1215,  0.0892, -0.4133],\n",
            "          [-0.5135, -0.0015,  0.2815]],\n",
            "\n",
            "         [[ 0.4064,  0.3484,  0.0824],\n",
            "          [-0.3576, -0.1419,  0.1860],\n",
            "          [ 0.1263,  0.0663, -0.0773]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0986, -0.3296, -0.0628],\n",
            "          [ 0.1932, -0.0111, -0.1809],\n",
            "          [-0.4082,  0.2010,  0.4151]],\n",
            "\n",
            "         [[ 0.3905, -0.1422,  0.1168],\n",
            "          [ 0.2743,  0.1392,  0.1659],\n",
            "          [-0.5133, -0.0872, -0.3071]],\n",
            "\n",
            "         [[ 0.3112, -0.1151, -0.5270],\n",
            "          [ 0.2723,  0.4150, -0.1732],\n",
            "          [-0.1953,  0.3686, -0.0019]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4192, -0.2762, -0.0655],\n",
            "          [ 0.4372, -0.0550,  0.4114],\n",
            "          [-0.1426,  0.0206,  0.1204]],\n",
            "\n",
            "         [[-0.4596, -0.2401, -0.1647],\n",
            "          [-0.4392, -0.5064,  0.4617],\n",
            "          [-0.2885, -0.2686, -0.2663]],\n",
            "\n",
            "         [[-0.3273, -0.1328,  0.1057],\n",
            "          [-0.1152,  0.3012,  0.4565],\n",
            "          [-0.2224, -0.4231,  0.1333]]],\n",
            "\n",
            "\n",
            "        [[[-0.3939,  0.1892,  0.0566],\n",
            "          [ 0.0469, -0.4215, -0.3740],\n",
            "          [-0.0415,  0.3681, -0.2970]],\n",
            "\n",
            "         [[-0.0916,  0.4274,  0.1283],\n",
            "          [ 0.6049,  0.1746,  0.2226],\n",
            "          [ 0.3766,  0.0368, -0.1294]],\n",
            "\n",
            "         [[-0.2741, -0.5389,  0.1219],\n",
            "          [ 0.3426, -0.4656,  0.3466],\n",
            "          [-0.4311, -0.0744,  0.0656]]],\n",
            "\n",
            "\n",
            "        [[[-0.2264,  0.0816,  0.1452],\n",
            "          [-0.1760,  0.0792,  0.0659],\n",
            "          [ 0.2075, -0.3145, -0.3091]],\n",
            "\n",
            "         [[ 0.3670, -0.2122,  0.1413],\n",
            "          [-0.0078, -0.2806, -0.4334],\n",
            "          [ 0.3987,  0.3850, -0.1461]],\n",
            "\n",
            "         [[-0.2617,  0.2262,  0.0908],\n",
            "          [-0.1053, -0.4174, -0.3077],\n",
            "          [ 0.2029,  0.4003,  0.1120]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2852,  0.2582,  0.1476],\n",
            "          [-0.2994,  0.5046, -0.0968],\n",
            "          [-0.4116,  0.4076, -0.3153]],\n",
            "\n",
            "         [[ 0.1666, -0.3351, -0.2288],\n",
            "          [-0.3579,  0.0124, -0.1731],\n",
            "          [ 0.3399,  0.4176, -0.3560]],\n",
            "\n",
            "         [[ 0.1745, -0.3114, -0.0165],\n",
            "          [-0.0532,  0.3423,  0.0328],\n",
            "          [ 0.0292,  0.3879, -0.3569]]],\n",
            "\n",
            "\n",
            "        [[[-0.2988,  0.1325, -0.2490],\n",
            "          [-0.1827, -0.2597,  0.0863],\n",
            "          [-0.3190, -0.4340,  0.1438]],\n",
            "\n",
            "         [[ 0.4415,  0.3393, -0.2814],\n",
            "          [-0.2499,  0.2733, -0.3446],\n",
            "          [ 0.3540,  0.4738,  0.2663]],\n",
            "\n",
            "         [[ 0.0269, -0.0864,  0.2544],\n",
            "          [ 0.1309, -0.5031,  0.0827],\n",
            "          [ 0.1375,  0.4224,  0.2049]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4999, 0.2732, 0.6244, 0.1898, 0.2321, 0.5541, 0.5139, 0.6113, 0.4134,\n",
            "        0.4145, 0.4790, 0.4888, 0.4227, 0.3022, 0.8780, 0.0028],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1109, -0.1263,  0.2166, -0.0754,  0.1086,  0.0947,  0.1390,  0.2923,\n",
            "        -0.0666,  0.1194,  0.2035,  0.0535,  0.1246,  0.0289,  0.4296, -0.0162],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 9.2448e-02,  1.3710e-01,  3.3562e-02],\n",
            "          [-1.5869e-01,  2.2159e-01, -1.2716e-01],\n",
            "          [-1.6933e-01,  1.9641e-01,  9.8757e-02]],\n",
            "\n",
            "         [[ 1.2300e-01, -6.8168e-02,  2.6133e-03],\n",
            "          [ 1.0761e-01, -8.7804e-02,  2.1209e-01],\n",
            "          [-3.3818e-02, -9.5180e-02,  2.8412e-01]],\n",
            "\n",
            "         [[-3.7199e-02,  2.1573e-02, -2.7747e-02],\n",
            "          [ 2.6856e-01, -1.9245e-01, -4.6359e-02],\n",
            "          [-6.4754e-02,  5.2243e-02,  4.1403e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5399e-02, -3.8540e-02,  1.7802e-01],\n",
            "          [-1.1912e-01, -2.4531e-01,  1.2693e-01],\n",
            "          [ 4.6493e-02,  9.7633e-02,  1.5353e-01]],\n",
            "\n",
            "         [[-3.3323e-02, -2.1053e-01, -3.8537e-02],\n",
            "          [-2.0569e-01,  2.5344e-01,  4.3332e-02],\n",
            "          [ 8.2643e-02, -7.6064e-02,  1.1231e-01]],\n",
            "\n",
            "         [[ 1.6849e-01,  7.5569e-03,  7.3696e-02],\n",
            "          [ 1.6161e-01,  1.9960e-01,  4.7860e-02],\n",
            "          [ 6.9666e-02, -4.3843e-02, -2.0856e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4868e-01, -4.8340e-04, -1.9103e-01],\n",
            "          [-1.0778e-01, -4.3455e-02,  3.0273e-02],\n",
            "          [-4.8474e-02,  4.8931e-02,  1.7723e-02]],\n",
            "\n",
            "         [[-1.0657e-01, -1.2623e-01, -1.9593e-01],\n",
            "          [ 1.3049e-01,  3.0439e-04, -5.0288e-02],\n",
            "          [ 1.3902e-01, -1.0744e-01, -2.2820e-02]],\n",
            "\n",
            "         [[-1.5288e-01,  1.0609e-01, -1.0819e-01],\n",
            "          [-9.7569e-02, -2.4616e-01,  6.8960e-02],\n",
            "          [ 1.2823e-01, -7.3825e-02, -2.7077e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5567e-02,  1.6933e-01, -1.9771e-02],\n",
            "          [ 1.7023e-02, -9.5925e-02, -9.0901e-02],\n",
            "          [ 1.9764e-02, -1.7152e-01,  1.3629e-02]],\n",
            "\n",
            "         [[-2.8834e-02,  1.0511e-01, -1.3436e-01],\n",
            "          [-1.0044e-01, -1.3097e-01,  1.6550e-01],\n",
            "          [-1.9417e-01, -2.0051e-01,  8.5749e-02]],\n",
            "\n",
            "         [[ 7.0788e-02, -2.4708e-02, -1.0467e-01],\n",
            "          [-7.8047e-02, -1.0065e-01, -1.5904e-01],\n",
            "          [ 3.2671e-03,  1.1322e-01,  1.2590e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0004e-01, -5.8689e-03, -3.3415e-01],\n",
            "          [ 1.3047e-01, -7.1213e-02,  6.9172e-02],\n",
            "          [ 1.2301e-01,  1.8229e-01, -1.3644e-01]],\n",
            "\n",
            "         [[-1.1273e-01,  8.6916e-02, -1.0785e-01],\n",
            "          [ 9.9969e-02,  4.9723e-02, -2.0414e-01],\n",
            "          [-1.5205e-01,  1.7196e-01, -1.8527e-01]],\n",
            "\n",
            "         [[-1.0706e-01,  2.3594e-02, -3.4296e-02],\n",
            "          [ 1.9841e-01, -1.6357e-02,  1.2943e-01],\n",
            "          [ 1.0424e-01,  1.7519e-02,  2.5244e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.7021e-02, -3.0570e-02, -4.9933e-02],\n",
            "          [-1.8665e-01, -8.1883e-04, -1.7689e-01],\n",
            "          [-6.4205e-02, -1.6068e-01,  1.2546e-01]],\n",
            "\n",
            "         [[ 1.3943e-01, -7.8102e-02, -2.1778e-01],\n",
            "          [ 4.9754e-02, -7.4009e-02,  4.0301e-02],\n",
            "          [ 5.1622e-02, -5.6262e-02,  8.8547e-02]],\n",
            "\n",
            "         [[ 1.9401e-01, -1.5898e-01,  1.9890e-02],\n",
            "          [ 5.7203e-02,  1.2727e-01,  1.9303e-01],\n",
            "          [ 8.6064e-02, -1.4376e-01,  1.5764e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-9.3098e-03, -1.2523e-01,  2.3368e-02],\n",
            "          [-3.6736e-02,  1.0556e-01, -1.8784e-01],\n",
            "          [-8.1713e-03,  3.1860e-02,  6.5720e-02]],\n",
            "\n",
            "         [[ 9.0877e-02, -1.4499e-01,  1.1010e-01],\n",
            "          [ 1.0318e-01, -3.1094e-02, -1.1675e-01],\n",
            "          [-8.5184e-02, -5.8186e-02, -1.2747e-01]],\n",
            "\n",
            "         [[-2.3914e-01,  8.4813e-04, -9.6275e-02],\n",
            "          [ 9.6081e-02, -6.2010e-02,  2.6344e-01],\n",
            "          [-2.2471e-01, -1.3231e-02,  1.8478e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.1199e-02, -4.9197e-02,  5.8812e-02],\n",
            "          [ 3.3192e-02, -6.3730e-02,  1.7941e-01],\n",
            "          [-1.4922e-01,  6.7083e-02, -1.4722e-01]],\n",
            "\n",
            "         [[-7.3389e-02,  2.4069e-02,  3.8372e-02],\n",
            "          [-1.5722e-02,  3.4581e-02, -2.3397e-01],\n",
            "          [-1.6025e-02, -8.5314e-02, -5.9774e-02]],\n",
            "\n",
            "         [[ 8.0281e-02,  1.6831e-01,  6.3185e-02],\n",
            "          [ 1.3800e-01, -3.5159e-02,  6.3396e-02],\n",
            "          [ 1.0253e-01,  1.8504e-01, -1.6178e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3742e-01, -6.9392e-02, -7.2900e-02],\n",
            "          [-2.2106e-01, -7.9933e-03, -7.9817e-02],\n",
            "          [ 1.6695e-01,  1.4227e-01, -1.9468e-02]],\n",
            "\n",
            "         [[ 1.1760e-01, -1.6530e-01, -3.6036e-02],\n",
            "          [-1.5898e-01,  1.2859e-01, -5.8666e-02],\n",
            "          [ 1.5179e-01, -7.8419e-02, -9.8052e-03]],\n",
            "\n",
            "         [[ 1.3991e-01,  1.2611e-01,  2.2930e-01],\n",
            "          [ 6.3496e-02, -9.8406e-03,  1.3461e-01],\n",
            "          [ 2.5807e-02, -1.7506e-01,  2.3577e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2589e-01, -1.0212e-01, -1.9609e-01],\n",
            "          [-2.1382e-02, -7.3328e-02,  6.1260e-03],\n",
            "          [-6.8890e-02, -5.5205e-02, -2.5595e-02]],\n",
            "\n",
            "         [[-2.2444e-01,  1.0942e-01, -1.6049e-01],\n",
            "          [ 9.4787e-02, -1.2206e-02,  2.2167e-01],\n",
            "          [ 9.3975e-02, -8.3656e-02, -1.8736e-02]],\n",
            "\n",
            "         [[-9.1560e-03,  7.0305e-02, -1.7018e-01],\n",
            "          [-2.4594e-02,  1.3142e-01,  2.1547e-02],\n",
            "          [-1.1846e-01, -7.6368e-02,  6.4617e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1344e-02, -8.0392e-03, -1.0851e-01],\n",
            "          [-2.7059e-01, -2.3223e-01,  9.9552e-02],\n",
            "          [-6.1603e-02,  3.8218e-02,  2.9224e-02]],\n",
            "\n",
            "         [[-6.8193e-02, -1.3955e-01,  8.3300e-03],\n",
            "          [-9.2176e-02,  1.9824e-01, -9.4337e-02],\n",
            "          [-1.7554e-01,  2.0384e-01,  8.7847e-02]],\n",
            "\n",
            "         [[ 6.1985e-02, -5.1763e-03, -1.8454e-03],\n",
            "          [-4.0030e-02,  1.8060e-01,  1.1861e-01],\n",
            "          [ 2.4679e-01, -9.7572e-02,  1.6720e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.2546e-02,  1.3732e-01,  7.4309e-02],\n",
            "          [-6.3417e-02,  1.3221e-01,  1.7204e-01],\n",
            "          [-5.1564e-02,  1.3262e-01,  6.8330e-04]],\n",
            "\n",
            "         [[ 1.2069e-01,  1.7233e-01, -1.7762e-02],\n",
            "          [ 1.9768e-01, -1.4473e-01,  1.4806e-01],\n",
            "          [-2.8473e-01,  1.0096e-01,  7.9374e-02]],\n",
            "\n",
            "         [[ 4.6609e-02,  1.7323e-01, -7.4258e-02],\n",
            "          [-1.7376e-01,  8.8709e-03, -1.4847e-01],\n",
            "          [-1.7631e-01, -1.1432e-01,  3.5941e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.8001, 0.7163, 0.9566, 0.4065, 0.7605, 0.5868, 0.1716, 0.4756, 0.3902,\n",
            "        0.5482, 0.7737, 0.7604, 0.1534, 0.9132, 0.1559, 0.8120],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0816,  0.0012, -0.0123,  0.1826,  0.0208, -0.0594,  0.1518, -0.0335,\n",
            "        -0.1048,  0.2621,  0.1108,  0.1789, -0.0041,  0.0112,  0.0022,  0.1401],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0094, -0.1766, -0.0563],\n",
            "          [-0.1369, -0.0182, -0.1901],\n",
            "          [-0.0510, -0.1631, -0.0248]],\n",
            "\n",
            "         [[ 0.1726, -0.1387, -0.1442],\n",
            "          [ 0.0379, -0.0697,  0.0183],\n",
            "          [ 0.1181, -0.1706,  0.2052]],\n",
            "\n",
            "         [[-0.0417, -0.0143, -0.0750],\n",
            "          [ 0.0980,  0.0679, -0.0814],\n",
            "          [ 0.0337, -0.0013, -0.1106]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1325, -0.1433, -0.1657],\n",
            "          [-0.0648,  0.1314, -0.2036],\n",
            "          [ 0.1351,  0.1350, -0.1549]],\n",
            "\n",
            "         [[-0.0988, -0.2023,  0.0065],\n",
            "          [-0.0503,  0.1020, -0.0603],\n",
            "          [-0.1249, -0.0119,  0.0327]],\n",
            "\n",
            "         [[-0.2914, -0.0911, -0.1151],\n",
            "          [-0.0801, -0.0086,  0.1797],\n",
            "          [-0.0753, -0.1538, -0.0611]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1053, -0.0119,  0.1386],\n",
            "          [ 0.0033, -0.1638, -0.0476],\n",
            "          [-0.0147, -0.0490, -0.1326]],\n",
            "\n",
            "         [[ 0.0120,  0.0981,  0.0173],\n",
            "          [-0.2073,  0.0104, -0.1764],\n",
            "          [-0.0283, -0.2111, -0.0827]],\n",
            "\n",
            "         [[-0.0101,  0.1919,  0.0281],\n",
            "          [-0.1255,  0.2432,  0.2511],\n",
            "          [-0.1664,  0.0591,  0.0828]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0273,  0.2050,  0.1121],\n",
            "          [-0.0202,  0.2639,  0.1650],\n",
            "          [ 0.0944,  0.0488,  0.0125]],\n",
            "\n",
            "         [[ 0.1914,  0.0060,  0.1401],\n",
            "          [ 0.1465,  0.0637,  0.1470],\n",
            "          [-0.0658,  0.2060,  0.0167]],\n",
            "\n",
            "         [[-0.0055,  0.1678,  0.0153],\n",
            "          [ 0.0603, -0.1505, -0.0666],\n",
            "          [ 0.0061, -0.1678,  0.0161]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1560, -0.1628,  0.0474],\n",
            "          [ 0.1416, -0.0226, -0.0374],\n",
            "          [-0.1423, -0.0143, -0.0832]],\n",
            "\n",
            "         [[ 0.1551, -0.2006,  0.1331],\n",
            "          [-0.1538, -0.0753,  0.0158],\n",
            "          [ 0.0853, -0.0446, -0.0809]],\n",
            "\n",
            "         [[ 0.0431,  0.0314, -0.1714],\n",
            "          [ 0.1236,  0.1018,  0.0773],\n",
            "          [ 0.0533, -0.1539, -0.1177]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1352,  0.0549,  0.1066],\n",
            "          [ 0.0208,  0.0086,  0.1082],\n",
            "          [ 0.0593, -0.1451,  0.1265]],\n",
            "\n",
            "         [[ 0.1457, -0.1490, -0.1701],\n",
            "          [-0.1349,  0.0750,  0.0062],\n",
            "          [-0.0730,  0.1350, -0.0413]],\n",
            "\n",
            "         [[-0.0137,  0.1623,  0.0625],\n",
            "          [ 0.0943,  0.0447, -0.1992],\n",
            "          [-0.0637,  0.0165, -0.0797]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1104,  0.1720,  0.1897],\n",
            "          [-0.2010,  0.0422, -0.0053],\n",
            "          [ 0.0171, -0.0563,  0.2006]],\n",
            "\n",
            "         [[-0.0167,  0.2070, -0.1328],\n",
            "          [ 0.1485, -0.0579, -0.0110],\n",
            "          [ 0.0886,  0.0985, -0.0940]],\n",
            "\n",
            "         [[-0.0255,  0.0215,  0.1304],\n",
            "          [ 0.1034, -0.0764, -0.0508],\n",
            "          [-0.0101,  0.1435,  0.1146]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0950,  0.1566,  0.0095],\n",
            "          [ 0.0951,  0.1025,  0.0276],\n",
            "          [ 0.1673, -0.0292, -0.0319]],\n",
            "\n",
            "         [[-0.1686,  0.0281,  0.1108],\n",
            "          [ 0.1575,  0.0263, -0.2221],\n",
            "          [-0.0859,  0.0280,  0.0281]],\n",
            "\n",
            "         [[ 0.0719,  0.1948, -0.0055],\n",
            "          [-0.1407, -0.0476,  0.1282],\n",
            "          [-0.2141,  0.1372, -0.1388]]],\n",
            "\n",
            "\n",
            "        [[[-0.0606, -0.1486,  0.0150],\n",
            "          [-0.0267, -0.0301, -0.2511],\n",
            "          [ 0.0785, -0.0030, -0.0061]],\n",
            "\n",
            "         [[-0.0008,  0.1784, -0.0153],\n",
            "          [-0.2033,  0.0663, -0.0426],\n",
            "          [-0.1238,  0.2753, -0.1152]],\n",
            "\n",
            "         [[ 0.0231,  0.0500,  0.1058],\n",
            "          [-0.0840, -0.1561,  0.0724],\n",
            "          [-0.0861,  0.2094, -0.0917]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1156, -0.1290,  0.1675],\n",
            "          [-0.0745,  0.1405,  0.0135],\n",
            "          [ 0.2167,  0.1143, -0.1217]],\n",
            "\n",
            "         [[-0.1211, -0.0991, -0.0289],\n",
            "          [-0.1580, -0.0650,  0.1051],\n",
            "          [-0.0045, -0.1299,  0.0133]],\n",
            "\n",
            "         [[-0.2135,  0.0674,  0.0370],\n",
            "          [-0.0071, -0.2647,  0.2534],\n",
            "          [ 0.0724, -0.1743, -0.2127]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1684, -0.1002, -0.0407],\n",
            "          [ 0.0790,  0.0756, -0.0818],\n",
            "          [-0.0552,  0.0238, -0.1040]],\n",
            "\n",
            "         [[ 0.0280,  0.1384, -0.1192],\n",
            "          [ 0.0962,  0.0188, -0.0049],\n",
            "          [ 0.0875,  0.0244, -0.0595]],\n",
            "\n",
            "         [[-0.0282, -0.1173,  0.1081],\n",
            "          [-0.0879, -0.1424,  0.2095],\n",
            "          [-0.2167, -0.2218,  0.0920]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1204,  0.1959, -0.0690],\n",
            "          [ 0.0031, -0.1301,  0.1546],\n",
            "          [-0.0182, -0.0873,  0.0953]],\n",
            "\n",
            "         [[ 0.0982,  0.1719, -0.0398],\n",
            "          [ 0.1141,  0.1622,  0.1559],\n",
            "          [-0.0367, -0.1605,  0.1713]],\n",
            "\n",
            "         [[-0.0685, -0.0881, -0.1310],\n",
            "          [ 0.0846,  0.1621,  0.1095],\n",
            "          [ 0.0385, -0.0279, -0.1055]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.9622, 0.7635, 0.1019, 0.7320, 0.4670, 0.5861, 0.5767, 0.9089, 0.4704,\n",
            "        0.8235, 0.7755, 0.4276, 0.4893, 0.5061, 0.7059, 0.2069],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.2826,  0.1533,  0.1221, -0.0435,  0.1758, -0.0939,  0.1776, -0.0898,\n",
            "         0.0758,  0.1913,  0.1498,  0.1576,  0.2172, -0.0642,  0.0569, -0.0484],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-6.4111e-02,  1.3149e-01,  1.2549e-01],\n",
            "          [ 2.0518e-01,  2.0408e-01, -1.1807e-01],\n",
            "          [ 1.2475e-01,  7.4996e-02, -2.8250e-02]],\n",
            "\n",
            "         [[ 1.2453e-01,  7.2783e-03, -1.5152e-01],\n",
            "          [-9.6110e-02,  1.1801e-01, -9.2375e-02],\n",
            "          [-3.5506e-02,  1.6085e-01,  4.8048e-02]],\n",
            "\n",
            "         [[-4.7388e-02, -1.9021e-01,  1.3341e-01],\n",
            "          [-1.1163e-01,  1.1902e-01, -1.2876e-01],\n",
            "          [ 1.1726e-01,  4.3909e-02, -3.1946e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.0633e-01,  1.7912e-01,  5.7372e-02],\n",
            "          [-1.6256e-02,  1.1100e-01, -7.4089e-02],\n",
            "          [ 1.8982e-02, -6.8873e-02,  1.7320e-01]],\n",
            "\n",
            "         [[ 5.6917e-02,  1.6516e-01, -5.1978e-02],\n",
            "          [ 1.0418e-01, -4.8866e-02, -1.1849e-01],\n",
            "          [ 2.1311e-01, -3.1173e-02, -1.0090e-01]],\n",
            "\n",
            "         [[ 1.2185e-01,  2.0659e-01, -1.7217e-01],\n",
            "          [ 1.1322e-01,  1.3610e-02, -8.8263e-02],\n",
            "          [ 1.9755e-01, -5.3094e-02,  3.9809e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.6362e-02, -7.9437e-02, -1.3700e-01],\n",
            "          [ 5.9958e-02, -1.2499e-01, -5.3740e-02],\n",
            "          [-2.1919e-01, -2.8945e-02,  1.0132e-01]],\n",
            "\n",
            "         [[ 1.0544e-01, -1.1356e-01, -2.2766e-01],\n",
            "          [ 1.4358e-01, -9.7276e-02,  1.0930e-01],\n",
            "          [ 8.6354e-03, -9.8226e-02,  1.4963e-01]],\n",
            "\n",
            "         [[ 2.1475e-01, -1.2775e-01, -1.6742e-01],\n",
            "          [ 1.3736e-01, -4.1150e-02,  5.6993e-02],\n",
            "          [-4.3534e-02, -8.0749e-02,  8.8799e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6350e-01,  1.9128e-01, -8.9477e-02],\n",
            "          [-5.8070e-02, -6.1296e-02,  2.3533e-02],\n",
            "          [-6.6736e-03,  1.4689e-01,  1.1355e-01]],\n",
            "\n",
            "         [[-3.9878e-02, -1.0135e-01,  3.8851e-02],\n",
            "          [ 1.3260e-01,  1.3150e-01,  6.0417e-03],\n",
            "          [-1.2862e-01, -2.0134e-01,  2.3367e-03]],\n",
            "\n",
            "         [[ 1.9885e-01, -2.0710e-01, -8.1705e-02],\n",
            "          [ 7.7923e-02,  1.6404e-02,  1.6941e-01],\n",
            "          [ 1.5724e-01, -1.0847e-01,  1.2209e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9042e-01, -1.1953e-02, -5.6578e-02],\n",
            "          [ 5.6862e-02,  4.9321e-02,  1.7764e-01],\n",
            "          [-1.0376e-02, -1.4354e-01,  2.1298e-01]],\n",
            "\n",
            "         [[ 1.6581e-01,  9.0456e-02, -3.2949e-03],\n",
            "          [ 6.2770e-02,  1.1126e-01, -9.3068e-02],\n",
            "          [ 1.4669e-01,  1.3875e-01,  4.6506e-02]],\n",
            "\n",
            "         [[ 1.7107e-01, -5.6071e-02,  2.2251e-01],\n",
            "          [-1.6035e-01,  6.0062e-02,  1.9208e-01],\n",
            "          [-4.5839e-02,  1.7125e-01, -6.4326e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9299e-01,  2.7450e-02, -1.0434e-01],\n",
            "          [-2.0584e-01,  1.8219e-01,  1.8755e-02],\n",
            "          [-2.3675e-02, -7.6587e-02,  8.9902e-02]],\n",
            "\n",
            "         [[ 3.3291e-02, -8.0905e-02, -1.8004e-01],\n",
            "          [ 8.7416e-02, -4.5705e-02, -1.0720e-01],\n",
            "          [-4.2911e-02,  2.1694e-01,  7.4016e-02]],\n",
            "\n",
            "         [[ 8.9525e-02,  9.3041e-03, -5.4762e-02],\n",
            "          [ 1.5992e-02,  1.4775e-01, -1.6403e-01],\n",
            "          [ 6.6122e-02, -1.0784e-01, -8.4647e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.0104e-02,  1.2945e-02, -3.4957e-02],\n",
            "          [ 5.4364e-02,  2.0563e-01, -1.5868e-02],\n",
            "          [-3.2793e-02,  2.5840e-02,  3.9056e-02]],\n",
            "\n",
            "         [[-3.4863e-02, -3.9298e-03,  1.5653e-01],\n",
            "          [-1.1884e-02,  5.0171e-02,  2.4712e-01],\n",
            "          [ 1.0402e-01,  3.3270e-02, -6.6408e-02]],\n",
            "\n",
            "         [[ 2.1995e-02,  2.2895e-01,  9.5787e-03],\n",
            "          [-7.6051e-02,  7.6519e-02, -2.7580e-02],\n",
            "          [-8.8305e-03, -3.7040e-02, -8.6623e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.1743e-03,  1.6165e-01, -1.3827e-02],\n",
            "          [ 7.0756e-02,  1.8739e-01, -1.0707e-01],\n",
            "          [-1.8080e-01,  1.7378e-01, -1.7998e-01]],\n",
            "\n",
            "         [[ 1.1561e-01,  2.0286e-01,  1.5906e-01],\n",
            "          [-5.8792e-02,  2.7058e-02,  6.5280e-02],\n",
            "          [-1.4833e-02, -1.5694e-02,  5.4399e-02]],\n",
            "\n",
            "         [[ 7.6326e-03, -1.8279e-01,  9.6966e-02],\n",
            "          [-1.3555e-02, -1.0485e-01, -1.4452e-01],\n",
            "          [ 7.1309e-02,  1.6891e-01,  3.4932e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1971e-03, -1.4034e-01, -1.4346e-01],\n",
            "          [ 3.9933e-03, -6.7669e-02, -1.5039e-01],\n",
            "          [ 1.1260e-01, -3.5770e-02,  7.6390e-02]],\n",
            "\n",
            "         [[-3.0073e-02,  1.6157e-01, -6.9564e-02],\n",
            "          [-7.7783e-02, -1.4773e-01, -8.6933e-02],\n",
            "          [-7.0921e-02, -1.6644e-01,  1.4927e-01]],\n",
            "\n",
            "         [[ 1.1650e-02, -2.1968e-01,  5.2917e-02],\n",
            "          [-1.1452e-01, -6.7064e-02,  9.0091e-02],\n",
            "          [-5.2570e-02, -5.0264e-02,  5.6256e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4567e-02,  2.4067e-01,  1.6043e-01],\n",
            "          [-5.4487e-02, -7.7382e-02,  3.8315e-02],\n",
            "          [ 1.1967e-01,  1.1469e-01,  2.1369e-01]],\n",
            "\n",
            "         [[ 1.0413e-02,  2.8136e-02,  1.4572e-01],\n",
            "          [ 7.0727e-02,  1.4472e-01,  2.4287e-01],\n",
            "          [ 1.4068e-01, -1.5648e-01,  2.9686e-01]],\n",
            "\n",
            "         [[-1.8221e-01, -2.0379e-01,  1.5548e-01],\n",
            "          [-2.7490e-02,  5.0015e-02,  1.4420e-01],\n",
            "          [ 7.9626e-02, -2.1268e-02, -5.3756e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0036e-01,  1.9585e-01, -4.7805e-02],\n",
            "          [-1.6586e-01, -1.0756e-01,  9.0839e-02],\n",
            "          [ 8.0373e-02, -2.0157e-01, -1.3943e-02]],\n",
            "\n",
            "         [[ 6.2809e-02, -2.4663e-03, -7.2105e-02],\n",
            "          [ 1.9621e-01,  1.2341e-01,  7.8239e-02],\n",
            "          [-1.1776e-01,  8.8270e-02,  2.1352e-01]],\n",
            "\n",
            "         [[-1.2361e-01, -1.7452e-01, -6.3030e-02],\n",
            "          [-3.4158e-02, -8.5570e-02, -1.7427e-02],\n",
            "          [ 1.5804e-01,  7.9058e-02,  6.6646e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7102e-01,  6.9740e-04, -7.1651e-02],\n",
            "          [ 4.3790e-02,  2.0125e-04,  8.2624e-02],\n",
            "          [-4.9429e-03,  1.4050e-02, -1.3000e-02]],\n",
            "\n",
            "         [[ 1.0836e-01,  1.0633e-02,  4.5890e-02],\n",
            "          [-2.0923e-01, -1.5098e-01,  1.4632e-01],\n",
            "          [-9.1696e-02, -1.7004e-01, -2.3232e-01]],\n",
            "\n",
            "         [[-4.9986e-02, -1.9424e-01, -6.2192e-02],\n",
            "          [-1.0722e-01,  2.3484e-02,  3.2345e-02],\n",
            "          [ 1.2581e-01,  6.8277e-02,  1.3808e-01]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4908, 0.3315, 0.0544, 0.2832, 0.3110, 0.3682, 0.5331, 0.9582, 0.2681,\n",
            "        0.2994, 0.6207, 0.5043, 0.4590, 0.7816, 0.6970, 0.6934],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1044,  0.1169, -0.0605,  0.0199,  0.0146, -0.0833, -0.0748, -0.0772,\n",
            "        -0.0635, -0.0702,  0.0222,  0.1876,  0.1084,  0.0409,  0.0451, -0.2006],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.2128, -0.1683,  0.0936],\n",
            "          [ 0.0702, -0.0069,  0.1454],\n",
            "          [-0.1755, -0.0026,  0.1381]],\n",
            "\n",
            "         [[ 0.1060, -0.1372, -0.1427],\n",
            "          [-0.0748,  0.1883, -0.0937],\n",
            "          [ 0.1360,  0.0273, -0.0841]],\n",
            "\n",
            "         [[ 0.1907,  0.2054, -0.1666],\n",
            "          [-0.0600,  0.1303,  0.0771],\n",
            "          [ 0.1683,  0.1215, -0.0738]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0254,  0.1488,  0.0255],\n",
            "          [ 0.1241, -0.1125,  0.0857],\n",
            "          [-0.0215, -0.0576,  0.0701]],\n",
            "\n",
            "         [[-0.1103, -0.0952,  0.1303],\n",
            "          [-0.1795, -0.1125, -0.1064],\n",
            "          [-0.0047, -0.0750,  0.0724]],\n",
            "\n",
            "         [[-0.1693, -0.1023,  0.0879],\n",
            "          [ 0.0489, -0.0826, -0.0493],\n",
            "          [ 0.0340,  0.1395, -0.0997]]],\n",
            "\n",
            "\n",
            "        [[[-0.0430, -0.1804, -0.1471],\n",
            "          [-0.1566, -0.0176, -0.0615],\n",
            "          [ 0.0051, -0.1214, -0.1542]],\n",
            "\n",
            "         [[-0.1773, -0.1524,  0.1692],\n",
            "          [ 0.0691, -0.0095, -0.1588],\n",
            "          [ 0.1688,  0.1306, -0.0334]],\n",
            "\n",
            "         [[ 0.0527,  0.1497,  0.0665],\n",
            "          [-0.1959,  0.1366,  0.1416],\n",
            "          [ 0.0399, -0.0880, -0.1539]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1595,  0.1685, -0.1660],\n",
            "          [-0.1826,  0.0429, -0.1084],\n",
            "          [-0.1488,  0.0676, -0.1806]],\n",
            "\n",
            "         [[-0.0017, -0.1404, -0.0703],\n",
            "          [-0.0122, -0.0124, -0.0565],\n",
            "          [-0.0487,  0.0471,  0.0822]],\n",
            "\n",
            "         [[-0.1134, -0.1439, -0.0529],\n",
            "          [-0.0684,  0.0283,  0.0871],\n",
            "          [-0.1602, -0.0776,  0.1687]]],\n",
            "\n",
            "\n",
            "        [[[-0.1536, -0.2317, -0.0203],\n",
            "          [-0.1442, -0.1383,  0.1183],\n",
            "          [-0.0928,  0.0558,  0.0082]],\n",
            "\n",
            "         [[-0.0895, -0.0789,  0.0303],\n",
            "          [ 0.0372,  0.1879, -0.1302],\n",
            "          [ 0.0720,  0.1863, -0.0843]],\n",
            "\n",
            "         [[ 0.1204, -0.1548,  0.0473],\n",
            "          [ 0.0575, -0.0447, -0.1286],\n",
            "          [ 0.1334, -0.0011,  0.1058]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1442,  0.0036, -0.0520],\n",
            "          [-0.0592, -0.0089,  0.0583],\n",
            "          [-0.0179, -0.2255, -0.0783]],\n",
            "\n",
            "         [[ 0.1105,  0.0028,  0.1452],\n",
            "          [-0.0151,  0.0286,  0.1319],\n",
            "          [-0.0250,  0.1850,  0.1311]],\n",
            "\n",
            "         [[-0.1033, -0.1314, -0.1321],\n",
            "          [ 0.0957, -0.1360,  0.1282],\n",
            "          [ 0.1338,  0.0719, -0.0263]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0638, -0.0921,  0.0166],\n",
            "          [ 0.0871, -0.1248,  0.1646],\n",
            "          [ 0.1072,  0.0451, -0.0663]],\n",
            "\n",
            "         [[-0.1228, -0.0975, -0.1997],\n",
            "          [ 0.1341,  0.1872,  0.0438],\n",
            "          [-0.0058, -0.0551,  0.0789]],\n",
            "\n",
            "         [[-0.1656, -0.1806, -0.0514],\n",
            "          [ 0.1831, -0.0052, -0.0441],\n",
            "          [ 0.1737,  0.1678, -0.1586]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1742, -0.1306,  0.0628],\n",
            "          [ 0.0454, -0.0167, -0.1826],\n",
            "          [-0.0696,  0.1070, -0.0222]],\n",
            "\n",
            "         [[ 0.0595, -0.1127, -0.2040],\n",
            "          [ 0.1804, -0.1063,  0.1180],\n",
            "          [-0.1663, -0.0362,  0.1126]],\n",
            "\n",
            "         [[-0.1853, -0.0433,  0.1162],\n",
            "          [-0.1129,  0.2016, -0.1319],\n",
            "          [ 0.0180, -0.0199,  0.1695]]],\n",
            "\n",
            "\n",
            "        [[[-0.0399, -0.0485, -0.0278],\n",
            "          [-0.1470,  0.0618,  0.0437],\n",
            "          [ 0.0799, -0.0038, -0.1611]],\n",
            "\n",
            "         [[ 0.2409, -0.1223, -0.0106],\n",
            "          [ 0.1207,  0.2089,  0.1825],\n",
            "          [-0.1882, -0.1358,  0.1136]],\n",
            "\n",
            "         [[-0.0469,  0.0944, -0.0881],\n",
            "          [ 0.1013,  0.0229, -0.1327],\n",
            "          [-0.0248,  0.1192,  0.0175]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0400, -0.0022,  0.0494],\n",
            "          [ 0.1120,  0.1376,  0.0847],\n",
            "          [-0.0004, -0.1270, -0.2464]],\n",
            "\n",
            "         [[ 0.0601, -0.1994,  0.0989],\n",
            "          [-0.1809,  0.1853,  0.0116],\n",
            "          [-0.1924,  0.0850, -0.0433]],\n",
            "\n",
            "         [[ 0.2104, -0.0052, -0.0638],\n",
            "          [ 0.1726,  0.1449,  0.0697],\n",
            "          [ 0.1570,  0.0471, -0.0660]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0192, -0.0807,  0.1128],\n",
            "          [ 0.0584,  0.0932, -0.0884],\n",
            "          [-0.2860,  0.0131,  0.0731]],\n",
            "\n",
            "         [[-0.0453,  0.1874,  0.1773],\n",
            "          [ 0.0502,  0.1119, -0.1402],\n",
            "          [ 0.0576,  0.1186, -0.1224]],\n",
            "\n",
            "         [[-0.1040,  0.0569,  0.1196],\n",
            "          [ 0.0357,  0.0041, -0.0828],\n",
            "          [-0.0248,  0.0329, -0.1752]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1515, -0.1262,  0.0216],\n",
            "          [-0.0814,  0.2475, -0.0236],\n",
            "          [ 0.0204,  0.1707,  0.1609]],\n",
            "\n",
            "         [[-0.1133,  0.1246, -0.1130],\n",
            "          [-0.0775,  0.0455, -0.1541],\n",
            "          [ 0.0308, -0.0880,  0.0157]],\n",
            "\n",
            "         [[-0.2324,  0.0891,  0.0880],\n",
            "          [-0.3235, -0.1045, -0.0006],\n",
            "          [-0.1417, -0.1556,  0.0089]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1820, 0.0841, 0.7775, 0.6792, 0.6091, 0.7423, 0.8596, 0.3234, 0.1643,\n",
            "        0.8247, 0.0716, 0.2719, 0.5007, 0.3967, 0.9103, 0.8522],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1170,  0.0913,  0.0038,  0.1869,  0.2911,  0.0817,  0.1730,  0.1217,\n",
            "        -0.0090, -0.1161,  0.1263,  0.1430,  0.2351, -0.1431, -0.2606,  0.2481],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.1413,  0.1365,  0.0472],\n",
            "          [ 0.0614, -0.0680,  0.1580],\n",
            "          [ 0.0569, -0.0242,  0.1081]],\n",
            "\n",
            "         [[-0.1447, -0.0689,  0.0967],\n",
            "          [ 0.1274,  0.1780,  0.0855],\n",
            "          [-0.0375,  0.0402,  0.0160]],\n",
            "\n",
            "         [[-0.0097, -0.0076,  0.1744],\n",
            "          [ 0.2092,  0.1917,  0.0218],\n",
            "          [ 0.1513, -0.0479,  0.0419]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0288,  0.1538,  0.0649],\n",
            "          [ 0.1803,  0.0249, -0.1718],\n",
            "          [-0.1371, -0.1535,  0.1737]],\n",
            "\n",
            "         [[ 0.1116,  0.0845,  0.0576],\n",
            "          [ 0.0941,  0.0777,  0.1919],\n",
            "          [ 0.2417, -0.1199,  0.1322]],\n",
            "\n",
            "         [[-0.0178,  0.1601, -0.1001],\n",
            "          [-0.0172, -0.1858,  0.0930],\n",
            "          [-0.0280,  0.1744,  0.0265]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0689, -0.0745, -0.0843],\n",
            "          [-0.2285, -0.2137,  0.0382],\n",
            "          [-0.1252, -0.0936, -0.0665]],\n",
            "\n",
            "         [[ 0.1187, -0.1371,  0.0572],\n",
            "          [-0.0686, -0.1017,  0.0211],\n",
            "          [-0.1920,  0.1891,  0.0866]],\n",
            "\n",
            "         [[-0.2669,  0.0877, -0.1175],\n",
            "          [ 0.1239, -0.0275, -0.1608],\n",
            "          [ 0.1392, -0.0673,  0.0532]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1700, -0.1576, -0.1510],\n",
            "          [-0.0637,  0.1659,  0.1741],\n",
            "          [-0.1082, -0.0501, -0.1723]],\n",
            "\n",
            "         [[ 0.0595, -0.2324, -0.1666],\n",
            "          [-0.1704, -0.1700,  0.0135],\n",
            "          [-0.0700,  0.1685,  0.1690]],\n",
            "\n",
            "         [[ 0.0612, -0.1056, -0.0152],\n",
            "          [-0.0316,  0.0522, -0.1581],\n",
            "          [-0.0816, -0.0244, -0.0210]]],\n",
            "\n",
            "\n",
            "        [[[-0.1377,  0.1649, -0.1297],\n",
            "          [-0.1611, -0.1987, -0.2240],\n",
            "          [-0.1474,  0.1438,  0.1688]],\n",
            "\n",
            "         [[-0.0225, -0.0839, -0.0294],\n",
            "          [-0.0240, -0.1038,  0.1297],\n",
            "          [-0.0341,  0.0023, -0.1716]],\n",
            "\n",
            "         [[ 0.0232,  0.1973,  0.1522],\n",
            "          [-0.0704, -0.0836,  0.0478],\n",
            "          [-0.0502,  0.0738,  0.0293]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1865,  0.0043,  0.0759],\n",
            "          [ 0.1553,  0.0466,  0.1180],\n",
            "          [ 0.0029, -0.0004, -0.2159]],\n",
            "\n",
            "         [[ 0.0489, -0.1947, -0.0817],\n",
            "          [-0.1341, -0.2545, -0.1537],\n",
            "          [ 0.0208, -0.2035, -0.0380]],\n",
            "\n",
            "         [[ 0.1906,  0.0904,  0.2143],\n",
            "          [-0.1881,  0.0322,  0.1067],\n",
            "          [-0.0755, -0.1189,  0.0279]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1879,  0.1343, -0.0714],\n",
            "          [-0.1408, -0.1348,  0.1753],\n",
            "          [ 0.1367,  0.1903,  0.1974]],\n",
            "\n",
            "         [[-0.0711,  0.0903, -0.1404],\n",
            "          [-0.0559, -0.0976,  0.1305],\n",
            "          [-0.1535, -0.0697, -0.0090]],\n",
            "\n",
            "         [[ 0.1254,  0.1069, -0.1553],\n",
            "          [ 0.1173,  0.0310, -0.0819],\n",
            "          [-0.2045,  0.0657, -0.1292]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0593, -0.0960, -0.0269],\n",
            "          [-0.1532,  0.0551,  0.1331],\n",
            "          [-0.1024,  0.0607,  0.0663]],\n",
            "\n",
            "         [[-0.1000, -0.0451, -0.1733],\n",
            "          [-0.1489,  0.1546, -0.1172],\n",
            "          [-0.0224,  0.0407, -0.0548]],\n",
            "\n",
            "         [[-0.0229,  0.0222, -0.0263],\n",
            "          [-0.0059, -0.1164,  0.0851],\n",
            "          [ 0.2006,  0.1603, -0.1206]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1672,  0.0860, -0.1285],\n",
            "          [-0.0307, -0.1509,  0.0715],\n",
            "          [ 0.0153, -0.1894, -0.1185]],\n",
            "\n",
            "         [[ 0.0189,  0.0993,  0.0508],\n",
            "          [-0.1651,  0.0188,  0.1268],\n",
            "          [-0.0198,  0.1892,  0.2199]],\n",
            "\n",
            "         [[ 0.1240,  0.0995, -0.1651],\n",
            "          [-0.1330,  0.0421,  0.0704],\n",
            "          [-0.2213,  0.0474,  0.0939]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0451, -0.1121,  0.0135],\n",
            "          [-0.1689, -0.0557, -0.0027],\n",
            "          [ 0.0110, -0.1481,  0.0728]],\n",
            "\n",
            "         [[ 0.1923, -0.1435,  0.1751],\n",
            "          [ 0.0975, -0.1193,  0.0453],\n",
            "          [ 0.1520, -0.1140, -0.0169]],\n",
            "\n",
            "         [[ 0.0297,  0.1087, -0.1441],\n",
            "          [ 0.0252, -0.0991, -0.0852],\n",
            "          [ 0.1477,  0.0152,  0.0965]]],\n",
            "\n",
            "\n",
            "        [[[-0.1516, -0.0285,  0.1739],\n",
            "          [ 0.0693, -0.0455, -0.1929],\n",
            "          [-0.0574, -0.1094,  0.0644]],\n",
            "\n",
            "         [[ 0.1525, -0.1205,  0.0894],\n",
            "          [ 0.0856,  0.0316, -0.0770],\n",
            "          [ 0.1114, -0.1029, -0.0673]],\n",
            "\n",
            "         [[-0.0154, -0.1106, -0.0460],\n",
            "          [-0.0378,  0.1755,  0.2202],\n",
            "          [-0.1258,  0.1228,  0.0168]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0251,  0.1463,  0.0849],\n",
            "          [ 0.0354, -0.1033, -0.1907],\n",
            "          [-0.1546, -0.1212, -0.1242]],\n",
            "\n",
            "         [[ 0.0764, -0.0279,  0.0136],\n",
            "          [ 0.0528, -0.1130, -0.0091],\n",
            "          [-0.0328,  0.0269, -0.0412]],\n",
            "\n",
            "         [[ 0.0614,  0.1917,  0.2243],\n",
            "          [-0.1069,  0.1336, -0.0604],\n",
            "          [-0.1796, -0.0528, -0.1839]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.7609, 0.8261, 0.5055, 0.0259, 0.9362, 0.6035, 0.0811, 0.6709, 0.9123,\n",
            "        0.6418, 0.2552, 0.5285, 0.3762, 0.7583, 0.0060, 0.5486],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0645,  0.0155, -0.0050, -0.0160,  0.0465,  0.0585, -0.0411,  0.1804,\n",
            "         0.0104,  0.0908, -0.0457, -0.0505, -0.0038,  0.1142, -0.0201, -0.1392],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0517,  0.0587, -0.2127],\n",
            "          [ 0.0876,  0.0664, -0.1676],\n",
            "          [ 0.0271,  0.1234,  0.1117]],\n",
            "\n",
            "         [[ 0.0008,  0.0845, -0.0615],\n",
            "          [ 0.0971,  0.0905,  0.2618],\n",
            "          [ 0.0768,  0.1306,  0.0114]],\n",
            "\n",
            "         [[-0.0745, -0.0595, -0.0822],\n",
            "          [ 0.1713,  0.0906,  0.1909],\n",
            "          [ 0.0005,  0.1534, -0.1650]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0072, -0.1664, -0.0409],\n",
            "          [-0.0644, -0.0425, -0.0652],\n",
            "          [-0.1128, -0.0147, -0.0186]],\n",
            "\n",
            "         [[-0.1489, -0.1249,  0.2075],\n",
            "          [ 0.1933,  0.1295,  0.0705],\n",
            "          [-0.1621, -0.1908,  0.0251]],\n",
            "\n",
            "         [[ 0.1237,  0.0023, -0.1164],\n",
            "          [ 0.0955,  0.1003,  0.0635],\n",
            "          [-0.1594, -0.0304, -0.0438]]],\n",
            "\n",
            "\n",
            "        [[[-0.2045,  0.1708, -0.0437],\n",
            "          [-0.1621,  0.0909,  0.1461],\n",
            "          [-0.0520, -0.0321,  0.1546]],\n",
            "\n",
            "         [[-0.1574,  0.0861, -0.1020],\n",
            "          [-0.1584, -0.0708,  0.1897],\n",
            "          [-0.0117,  0.0796, -0.1113]],\n",
            "\n",
            "         [[ 0.0561,  0.1791, -0.0977],\n",
            "          [-0.0805, -0.1761,  0.0796],\n",
            "          [-0.0915,  0.1838,  0.0880]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1562,  0.0683, -0.1611],\n",
            "          [-0.0916,  0.0409,  0.0851],\n",
            "          [ 0.1364,  0.1031, -0.1873]],\n",
            "\n",
            "         [[ 0.0176, -0.1368,  0.0253],\n",
            "          [-0.1480, -0.1745,  0.1142],\n",
            "          [ 0.0886,  0.0208,  0.1248]],\n",
            "\n",
            "         [[-0.0746,  0.0732,  0.0304],\n",
            "          [ 0.1183, -0.1814, -0.0516],\n",
            "          [-0.1264, -0.1136,  0.1061]]],\n",
            "\n",
            "\n",
            "        [[[-0.0824, -0.0739,  0.0433],\n",
            "          [ 0.0353,  0.2340,  0.0381],\n",
            "          [ 0.1009,  0.2417,  0.0702]],\n",
            "\n",
            "         [[-0.1385,  0.0537, -0.1041],\n",
            "          [-0.1699,  0.0215, -0.0686],\n",
            "          [-0.0890,  0.1388, -0.0041]],\n",
            "\n",
            "         [[-0.1361, -0.0414, -0.1228],\n",
            "          [ 0.1288, -0.1885, -0.0038],\n",
            "          [-0.0409,  0.1148, -0.1081]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0090, -0.0591,  0.1663],\n",
            "          [ 0.0243, -0.0223, -0.1486],\n",
            "          [ 0.0448, -0.0956, -0.1978]],\n",
            "\n",
            "         [[ 0.1327, -0.0651, -0.1677],\n",
            "          [-0.0864, -0.0841,  0.0688],\n",
            "          [-0.1594,  0.1038, -0.1510]],\n",
            "\n",
            "         [[-0.0482,  0.1687, -0.1202],\n",
            "          [ 0.1438,  0.2180,  0.0520],\n",
            "          [-0.0432,  0.0218, -0.1159]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1326,  0.0183, -0.0764],\n",
            "          [-0.0787,  0.1409, -0.1895],\n",
            "          [-0.0895, -0.0093,  0.0320]],\n",
            "\n",
            "         [[-0.0142,  0.1386,  0.2159],\n",
            "          [ 0.0117, -0.0571,  0.1280],\n",
            "          [ 0.0879,  0.1584,  0.0269]],\n",
            "\n",
            "         [[ 0.0363, -0.1622,  0.1763],\n",
            "          [-0.0207, -0.0136, -0.0751],\n",
            "          [ 0.0391, -0.0747,  0.0026]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2582, -0.0787,  0.1663],\n",
            "          [-0.1833,  0.1014,  0.1462],\n",
            "          [ 0.1495,  0.1085,  0.1346]],\n",
            "\n",
            "         [[ 0.1555, -0.1916,  0.1592],\n",
            "          [ 0.0619, -0.0210,  0.1621],\n",
            "          [-0.0810,  0.0407,  0.1469]],\n",
            "\n",
            "         [[ 0.0448,  0.0486,  0.1493],\n",
            "          [ 0.0236,  0.1443,  0.0646],\n",
            "          [-0.0138,  0.1896, -0.0569]]],\n",
            "\n",
            "\n",
            "        [[[-0.1183,  0.0928, -0.0932],\n",
            "          [ 0.1562,  0.1370,  0.0682],\n",
            "          [ 0.0860,  0.1100,  0.0191]],\n",
            "\n",
            "         [[-0.0258, -0.0701,  0.1111],\n",
            "          [-0.0178,  0.1441,  0.0534],\n",
            "          [-0.1393,  0.1529,  0.0213]],\n",
            "\n",
            "         [[-0.1642, -0.2243, -0.1621],\n",
            "          [ 0.0734,  0.0419, -0.1739],\n",
            "          [-0.1704, -0.0989, -0.1493]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0059, -0.0164, -0.0642],\n",
            "          [-0.0023, -0.2330,  0.1149],\n",
            "          [-0.1531,  0.0875, -0.0038]],\n",
            "\n",
            "         [[ 0.0857, -0.2016,  0.1302],\n",
            "          [-0.0527, -0.1339, -0.1605],\n",
            "          [ 0.1398,  0.0332,  0.1284]],\n",
            "\n",
            "         [[ 0.0261, -0.0978, -0.2229],\n",
            "          [-0.2145,  0.1347, -0.1630],\n",
            "          [ 0.0266,  0.0151, -0.0728]]],\n",
            "\n",
            "\n",
            "        [[[-0.1382,  0.1117,  0.1200],\n",
            "          [ 0.0902,  0.0919,  0.0455],\n",
            "          [ 0.0055,  0.0790, -0.1772]],\n",
            "\n",
            "         [[ 0.1046,  0.1397, -0.1229],\n",
            "          [-0.0531,  0.0972,  0.1327],\n",
            "          [-0.1246, -0.1432,  0.0178]],\n",
            "\n",
            "         [[ 0.0978,  0.0464, -0.0108],\n",
            "          [-0.0699,  0.1582,  0.1201],\n",
            "          [-0.1981, -0.0874,  0.0931]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1829,  0.1699,  0.1908],\n",
            "          [-0.1097, -0.0610, -0.1492],\n",
            "          [-0.1740, -0.1110,  0.0272]],\n",
            "\n",
            "         [[-0.0197,  0.0602, -0.0489],\n",
            "          [-0.1792, -0.0291,  0.0855],\n",
            "          [-0.0292, -0.1599, -0.2093]],\n",
            "\n",
            "         [[ 0.0375,  0.0593,  0.1302],\n",
            "          [ 0.0230, -0.2012,  0.0877],\n",
            "          [-0.0078,  0.0361,  0.0131]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.8388, 0.0288, 0.4809, 0.3584, 0.3561, 0.4501, 0.0782, 0.6054, 0.8092,\n",
            "        0.5943, 0.2947, 0.1060, 0.3462, 0.5568, 0.2829, 0.1232],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1161,  0.0413,  0.0689,  0.1673,  0.1997, -0.0183,  0.0146,  0.0079,\n",
            "        -0.0202, -0.0223,  0.0322, -0.0171,  0.0495,  0.0460,  0.0031,  0.0663],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 8.7274e-02,  3.4621e-04, -2.6880e-02],\n",
            "          [-1.6021e-01,  7.9082e-02, -5.8637e-02],\n",
            "          [-8.0360e-02, -1.0040e-01,  4.7266e-02]],\n",
            "\n",
            "         [[ 1.5181e-01, -5.6422e-02, -4.2584e-02],\n",
            "          [ 1.3528e-01,  1.0381e-02, -6.9852e-03],\n",
            "          [-9.8660e-02, -1.6716e-01, -9.7624e-02]],\n",
            "\n",
            "         [[-1.4629e-01, -1.0425e-01, -2.1455e-01],\n",
            "          [ 4.5989e-02, -1.8839e-02, -1.3860e-01],\n",
            "          [-1.9208e-01, -5.3669e-02, -9.1303e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5111e-01,  1.5421e-01,  1.9111e-01],\n",
            "          [ 1.4902e-01, -1.3253e-01,  1.1658e-01],\n",
            "          [-8.1728e-02,  1.2420e-01, -4.2854e-02]],\n",
            "\n",
            "         [[-1.6761e-01, -2.1807e-01, -8.0698e-02],\n",
            "          [-4.4275e-02, -2.1530e-01, -3.2963e-02],\n",
            "          [-1.4659e-01,  1.2925e-02, -1.0096e-01]],\n",
            "\n",
            "         [[ 1.0215e-01, -6.5900e-02,  1.2193e-01],\n",
            "          [ 9.3950e-02,  3.1939e-02, -3.9270e-02],\n",
            "          [ 1.5002e-01,  1.9333e-01,  5.9493e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9201e-01, -1.4787e-01, -8.2476e-02],\n",
            "          [ 3.3380e-02, -2.4467e-01,  5.9825e-02],\n",
            "          [-1.8173e-01,  6.3110e-02, -2.1213e-01]],\n",
            "\n",
            "         [[-1.9025e-01, -1.3286e-01,  1.6110e-01],\n",
            "          [-7.3069e-02, -1.1047e-01,  6.8171e-02],\n",
            "          [-1.9479e-01,  8.5931e-02, -3.3336e-02]],\n",
            "\n",
            "         [[-4.7661e-02, -3.9092e-02,  1.2387e-03],\n",
            "          [ 3.9959e-03, -7.7725e-02, -1.1792e-02],\n",
            "          [-1.6345e-02,  1.6743e-01, -7.9155e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.0725e-02,  1.1247e-02, -5.0812e-02],\n",
            "          [ 9.3251e-02, -1.3575e-01, -1.9222e-01],\n",
            "          [-1.5100e-01, -5.9777e-02, -1.0317e-01]],\n",
            "\n",
            "         [[-7.0037e-02, -2.1026e-01, -2.4029e-03],\n",
            "          [ 8.6929e-02, -8.7872e-02, -1.0348e-01],\n",
            "          [ 3.1165e-02,  7.5075e-02,  1.7317e-01]],\n",
            "\n",
            "         [[ 3.5397e-02, -5.0704e-02, -2.2267e-01],\n",
            "          [-2.2904e-02, -1.8604e-01, -1.1772e-01],\n",
            "          [ 9.1134e-02, -2.7461e-02,  5.2162e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.6002e-02, -2.7674e-02, -1.5381e-02],\n",
            "          [ 2.6502e-02, -1.2847e-01,  1.5864e-01],\n",
            "          [-9.1712e-02,  8.5317e-07,  1.7146e-01]],\n",
            "\n",
            "         [[ 1.3951e-01, -1.2312e-01, -2.0088e-01],\n",
            "          [-1.1135e-03,  7.0613e-02,  8.1636e-04],\n",
            "          [-8.2793e-02,  2.1381e-02,  7.7412e-02]],\n",
            "\n",
            "         [[ 1.6290e-01,  4.1288e-02,  8.4626e-02],\n",
            "          [ 2.0642e-01,  2.2765e-01,  1.4629e-01],\n",
            "          [-1.8401e-02,  2.1712e-01,  2.2049e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.2229e-02, -1.0656e-01,  3.2826e-02],\n",
            "          [-1.9441e-01, -1.7481e-02, -1.1434e-01],\n",
            "          [-1.0748e-01, -1.0037e-01, -1.3731e-01]],\n",
            "\n",
            "         [[ 1.0137e-01,  1.4023e-01,  1.7723e-02],\n",
            "          [-3.9522e-02, -4.6680e-03,  1.0757e-02],\n",
            "          [ 1.2475e-01,  2.2657e-01,  2.1702e-02]],\n",
            "\n",
            "         [[ 1.6491e-02, -1.0898e-01, -3.5049e-02],\n",
            "          [-2.6706e-01, -4.8807e-03,  2.7484e-02],\n",
            "          [ 4.3757e-03, -9.6284e-02, -2.9964e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 6.7649e-02, -1.2681e-01,  1.9957e-01],\n",
            "          [ 1.6096e-01,  7.5571e-02,  1.4363e-01],\n",
            "          [ 2.2661e-02, -6.8440e-02, -1.2340e-01]],\n",
            "\n",
            "         [[-1.4890e-01,  1.8748e-01,  7.4776e-02],\n",
            "          [ 5.0667e-02, -1.5721e-01, -4.8817e-02],\n",
            "          [ 9.4028e-02, -8.3876e-03, -5.1882e-02]],\n",
            "\n",
            "         [[-7.7973e-02,  5.1402e-02,  2.7175e-02],\n",
            "          [ 1.1186e-01,  7.8438e-02, -2.2012e-02],\n",
            "          [ 1.8510e-01,  1.2798e-01,  1.0412e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9971e-01,  7.4679e-02, -7.1390e-02],\n",
            "          [-3.0172e-02, -2.0323e-01,  1.6208e-01],\n",
            "          [-4.9876e-02,  1.3340e-01,  1.9386e-01]],\n",
            "\n",
            "         [[-1.3747e-01, -9.5055e-02,  4.3845e-02],\n",
            "          [-1.8319e-01, -1.3907e-02, -1.7613e-01],\n",
            "          [-9.6697e-02, -2.8492e-02, -5.3051e-02]],\n",
            "\n",
            "         [[ 1.4404e-01, -1.1443e-01, -3.7915e-02],\n",
            "          [-1.2132e-01, -8.2482e-02, -7.9086e-02],\n",
            "          [-1.6689e-01,  1.4022e-01, -9.0589e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8247e-01, -2.5938e-02, -1.7689e-02],\n",
            "          [ 1.3237e-01,  1.5244e-01,  1.4721e-01],\n",
            "          [ 2.1296e-02,  1.6684e-02, -2.8793e-03]],\n",
            "\n",
            "         [[-1.6049e-01,  1.3150e-01, -1.0929e-01],\n",
            "          [-4.7011e-02, -8.6059e-02,  3.1293e-02],\n",
            "          [ 1.7174e-01,  5.7870e-02, -1.9896e-02]],\n",
            "\n",
            "         [[-9.1356e-02, -1.5978e-01, -4.0316e-02],\n",
            "          [ 6.7515e-02,  2.1940e-02,  8.4878e-02],\n",
            "          [ 1.1913e-02, -7.9681e-02,  3.0790e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.8242e-02, -2.0403e-01,  1.6945e-01],\n",
            "          [ 8.2714e-02, -1.0930e-01, -1.1862e-01],\n",
            "          [ 1.8998e-02,  2.6218e-02,  1.4244e-01]],\n",
            "\n",
            "         [[-8.9069e-02,  6.8236e-02, -3.1684e-02],\n",
            "          [-1.8902e-01, -2.0695e-01,  8.3321e-03],\n",
            "          [-2.1650e-01, -1.7613e-01, -9.5793e-02]],\n",
            "\n",
            "         [[-1.8890e-01, -1.5160e-01,  1.8294e-01],\n",
            "          [-1.1516e-01, -1.0123e-01,  1.6624e-01],\n",
            "          [-8.0917e-02,  2.4380e-02, -1.4629e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8459e-01, -1.0489e-02, -3.8171e-02],\n",
            "          [ 2.2610e-01,  1.0040e-01,  1.3439e-01],\n",
            "          [ 1.8919e-01,  8.3385e-02, -1.2340e-01]],\n",
            "\n",
            "         [[ 1.3312e-01, -6.5131e-02,  1.0760e-01],\n",
            "          [ 1.9155e-01,  8.6433e-03, -1.0167e-01],\n",
            "          [ 8.6525e-02,  1.8386e-01,  1.5167e-03]],\n",
            "\n",
            "         [[-6.0161e-02, -6.2888e-02,  3.8213e-02],\n",
            "          [-1.1489e-01,  1.3564e-01,  1.5499e-01],\n",
            "          [ 1.7058e-01, -3.4504e-02,  1.5555e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.0750e-01,  1.7752e-01,  1.0721e-01],\n",
            "          [ 1.2635e-01,  2.3348e-01,  4.9247e-02],\n",
            "          [-7.9186e-02,  1.7745e-01,  2.1532e-01]],\n",
            "\n",
            "         [[ 9.3673e-02,  1.1575e-01,  5.9096e-02],\n",
            "          [-1.0342e-01, -1.9313e-01, -2.9869e-02],\n",
            "          [-3.7579e-02, -1.0834e-01,  6.8199e-02]],\n",
            "\n",
            "         [[ 1.6438e-01,  2.3216e-01,  1.7476e-01],\n",
            "          [ 4.1976e-02, -1.4223e-01, -1.9108e-01],\n",
            "          [ 1.0880e-01, -5.4524e-02,  4.1290e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.9165, 0.8750, 0.6693, 0.3827, 0.3876, 0.2735, 0.5501, 0.6187, 0.5269,\n",
            "        0.3481, 1.0253, 0.4982, 0.1876, 0.8578, 0.4076, 0.6373, 0.5596, 0.7512,\n",
            "        0.7594, 0.6416, 0.6374, 0.2494, 0.7488, 0.1063, 0.8140, 0.6948, 0.2597,\n",
            "        0.9648, 0.9080, 0.3351, 0.9153, 0.6792], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0975, -0.0457,  0.0591,  0.0276,  0.0905,  0.0877,  0.0056,  0.0571,\n",
            "         0.0740, -0.0441,  0.1178,  0.0436,  0.0170, -0.0193,  0.0123,  0.0887,\n",
            "         0.0965,  0.0117,  0.0176, -0.0073,  0.0760, -0.0137,  0.0428, -0.0514,\n",
            "         0.0254, -0.0037,  0.0080,  0.0376,  0.1789,  0.0314,  0.2276,  0.0338],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 9.5762e-02, -8.0324e-02, -1.2581e-01],\n",
            "          [-8.0092e-02,  5.0773e-02, -4.0705e-02],\n",
            "          [-6.2267e-02,  1.0186e-01,  1.2676e-01]],\n",
            "\n",
            "         [[ 8.3054e-02, -1.3344e-02, -2.1011e-02],\n",
            "          [-9.9990e-02, -1.3970e-01, -9.0396e-02],\n",
            "          [ 6.6999e-02, -5.3291e-02, -2.8588e-02]],\n",
            "\n",
            "         [[-5.2312e-02,  1.1345e-01,  1.6307e-02],\n",
            "          [ 8.8361e-02,  1.4317e-01, -1.2458e-02],\n",
            "          [-4.0519e-02,  1.8520e-01,  3.5645e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3609e-02, -7.6117e-02,  2.6316e-02],\n",
            "          [ 1.0652e-01, -1.1400e-01, -7.7832e-02],\n",
            "          [ 1.4094e-02,  1.3831e-04,  5.9026e-03]],\n",
            "\n",
            "         [[ 5.1365e-02, -9.2172e-02,  3.2160e-02],\n",
            "          [-6.9505e-02, -5.2737e-03,  2.4422e-02],\n",
            "          [ 6.6312e-02,  3.8950e-02,  8.7418e-02]],\n",
            "\n",
            "         [[ 1.1748e-01, -1.5958e-01,  4.1208e-02],\n",
            "          [ 6.8898e-03,  7.2510e-02, -1.2255e-01],\n",
            "          [ 1.1033e-01,  1.3388e-01, -7.1936e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.1549e-02, -3.3467e-02, -3.1171e-02],\n",
            "          [ 1.0343e-01, -8.7986e-02, -5.4590e-02],\n",
            "          [ 2.9070e-02,  6.4597e-03,  7.2289e-03]],\n",
            "\n",
            "         [[ 4.1093e-02,  9.5849e-02, -1.2426e-01],\n",
            "          [-8.8114e-02, -1.2932e-01, -2.6718e-02],\n",
            "          [-8.3889e-03, -1.1974e-01,  1.3214e-01]],\n",
            "\n",
            "         [[ 2.6221e-02,  2.3658e-02, -1.3613e-01],\n",
            "          [-6.2584e-02, -1.2019e-01,  9.8464e-02],\n",
            "          [-8.5514e-02, -4.0570e-02,  8.7582e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.2521e-03, -9.7358e-02, -1.6429e-02],\n",
            "          [ 3.4666e-03,  8.4072e-02, -1.2027e-01],\n",
            "          [ 1.0454e-01,  1.2620e-01,  7.2549e-02]],\n",
            "\n",
            "         [[-4.9978e-02,  3.4246e-02, -9.6083e-02],\n",
            "          [-1.3944e-02, -2.2308e-03,  6.4488e-02],\n",
            "          [ 6.8580e-02,  5.9234e-02,  1.5926e-02]],\n",
            "\n",
            "         [[ 3.2647e-02, -1.0102e-01, -1.4780e-01],\n",
            "          [-3.9113e-02,  7.5587e-02,  1.2505e-01],\n",
            "          [-9.0495e-02,  2.1324e-02,  1.4511e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.4886e-01, -2.6689e-02, -9.5297e-02],\n",
            "          [-3.9997e-02,  1.2682e-01, -8.2038e-02],\n",
            "          [ 1.3626e-01, -1.4985e-01,  9.1501e-02]],\n",
            "\n",
            "         [[-9.8318e-02,  4.5298e-02, -8.4839e-02],\n",
            "          [-5.0572e-02, -1.6923e-02, -6.4139e-02],\n",
            "          [ 1.3609e-01,  6.8201e-02, -2.1216e-02]],\n",
            "\n",
            "         [[-1.8584e-02,  1.0743e-01, -5.5850e-02],\n",
            "          [-1.3058e-02,  5.1339e-02,  7.4742e-02],\n",
            "          [-1.1669e-01,  1.1052e-01,  4.2824e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1218e-02,  6.1922e-02,  1.0984e-01],\n",
            "          [-6.0267e-03,  4.9236e-02,  1.1445e-01],\n",
            "          [-1.1256e-01,  4.4244e-02, -6.6752e-02]],\n",
            "\n",
            "         [[ 6.9870e-02,  2.6143e-03,  5.8545e-02],\n",
            "          [-3.6992e-02, -7.8890e-02,  5.3393e-02],\n",
            "          [-8.0531e-02, -5.6268e-02, -8.1114e-02]],\n",
            "\n",
            "         [[-1.1510e-01,  4.2928e-02,  1.2511e-01],\n",
            "          [ 5.3856e-02,  6.2041e-02,  1.2637e-01],\n",
            "          [-1.1556e-01, -2.5117e-02, -5.9825e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.4311e-01,  3.8771e-02, -2.2789e-02],\n",
            "          [-8.9501e-02, -1.1567e-01,  1.1683e-01],\n",
            "          [ 2.1609e-02,  6.4803e-02, -1.0862e-01]],\n",
            "\n",
            "         [[-1.6565e-02,  8.4718e-02,  3.0325e-02],\n",
            "          [ 6.9815e-02,  1.7723e-01,  3.1172e-02],\n",
            "          [ 1.1192e-01,  4.6989e-02, -9.2051e-02]],\n",
            "\n",
            "         [[ 2.6274e-02, -1.1616e-01, -5.0532e-03],\n",
            "          [-7.2899e-02, -1.0548e-01, -1.4444e-01],\n",
            "          [ 3.8951e-02, -9.9559e-03,  2.9339e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.7332e-02, -1.1433e-01,  6.5589e-02],\n",
            "          [ 2.0585e-02,  9.4455e-02, -1.3162e-01],\n",
            "          [-9.5334e-02,  6.6290e-02,  4.7707e-02]],\n",
            "\n",
            "         [[ 3.1766e-02, -5.2251e-02, -3.9777e-02],\n",
            "          [ 7.7636e-03,  5.3320e-02,  7.2544e-02],\n",
            "          [-8.4075e-02,  7.3581e-02,  1.2818e-01]],\n",
            "\n",
            "         [[-8.8877e-02,  1.1572e-01,  1.1376e-01],\n",
            "          [-1.0418e-01,  7.8755e-02, -6.6634e-02],\n",
            "          [ 5.5446e-02, -9.4790e-02,  7.5397e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.6883e-02,  1.0973e-01,  1.3418e-02],\n",
            "          [ 3.9401e-02,  7.3880e-02,  1.2522e-01],\n",
            "          [-1.2544e-01,  1.8090e-01,  7.5959e-02]],\n",
            "\n",
            "         [[-4.6551e-02,  9.1144e-02, -1.7230e-02],\n",
            "          [-1.0509e-01,  1.2207e-01,  1.2492e-01],\n",
            "          [-1.3113e-01, -6.4698e-02,  1.4099e-01]],\n",
            "\n",
            "         [[-1.4235e-01, -4.3874e-02,  2.7161e-02],\n",
            "          [ 7.8219e-02,  9.3310e-02, -4.4590e-02],\n",
            "          [-5.0019e-02, -3.0202e-02,  4.4479e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5854e-01,  4.5707e-02,  1.2068e-01],\n",
            "          [-6.2347e-02, -4.6878e-02, -1.4910e-01],\n",
            "          [ 5.5686e-02,  3.6524e-02,  1.3696e-01]],\n",
            "\n",
            "         [[-5.9459e-02, -1.1291e-01,  2.7183e-02],\n",
            "          [ 8.6022e-02, -8.3116e-02, -1.1880e-01],\n",
            "          [ 2.6858e-02,  4.2908e-02,  2.6635e-02]],\n",
            "\n",
            "         [[-1.2883e-01,  4.9592e-02,  7.0468e-03],\n",
            "          [ 9.6411e-02, -1.6332e-02, -1.3845e-01],\n",
            "          [ 2.5524e-02, -2.3244e-02, -1.0377e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.5828e-01,  5.9161e-02,  3.5797e-02],\n",
            "          [-2.3253e-02, -5.0818e-02, -2.3029e-04],\n",
            "          [-9.8364e-02, -1.1562e-01,  7.7226e-02]],\n",
            "\n",
            "         [[ 9.7783e-02, -7.2193e-02, -7.8835e-02],\n",
            "          [ 1.1154e-02, -2.9949e-02, -9.4687e-02],\n",
            "          [-2.9163e-02, -4.6921e-02,  1.8385e-02]],\n",
            "\n",
            "         [[ 1.6190e-02,  4.5887e-03,  2.2355e-02],\n",
            "          [ 7.9691e-03,  6.1067e-02, -3.9500e-02],\n",
            "          [ 7.5549e-02,  1.8610e-01,  1.2902e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.3955e-02,  1.6837e-01,  9.9748e-02],\n",
            "          [ 1.6463e-01, -5.9397e-02, -1.5755e-01],\n",
            "          [ 4.1668e-02,  1.1386e-02, -3.4025e-02]],\n",
            "\n",
            "         [[-2.0501e-02,  5.4387e-03, -1.3053e-01],\n",
            "          [ 2.9476e-02,  2.9138e-03, -1.3844e-01],\n",
            "          [ 1.8611e-01,  1.0482e-02, -9.1976e-02]],\n",
            "\n",
            "         [[-3.4251e-02,  3.4129e-02,  9.7372e-02],\n",
            "          [-1.5609e-01, -6.6541e-03,  7.8942e-02],\n",
            "          [-8.7689e-02, -7.1821e-02, -9.7682e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4133, 0.1615, 0.1507, 0.6474, 0.6385, 0.4548, 0.2252, 0.3660, 0.3215,\n",
            "        0.5345, 0.5593, 0.8863, 0.0675, 0.7027, 0.8299, 0.4053, 0.4267, 0.7674,\n",
            "        0.1419, 0.4628, 0.0636, 0.8182, 0.7323, 0.1459, 0.3472, 0.3111, 0.8655,\n",
            "        0.8251, 0.4281, 0.2628, 0.2375, 0.9418], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0502, -0.0005,  0.0085,  0.1066,  0.2242,  0.0538,  0.0679,  0.1045,\n",
            "         0.0974,  0.0493,  0.1064,  0.1518,  0.0312,  0.0379, -0.0400,  0.0269,\n",
            "        -0.0222,  0.0209, -0.0503,  0.0838,  0.0564, -0.0139,  0.1082,  0.1421,\n",
            "        -0.0118,  0.0567,  0.0530,  0.0429,  0.0562, -0.0378,  0.0796, -0.0106],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.4632]],\n",
            "\n",
            "         [[ 0.0136]],\n",
            "\n",
            "         [[-0.1961]],\n",
            "\n",
            "         [[ 0.4263]],\n",
            "\n",
            "         [[ 0.4430]],\n",
            "\n",
            "         [[-0.4137]],\n",
            "\n",
            "         [[-0.2665]],\n",
            "\n",
            "         [[ 0.1428]],\n",
            "\n",
            "         [[ 0.2774]],\n",
            "\n",
            "         [[ 0.4074]],\n",
            "\n",
            "         [[-0.6780]],\n",
            "\n",
            "         [[-0.2094]],\n",
            "\n",
            "         [[-0.3953]],\n",
            "\n",
            "         [[-0.3681]],\n",
            "\n",
            "         [[-0.3256]],\n",
            "\n",
            "         [[-0.0136]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1118]],\n",
            "\n",
            "         [[ 0.5027]],\n",
            "\n",
            "         [[ 0.5751]],\n",
            "\n",
            "         [[-0.0848]],\n",
            "\n",
            "         [[-0.0280]],\n",
            "\n",
            "         [[-0.3552]],\n",
            "\n",
            "         [[ 0.3088]],\n",
            "\n",
            "         [[ 0.5996]],\n",
            "\n",
            "         [[ 0.6241]],\n",
            "\n",
            "         [[ 0.5657]],\n",
            "\n",
            "         [[-0.4070]],\n",
            "\n",
            "         [[-0.2551]],\n",
            "\n",
            "         [[ 0.0202]],\n",
            "\n",
            "         [[ 0.2488]],\n",
            "\n",
            "         [[-0.4754]],\n",
            "\n",
            "         [[-0.5542]]],\n",
            "\n",
            "\n",
            "        [[[-0.2193]],\n",
            "\n",
            "         [[ 0.0110]],\n",
            "\n",
            "         [[ 0.0383]],\n",
            "\n",
            "         [[-0.4471]],\n",
            "\n",
            "         [[-0.5593]],\n",
            "\n",
            "         [[ 0.3222]],\n",
            "\n",
            "         [[ 0.1391]],\n",
            "\n",
            "         [[ 0.4646]],\n",
            "\n",
            "         [[-0.1554]],\n",
            "\n",
            "         [[ 0.1092]],\n",
            "\n",
            "         [[-0.2684]],\n",
            "\n",
            "         [[ 0.2150]],\n",
            "\n",
            "         [[-0.3099]],\n",
            "\n",
            "         [[-0.3902]],\n",
            "\n",
            "         [[ 0.0737]],\n",
            "\n",
            "         [[ 0.0161]]],\n",
            "\n",
            "\n",
            "        [[[-0.4992]],\n",
            "\n",
            "         [[-0.1527]],\n",
            "\n",
            "         [[ 0.5476]],\n",
            "\n",
            "         [[ 0.2769]],\n",
            "\n",
            "         [[-0.0795]],\n",
            "\n",
            "         [[ 0.1698]],\n",
            "\n",
            "         [[ 0.0434]],\n",
            "\n",
            "         [[ 0.0712]],\n",
            "\n",
            "         [[ 0.3011]],\n",
            "\n",
            "         [[ 0.2820]],\n",
            "\n",
            "         [[-0.5031]],\n",
            "\n",
            "         [[-0.1380]],\n",
            "\n",
            "         [[-0.5986]],\n",
            "\n",
            "         [[-0.6204]],\n",
            "\n",
            "         [[ 0.3550]],\n",
            "\n",
            "         [[-0.5386]]],\n",
            "\n",
            "\n",
            "        [[[-0.1202]],\n",
            "\n",
            "         [[-0.2055]],\n",
            "\n",
            "         [[-0.2621]],\n",
            "\n",
            "         [[-0.0754]],\n",
            "\n",
            "         [[-0.1677]],\n",
            "\n",
            "         [[ 0.5002]],\n",
            "\n",
            "         [[-0.3691]],\n",
            "\n",
            "         [[ 0.5325]],\n",
            "\n",
            "         [[-0.2908]],\n",
            "\n",
            "         [[ 0.5787]],\n",
            "\n",
            "         [[ 0.1543]],\n",
            "\n",
            "         [[ 0.3884]],\n",
            "\n",
            "         [[-0.4687]],\n",
            "\n",
            "         [[ 0.6099]],\n",
            "\n",
            "         [[-0.2626]],\n",
            "\n",
            "         [[ 0.5713]]],\n",
            "\n",
            "\n",
            "        [[[-0.2626]],\n",
            "\n",
            "         [[ 0.2977]],\n",
            "\n",
            "         [[-0.5591]],\n",
            "\n",
            "         [[-0.3772]],\n",
            "\n",
            "         [[ 0.2066]],\n",
            "\n",
            "         [[-0.3542]],\n",
            "\n",
            "         [[-0.3691]],\n",
            "\n",
            "         [[-0.1358]],\n",
            "\n",
            "         [[ 0.3800]],\n",
            "\n",
            "         [[ 0.3922]],\n",
            "\n",
            "         [[-0.1032]],\n",
            "\n",
            "         [[ 0.6083]],\n",
            "\n",
            "         [[-0.3446]],\n",
            "\n",
            "         [[ 0.6229]],\n",
            "\n",
            "         [[ 0.4045]],\n",
            "\n",
            "         [[-0.0146]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2709]],\n",
            "\n",
            "         [[ 0.3233]],\n",
            "\n",
            "         [[ 0.2508]],\n",
            "\n",
            "         [[-0.2727]],\n",
            "\n",
            "         [[ 0.4086]],\n",
            "\n",
            "         [[ 0.4431]],\n",
            "\n",
            "         [[-0.1892]],\n",
            "\n",
            "         [[ 0.2815]],\n",
            "\n",
            "         [[-0.1950]],\n",
            "\n",
            "         [[-0.5482]],\n",
            "\n",
            "         [[ 0.4904]],\n",
            "\n",
            "         [[ 0.5009]],\n",
            "\n",
            "         [[-0.6691]],\n",
            "\n",
            "         [[ 0.3328]],\n",
            "\n",
            "         [[ 0.0278]],\n",
            "\n",
            "         [[-0.4964]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1273]],\n",
            "\n",
            "         [[-0.3978]],\n",
            "\n",
            "         [[-0.0242]],\n",
            "\n",
            "         [[-0.3637]],\n",
            "\n",
            "         [[ 0.4903]],\n",
            "\n",
            "         [[-0.0678]],\n",
            "\n",
            "         [[ 0.3208]],\n",
            "\n",
            "         [[-0.4067]],\n",
            "\n",
            "         [[ 0.4073]],\n",
            "\n",
            "         [[ 0.4877]],\n",
            "\n",
            "         [[-0.0406]],\n",
            "\n",
            "         [[-0.0158]],\n",
            "\n",
            "         [[ 0.1565]],\n",
            "\n",
            "         [[ 0.1474]],\n",
            "\n",
            "         [[-0.4467]],\n",
            "\n",
            "         [[ 0.0496]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4636]],\n",
            "\n",
            "         [[ 0.5342]],\n",
            "\n",
            "         [[-0.5887]],\n",
            "\n",
            "         [[ 0.4539]],\n",
            "\n",
            "         [[ 0.1378]],\n",
            "\n",
            "         [[-0.2224]],\n",
            "\n",
            "         [[-0.4896]],\n",
            "\n",
            "         [[ 0.3062]],\n",
            "\n",
            "         [[-0.4006]],\n",
            "\n",
            "         [[ 0.3460]],\n",
            "\n",
            "         [[-0.4006]],\n",
            "\n",
            "         [[-0.2209]],\n",
            "\n",
            "         [[-0.3897]],\n",
            "\n",
            "         [[ 0.1032]],\n",
            "\n",
            "         [[-0.3883]],\n",
            "\n",
            "         [[ 0.3041]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1407]],\n",
            "\n",
            "         [[-0.4884]],\n",
            "\n",
            "         [[ 0.3266]],\n",
            "\n",
            "         [[ 0.0763]],\n",
            "\n",
            "         [[-0.3255]],\n",
            "\n",
            "         [[-0.5520]],\n",
            "\n",
            "         [[-0.2922]],\n",
            "\n",
            "         [[ 0.6312]],\n",
            "\n",
            "         [[ 0.6285]],\n",
            "\n",
            "         [[-0.2514]],\n",
            "\n",
            "         [[ 0.4359]],\n",
            "\n",
            "         [[ 0.1008]],\n",
            "\n",
            "         [[ 0.3593]],\n",
            "\n",
            "         [[ 0.5472]],\n",
            "\n",
            "         [[ 0.4707]],\n",
            "\n",
            "         [[-0.4677]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2108]],\n",
            "\n",
            "         [[ 0.0783]],\n",
            "\n",
            "         [[ 0.0998]],\n",
            "\n",
            "         [[-0.0251]],\n",
            "\n",
            "         [[-0.3945]],\n",
            "\n",
            "         [[ 0.1685]],\n",
            "\n",
            "         [[ 0.3127]],\n",
            "\n",
            "         [[ 0.3174]],\n",
            "\n",
            "         [[-0.3850]],\n",
            "\n",
            "         [[-0.1144]],\n",
            "\n",
            "         [[-0.0062]],\n",
            "\n",
            "         [[ 0.2123]],\n",
            "\n",
            "         [[-0.5054]],\n",
            "\n",
            "         [[-0.1491]],\n",
            "\n",
            "         [[-0.3327]],\n",
            "\n",
            "         [[ 0.4844]]],\n",
            "\n",
            "\n",
            "        [[[-0.4005]],\n",
            "\n",
            "         [[ 0.2662]],\n",
            "\n",
            "         [[-0.5605]],\n",
            "\n",
            "         [[-0.4846]],\n",
            "\n",
            "         [[ 0.5845]],\n",
            "\n",
            "         [[-0.6398]],\n",
            "\n",
            "         [[-0.0165]],\n",
            "\n",
            "         [[ 0.4020]],\n",
            "\n",
            "         [[-0.0035]],\n",
            "\n",
            "         [[ 0.0677]],\n",
            "\n",
            "         [[ 0.3173]],\n",
            "\n",
            "         [[-0.2516]],\n",
            "\n",
            "         [[ 0.0113]],\n",
            "\n",
            "         [[-0.5198]],\n",
            "\n",
            "         [[-0.3245]],\n",
            "\n",
            "         [[-0.2430]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2406]],\n",
            "\n",
            "         [[ 0.2361]],\n",
            "\n",
            "         [[ 0.3214]],\n",
            "\n",
            "         [[-0.5328]],\n",
            "\n",
            "         [[-0.5174]],\n",
            "\n",
            "         [[ 0.2120]],\n",
            "\n",
            "         [[-0.5868]],\n",
            "\n",
            "         [[ 0.0939]],\n",
            "\n",
            "         [[ 0.2347]],\n",
            "\n",
            "         [[-0.4703]],\n",
            "\n",
            "         [[ 0.3479]],\n",
            "\n",
            "         [[ 0.1352]],\n",
            "\n",
            "         [[-0.5300]],\n",
            "\n",
            "         [[-0.4719]],\n",
            "\n",
            "         [[-0.6568]],\n",
            "\n",
            "         [[-0.3455]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1290]],\n",
            "\n",
            "         [[-0.0802]],\n",
            "\n",
            "         [[ 0.2120]],\n",
            "\n",
            "         [[ 0.1704]],\n",
            "\n",
            "         [[-0.2897]],\n",
            "\n",
            "         [[-0.4724]],\n",
            "\n",
            "         [[-0.3803]],\n",
            "\n",
            "         [[ 0.2208]],\n",
            "\n",
            "         [[ 0.1171]],\n",
            "\n",
            "         [[ 0.4645]],\n",
            "\n",
            "         [[ 0.5597]],\n",
            "\n",
            "         [[ 0.4499]],\n",
            "\n",
            "         [[-0.3563]],\n",
            "\n",
            "         [[-0.1549]],\n",
            "\n",
            "         [[ 0.0812]],\n",
            "\n",
            "         [[-0.4951]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1466]],\n",
            "\n",
            "         [[ 0.2099]],\n",
            "\n",
            "         [[ 0.0739]],\n",
            "\n",
            "         [[ 0.1749]],\n",
            "\n",
            "         [[ 0.5183]],\n",
            "\n",
            "         [[-0.5243]],\n",
            "\n",
            "         [[ 0.5019]],\n",
            "\n",
            "         [[-0.0532]],\n",
            "\n",
            "         [[-0.4242]],\n",
            "\n",
            "         [[ 0.4832]],\n",
            "\n",
            "         [[ 0.3684]],\n",
            "\n",
            "         [[-0.3005]],\n",
            "\n",
            "         [[-0.1353]],\n",
            "\n",
            "         [[ 0.4035]],\n",
            "\n",
            "         [[-0.2156]],\n",
            "\n",
            "         [[ 0.1233]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2839]],\n",
            "\n",
            "         [[ 0.5587]],\n",
            "\n",
            "         [[ 0.4365]],\n",
            "\n",
            "         [[ 0.3843]],\n",
            "\n",
            "         [[-0.4165]],\n",
            "\n",
            "         [[-0.3313]],\n",
            "\n",
            "         [[-0.2193]],\n",
            "\n",
            "         [[ 0.5706]],\n",
            "\n",
            "         [[ 0.4611]],\n",
            "\n",
            "         [[ 0.4841]],\n",
            "\n",
            "         [[-0.3420]],\n",
            "\n",
            "         [[ 0.3908]],\n",
            "\n",
            "         [[-0.1015]],\n",
            "\n",
            "         [[ 0.1701]],\n",
            "\n",
            "         [[ 0.5467]],\n",
            "\n",
            "         [[ 0.2869]]],\n",
            "\n",
            "\n",
            "        [[[-0.5081]],\n",
            "\n",
            "         [[-0.4492]],\n",
            "\n",
            "         [[-0.5105]],\n",
            "\n",
            "         [[ 0.5592]],\n",
            "\n",
            "         [[-0.3005]],\n",
            "\n",
            "         [[-0.1940]],\n",
            "\n",
            "         [[ 0.0796]],\n",
            "\n",
            "         [[ 0.2689]],\n",
            "\n",
            "         [[ 0.4847]],\n",
            "\n",
            "         [[ 0.1217]],\n",
            "\n",
            "         [[-0.1803]],\n",
            "\n",
            "         [[-0.2166]],\n",
            "\n",
            "         [[ 0.2073]],\n",
            "\n",
            "         [[-0.4261]],\n",
            "\n",
            "         [[ 0.3615]],\n",
            "\n",
            "         [[-0.5329]]],\n",
            "\n",
            "\n",
            "        [[[-0.1648]],\n",
            "\n",
            "         [[-0.2334]],\n",
            "\n",
            "         [[ 0.2566]],\n",
            "\n",
            "         [[ 0.5592]],\n",
            "\n",
            "         [[ 0.2183]],\n",
            "\n",
            "         [[-0.3111]],\n",
            "\n",
            "         [[-0.2957]],\n",
            "\n",
            "         [[ 0.2074]],\n",
            "\n",
            "         [[-0.1599]],\n",
            "\n",
            "         [[ 0.1273]],\n",
            "\n",
            "         [[-0.4879]],\n",
            "\n",
            "         [[ 0.5239]],\n",
            "\n",
            "         [[-0.5792]],\n",
            "\n",
            "         [[-0.5739]],\n",
            "\n",
            "         [[-0.0276]],\n",
            "\n",
            "         [[-0.5951]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3931]],\n",
            "\n",
            "         [[-0.4457]],\n",
            "\n",
            "         [[-0.4667]],\n",
            "\n",
            "         [[-0.4958]],\n",
            "\n",
            "         [[ 0.5207]],\n",
            "\n",
            "         [[ 0.5272]],\n",
            "\n",
            "         [[-0.4698]],\n",
            "\n",
            "         [[ 0.1540]],\n",
            "\n",
            "         [[ 0.5375]],\n",
            "\n",
            "         [[-0.0101]],\n",
            "\n",
            "         [[-0.2930]],\n",
            "\n",
            "         [[-0.1547]],\n",
            "\n",
            "         [[-0.4281]],\n",
            "\n",
            "         [[ 0.0920]],\n",
            "\n",
            "         [[-0.5721]],\n",
            "\n",
            "         [[ 0.5176]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4137]],\n",
            "\n",
            "         [[-0.4867]],\n",
            "\n",
            "         [[ 0.4964]],\n",
            "\n",
            "         [[ 0.3733]],\n",
            "\n",
            "         [[-0.4786]],\n",
            "\n",
            "         [[ 0.2457]],\n",
            "\n",
            "         [[-0.5570]],\n",
            "\n",
            "         [[-0.1974]],\n",
            "\n",
            "         [[-0.2593]],\n",
            "\n",
            "         [[-0.2150]],\n",
            "\n",
            "         [[ 0.1890]],\n",
            "\n",
            "         [[ 0.0767]],\n",
            "\n",
            "         [[ 0.3694]],\n",
            "\n",
            "         [[-0.2610]],\n",
            "\n",
            "         [[-0.0160]],\n",
            "\n",
            "         [[-0.1783]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5505]],\n",
            "\n",
            "         [[-0.2814]],\n",
            "\n",
            "         [[ 0.0292]],\n",
            "\n",
            "         [[ 0.3275]],\n",
            "\n",
            "         [[ 0.1875]],\n",
            "\n",
            "         [[ 0.4367]],\n",
            "\n",
            "         [[ 0.2066]],\n",
            "\n",
            "         [[-0.0756]],\n",
            "\n",
            "         [[-0.3065]],\n",
            "\n",
            "         [[-0.2783]],\n",
            "\n",
            "         [[-0.4488]],\n",
            "\n",
            "         [[-0.1902]],\n",
            "\n",
            "         [[ 0.3951]],\n",
            "\n",
            "         [[-0.2942]],\n",
            "\n",
            "         [[ 0.0992]],\n",
            "\n",
            "         [[ 0.3965]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2029]],\n",
            "\n",
            "         [[-0.3720]],\n",
            "\n",
            "         [[ 0.0932]],\n",
            "\n",
            "         [[-0.2189]],\n",
            "\n",
            "         [[-0.2662]],\n",
            "\n",
            "         [[ 0.0180]],\n",
            "\n",
            "         [[ 0.0824]],\n",
            "\n",
            "         [[ 0.0982]],\n",
            "\n",
            "         [[-0.3443]],\n",
            "\n",
            "         [[-0.3642]],\n",
            "\n",
            "         [[-0.3157]],\n",
            "\n",
            "         [[ 0.2541]],\n",
            "\n",
            "         [[ 0.4190]],\n",
            "\n",
            "         [[-0.3136]],\n",
            "\n",
            "         [[ 0.2550]],\n",
            "\n",
            "         [[ 0.2188]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3274]],\n",
            "\n",
            "         [[-0.0855]],\n",
            "\n",
            "         [[ 0.0276]],\n",
            "\n",
            "         [[-0.5321]],\n",
            "\n",
            "         [[ 0.3280]],\n",
            "\n",
            "         [[-0.4818]],\n",
            "\n",
            "         [[ 0.1144]],\n",
            "\n",
            "         [[-0.5015]],\n",
            "\n",
            "         [[ 0.0685]],\n",
            "\n",
            "         [[-0.4523]],\n",
            "\n",
            "         [[ 0.1656]],\n",
            "\n",
            "         [[-0.1860]],\n",
            "\n",
            "         [[-0.5061]],\n",
            "\n",
            "         [[-0.3687]],\n",
            "\n",
            "         [[ 0.1778]],\n",
            "\n",
            "         [[ 0.1768]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0848]],\n",
            "\n",
            "         [[-0.3601]],\n",
            "\n",
            "         [[-0.3503]],\n",
            "\n",
            "         [[-0.2302]],\n",
            "\n",
            "         [[-0.2344]],\n",
            "\n",
            "         [[ 0.5927]],\n",
            "\n",
            "         [[-0.2348]],\n",
            "\n",
            "         [[ 0.1190]],\n",
            "\n",
            "         [[ 0.0866]],\n",
            "\n",
            "         [[ 0.4778]],\n",
            "\n",
            "         [[-0.6418]],\n",
            "\n",
            "         [[ 0.3735]],\n",
            "\n",
            "         [[ 0.1549]],\n",
            "\n",
            "         [[ 0.5047]],\n",
            "\n",
            "         [[ 0.4746]],\n",
            "\n",
            "         [[ 0.1449]]],\n",
            "\n",
            "\n",
            "        [[[-0.3215]],\n",
            "\n",
            "         [[ 0.5699]],\n",
            "\n",
            "         [[ 0.3206]],\n",
            "\n",
            "         [[-0.3082]],\n",
            "\n",
            "         [[-0.4449]],\n",
            "\n",
            "         [[-0.6274]],\n",
            "\n",
            "         [[-0.3793]],\n",
            "\n",
            "         [[-0.0580]],\n",
            "\n",
            "         [[-0.0120]],\n",
            "\n",
            "         [[ 0.5609]],\n",
            "\n",
            "         [[-0.2382]],\n",
            "\n",
            "         [[ 0.5884]],\n",
            "\n",
            "         [[-0.5168]],\n",
            "\n",
            "         [[ 0.5183]],\n",
            "\n",
            "         [[-0.2864]],\n",
            "\n",
            "         [[ 0.1399]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5598]],\n",
            "\n",
            "         [[ 0.1757]],\n",
            "\n",
            "         [[ 0.5853]],\n",
            "\n",
            "         [[ 0.4163]],\n",
            "\n",
            "         [[-0.4327]],\n",
            "\n",
            "         [[-0.2612]],\n",
            "\n",
            "         [[-0.2035]],\n",
            "\n",
            "         [[-0.4001]],\n",
            "\n",
            "         [[ 0.4382]],\n",
            "\n",
            "         [[ 0.2298]],\n",
            "\n",
            "         [[ 0.2582]],\n",
            "\n",
            "         [[ 0.4717]],\n",
            "\n",
            "         [[-0.2132]],\n",
            "\n",
            "         [[-0.0740]],\n",
            "\n",
            "         [[ 0.1684]],\n",
            "\n",
            "         [[-0.4675]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1991]],\n",
            "\n",
            "         [[ 0.2456]],\n",
            "\n",
            "         [[ 0.4246]],\n",
            "\n",
            "         [[ 0.2735]],\n",
            "\n",
            "         [[ 0.1497]],\n",
            "\n",
            "         [[ 0.3808]],\n",
            "\n",
            "         [[-0.1973]],\n",
            "\n",
            "         [[ 0.5226]],\n",
            "\n",
            "         [[-0.1801]],\n",
            "\n",
            "         [[-0.3379]],\n",
            "\n",
            "         [[ 0.1276]],\n",
            "\n",
            "         [[ 0.1195]],\n",
            "\n",
            "         [[-0.5582]],\n",
            "\n",
            "         [[ 0.0270]],\n",
            "\n",
            "         [[ 0.4597]],\n",
            "\n",
            "         [[ 0.5178]]],\n",
            "\n",
            "\n",
            "        [[[-0.6430]],\n",
            "\n",
            "         [[-0.5107]],\n",
            "\n",
            "         [[-0.0470]],\n",
            "\n",
            "         [[-0.2241]],\n",
            "\n",
            "         [[-0.3593]],\n",
            "\n",
            "         [[-0.1175]],\n",
            "\n",
            "         [[ 0.5103]],\n",
            "\n",
            "         [[-0.2274]],\n",
            "\n",
            "         [[ 0.5255]],\n",
            "\n",
            "         [[ 0.4275]],\n",
            "\n",
            "         [[-0.0592]],\n",
            "\n",
            "         [[-0.5257]],\n",
            "\n",
            "         [[-0.2798]],\n",
            "\n",
            "         [[-0.1505]],\n",
            "\n",
            "         [[-0.1467]],\n",
            "\n",
            "         [[-0.1345]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3836]],\n",
            "\n",
            "         [[ 0.1199]],\n",
            "\n",
            "         [[-0.4515]],\n",
            "\n",
            "         [[-0.4876]],\n",
            "\n",
            "         [[ 0.4579]],\n",
            "\n",
            "         [[ 0.2795]],\n",
            "\n",
            "         [[-0.1511]],\n",
            "\n",
            "         [[ 0.6724]],\n",
            "\n",
            "         [[ 0.1435]],\n",
            "\n",
            "         [[ 0.2314]],\n",
            "\n",
            "         [[ 0.1145]],\n",
            "\n",
            "         [[-0.1625]],\n",
            "\n",
            "         [[ 0.3053]],\n",
            "\n",
            "         [[-0.5012]],\n",
            "\n",
            "         [[-0.1105]],\n",
            "\n",
            "         [[-0.5857]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5214]],\n",
            "\n",
            "         [[-0.3565]],\n",
            "\n",
            "         [[-0.2362]],\n",
            "\n",
            "         [[ 0.0818]],\n",
            "\n",
            "         [[-0.4142]],\n",
            "\n",
            "         [[ 0.2713]],\n",
            "\n",
            "         [[ 0.1873]],\n",
            "\n",
            "         [[ 0.2072]],\n",
            "\n",
            "         [[-0.3778]],\n",
            "\n",
            "         [[ 0.3407]],\n",
            "\n",
            "         [[ 0.4629]],\n",
            "\n",
            "         [[-0.0250]],\n",
            "\n",
            "         [[ 0.0868]],\n",
            "\n",
            "         [[-0.0983]],\n",
            "\n",
            "         [[ 0.1335]],\n",
            "\n",
            "         [[ 0.1551]]],\n",
            "\n",
            "\n",
            "        [[[-0.1743]],\n",
            "\n",
            "         [[ 0.2682]],\n",
            "\n",
            "         [[-0.4670]],\n",
            "\n",
            "         [[-0.1860]],\n",
            "\n",
            "         [[ 0.4726]],\n",
            "\n",
            "         [[-0.0475]],\n",
            "\n",
            "         [[-0.3510]],\n",
            "\n",
            "         [[-0.2872]],\n",
            "\n",
            "         [[ 0.6251]],\n",
            "\n",
            "         [[-0.1962]],\n",
            "\n",
            "         [[-0.3586]],\n",
            "\n",
            "         [[-0.4020]],\n",
            "\n",
            "         [[-0.1866]],\n",
            "\n",
            "         [[ 0.1566]],\n",
            "\n",
            "         [[-0.3643]],\n",
            "\n",
            "         [[-0.1402]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3015]],\n",
            "\n",
            "         [[-0.0089]],\n",
            "\n",
            "         [[-0.1275]],\n",
            "\n",
            "         [[-0.4067]],\n",
            "\n",
            "         [[ 0.2174]],\n",
            "\n",
            "         [[ 0.1430]],\n",
            "\n",
            "         [[ 0.0105]],\n",
            "\n",
            "         [[-0.0849]],\n",
            "\n",
            "         [[ 0.4900]],\n",
            "\n",
            "         [[-0.3285]],\n",
            "\n",
            "         [[-0.5562]],\n",
            "\n",
            "         [[ 0.3233]],\n",
            "\n",
            "         [[-0.4046]],\n",
            "\n",
            "         [[-0.3612]],\n",
            "\n",
            "         [[ 0.0454]],\n",
            "\n",
            "         [[ 0.4823]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.6352,  0.0918,  0.1399, -0.0064,  0.0475,  0.7262,  0.3556,  0.2969,\n",
            "         0.4984,  0.7452,  0.2138,  0.0656,  1.0365,  0.0822,  0.3097,  0.4419,\n",
            "         0.1435,  0.3181,  0.2216,  0.5939, -0.0213,  0.3478,  0.6250,  0.1335,\n",
            "         0.0057,  0.0748,  0.3257,  0.0442,  0.0373,  0.3463,  0.4854,  0.1018],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0569, -0.0005,  0.0081,  0.0991,  0.2163,  0.0592,  0.0753,  0.1130,\n",
            "         0.1130,  0.0477,  0.1147,  0.1533,  0.0313,  0.0406, -0.0329,  0.0285,\n",
            "        -0.0218,  0.0198, -0.0489,  0.0845,  0.0575, -0.0141,  0.1133,  0.1329,\n",
            "        -0.0129,  0.0558,  0.0525,  0.0406,  0.0570, -0.0373,  0.0659, -0.0129],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-3.8213e-02, -7.6540e-02, -1.5420e-01],\n",
            "          [ 2.9335e-03, -7.1318e-02,  8.6552e-02],\n",
            "          [-1.2493e-01,  7.2492e-02, -6.0323e-02]],\n",
            "\n",
            "         [[ 1.9801e-02,  6.4624e-02, -9.2889e-02],\n",
            "          [-1.3537e-02,  3.6835e-02, -9.6017e-02],\n",
            "          [ 7.4147e-02, -9.9307e-02,  1.7188e-02]],\n",
            "\n",
            "         [[ 5.1904e-02, -1.2734e-01, -8.8793e-02],\n",
            "          [-1.2818e-01, -8.4978e-02,  1.3016e-02],\n",
            "          [-1.4761e-01, -3.0683e-02,  1.8730e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.9931e-03, -4.1511e-03,  2.3527e-02],\n",
            "          [-2.0600e-02,  1.4084e-01, -3.7167e-02],\n",
            "          [ 1.1311e-01,  1.0129e-01, -3.1798e-02]],\n",
            "\n",
            "         [[ 5.4513e-02, -1.2847e-01,  1.4086e-01],\n",
            "          [-1.3072e-01,  1.2557e-01,  6.7955e-02],\n",
            "          [-7.4921e-02,  5.1787e-02,  8.6671e-02]],\n",
            "\n",
            "         [[ 1.0778e-01,  6.3389e-02,  3.8515e-02],\n",
            "          [-4.4707e-02,  4.5428e-03, -4.2251e-03],\n",
            "          [-6.9637e-02, -1.2745e-01, -2.9707e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.3998e-02, -3.0246e-02,  3.5391e-02],\n",
            "          [-5.3463e-02,  8.5025e-02,  1.2888e-03],\n",
            "          [ 7.4684e-02,  1.4340e-02, -8.8829e-03]],\n",
            "\n",
            "         [[-3.8696e-02,  9.5748e-02, -7.5047e-02],\n",
            "          [-7.7823e-02, -9.6898e-02,  8.9423e-02],\n",
            "          [-1.0274e-01, -7.7296e-02, -9.5866e-02]],\n",
            "\n",
            "         [[ 3.3764e-02, -1.0817e-02,  1.4197e-01],\n",
            "          [ 1.2263e-01,  7.5826e-02,  1.1294e-01],\n",
            "          [ 1.1549e-01, -4.6164e-02,  1.1590e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.9738e-02,  9.3284e-02, -7.7949e-02],\n",
            "          [ 4.1922e-02, -1.0399e-02,  1.0231e-01],\n",
            "          [ 1.0846e-01,  5.9371e-02, -5.0680e-02]],\n",
            "\n",
            "         [[-2.9832e-02, -3.6435e-02, -1.0280e-01],\n",
            "          [-1.3653e-01, -3.6518e-02, -1.0138e-01],\n",
            "          [-1.0324e-01,  1.0382e-01,  1.2112e-01]],\n",
            "\n",
            "         [[ 3.2244e-02, -9.5415e-02, -5.8269e-02],\n",
            "          [ 2.9185e-02,  3.1292e-02,  8.6719e-02],\n",
            "          [ 5.6307e-02,  3.5388e-02, -6.6272e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 8.4971e-02,  1.6191e-02, -9.5378e-03],\n",
            "          [ 3.7637e-02,  8.4535e-02,  2.2132e-02],\n",
            "          [-1.5636e-02,  9.0379e-02,  1.2653e-01]],\n",
            "\n",
            "         [[-2.7013e-02, -1.0494e-01, -2.3186e-02],\n",
            "          [-2.4956e-02, -1.1903e-01, -4.0563e-02],\n",
            "          [ 7.2586e-02,  1.0722e-01,  6.6195e-02]],\n",
            "\n",
            "         [[-1.4388e-01,  5.5677e-02, -2.7099e-02],\n",
            "          [ 1.2914e-01, -1.3904e-01,  9.3863e-02],\n",
            "          [-6.5411e-02, -5.2258e-02,  1.2274e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1716e-01,  5.6370e-02,  2.8096e-02],\n",
            "          [-7.1682e-02, -1.2087e-01,  5.7923e-02],\n",
            "          [-1.1363e-01, -5.0799e-03,  8.9174e-02]],\n",
            "\n",
            "         [[ 7.4644e-02, -5.9055e-02, -1.0159e-02],\n",
            "          [ 1.0495e-01,  9.1324e-02,  7.6507e-02],\n",
            "          [-1.1875e-01, -8.4780e-02, -3.9139e-02]],\n",
            "\n",
            "         [[ 1.0849e-02,  2.3880e-01,  2.1471e-01],\n",
            "          [ 1.5944e-01,  1.7170e-01,  2.0269e-02],\n",
            "          [ 5.7410e-02, -4.0055e-02,  6.6475e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-8.2637e-02,  1.0071e-01, -7.9182e-02],\n",
            "          [-4.6414e-02, -4.7842e-02,  7.8728e-02],\n",
            "          [ 1.4267e-01, -1.2923e-01,  4.0560e-02]],\n",
            "\n",
            "         [[ 5.9516e-03,  1.2003e-01, -9.3798e-03],\n",
            "          [ 2.0983e-02,  8.9393e-02, -2.4317e-02],\n",
            "          [-2.6498e-02,  8.2592e-02, -3.7196e-02]],\n",
            "\n",
            "         [[-7.0991e-02, -4.9537e-02, -5.9723e-02],\n",
            "          [-9.4590e-02,  9.0081e-02, -3.4246e-02],\n",
            "          [-3.5564e-02,  1.1423e-01,  4.4569e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0934e-01,  1.3209e-01, -6.0839e-02],\n",
            "          [ 3.6462e-02, -1.1533e-01, -9.8967e-02],\n",
            "          [-3.7150e-02, -2.6945e-02,  8.1574e-02]],\n",
            "\n",
            "         [[ 1.3576e-01, -9.6458e-02,  3.3391e-02],\n",
            "          [ 8.2509e-02,  6.9561e-02, -1.6313e-01],\n",
            "          [-4.3031e-02,  1.0077e-02, -1.3687e-01]],\n",
            "\n",
            "         [[-1.6811e-02,  6.2530e-02, -4.7049e-02],\n",
            "          [-1.4962e-01, -5.0753e-02,  4.7702e-02],\n",
            "          [-1.7010e-02,  1.5039e-01,  9.6686e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9905e-02, -1.2970e-01,  1.0614e-02],\n",
            "          [ 1.8623e-02,  1.0484e-02, -1.7634e-01],\n",
            "          [-3.3505e-02, -2.0442e-01,  4.1052e-02]],\n",
            "\n",
            "         [[-1.3539e-01,  9.3720e-02,  1.1235e-01],\n",
            "          [ 5.5479e-02, -1.2181e-01,  6.2558e-02],\n",
            "          [ 1.2888e-04, -6.1177e-02,  5.7420e-02]],\n",
            "\n",
            "         [[ 7.8963e-02,  2.2356e-02, -6.3733e-02],\n",
            "          [ 1.4582e-01, -1.1153e-01,  1.2942e-01],\n",
            "          [-6.4524e-03, -5.7531e-02, -1.8649e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.3886e-02, -9.5397e-02, -1.2125e-01],\n",
            "          [-5.6938e-03,  3.2385e-02,  7.4993e-02],\n",
            "          [-8.3660e-02,  9.4940e-02,  1.9680e-02]],\n",
            "\n",
            "         [[-1.6737e-01, -1.2215e-01,  3.8062e-02],\n",
            "          [ 4.4816e-02,  7.7051e-02,  7.8271e-02],\n",
            "          [ 2.5742e-02, -3.0982e-02,  3.7449e-02]],\n",
            "\n",
            "         [[-9.9761e-02, -6.9145e-02, -9.8766e-02],\n",
            "          [-1.8766e-01, -8.5611e-02, -1.6403e-01],\n",
            "          [-1.3395e-01, -5.8068e-02, -2.4430e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0091e-01, -1.2324e-01, -8.5992e-02],\n",
            "          [-5.6066e-02, -8.9308e-02,  8.9805e-02],\n",
            "          [ 1.2435e-01,  9.7439e-02, -1.2697e-01]],\n",
            "\n",
            "         [[ 1.1249e-01,  3.7602e-02, -8.8526e-02],\n",
            "          [-4.5744e-02,  4.4559e-02, -6.4483e-03],\n",
            "          [ 9.7023e-02,  1.1711e-01,  8.1436e-02]],\n",
            "\n",
            "         [[-6.4676e-02,  9.6777e-02,  1.0943e-01],\n",
            "          [-7.8115e-02, -6.2342e-04,  9.3578e-02],\n",
            "          [ 3.2258e-02, -3.9004e-02, -7.3986e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.7429e-02, -1.0271e-02,  1.3233e-01],\n",
            "          [ 1.1958e-01,  1.0672e-01,  2.9484e-03],\n",
            "          [-1.0061e-01,  4.2498e-02,  1.9482e-02]],\n",
            "\n",
            "         [[ 1.6731e-02,  1.4517e-02, -9.5667e-02],\n",
            "          [-3.4341e-02,  4.8449e-03,  5.3023e-02],\n",
            "          [-2.3578e-02, -1.1513e-01, -7.1825e-03]],\n",
            "\n",
            "         [[ 6.3892e-03, -6.0674e-02, -6.2610e-02],\n",
            "          [ 4.5241e-02, -7.6830e-02, -9.8776e-02],\n",
            "          [ 2.0297e-02, -1.6966e-01, -4.7036e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.5329, 0.9046, 0.8742, 0.8379, 0.2263, 0.8151, 0.6598, 0.5130, 0.1603,\n",
            "        0.6416, 0.5202, 0.4382, 0.7136, 0.5663, 0.1008, 0.4379, 0.8512, 0.9725,\n",
            "        0.3406, 0.8599, 0.0261, 0.2998, 0.3108, 0.9079, 0.1595, 0.5488, 0.7904,\n",
            "        0.1022, 0.3970, 0.6314, 0.9672, 0.5394], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0694,  0.0024,  0.0178,  0.0353, -0.0155, -0.0257,  0.0369,  0.0420,\n",
            "        -0.0102,  0.1045, -0.0057,  0.0374,  0.0349,  0.0256,  0.0415, -0.0050,\n",
            "         0.0393, -0.0109,  0.0105, -0.0088,  0.0146, -0.0192, -0.0132,  0.1372,\n",
            "         0.0054,  0.0402,  0.0220,  0.0170,  0.0567, -0.0329,  0.1329, -0.0153],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-4.0569e-02, -4.4268e-02, -3.7462e-02],\n",
            "          [-9.9389e-02,  5.3164e-02,  3.4908e-02],\n",
            "          [ 5.7713e-02, -4.1976e-03, -1.6439e-02]],\n",
            "\n",
            "         [[ 2.6275e-02, -1.1323e-01, -7.3482e-03],\n",
            "          [-1.5137e-01, -3.6931e-04,  4.5112e-03],\n",
            "          [ 4.9216e-02,  3.1273e-03,  5.9363e-02]],\n",
            "\n",
            "         [[-5.4647e-03, -8.9248e-02, -3.0933e-02],\n",
            "          [-2.3661e-02,  7.8934e-02, -1.1292e-01],\n",
            "          [-1.2178e-01,  4.0798e-02,  9.8485e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.7702e-02,  7.1037e-02,  1.5901e-01],\n",
            "          [ 5.0525e-02, -2.9918e-02, -8.4963e-02],\n",
            "          [-3.5910e-02,  1.3302e-01, -2.3655e-02]],\n",
            "\n",
            "         [[-1.6681e-01, -5.5364e-03, -4.3083e-02],\n",
            "          [ 1.1528e-01, -1.0098e-01,  1.5244e-01],\n",
            "          [ 1.2166e-01, -1.7094e-02, -2.6293e-02]],\n",
            "\n",
            "         [[ 6.3094e-02,  5.5346e-02, -7.4435e-02],\n",
            "          [ 1.3541e-02, -7.0862e-02, -1.1467e-01],\n",
            "          [ 7.7762e-02,  1.1500e-01, -2.0655e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1949e-01,  1.2358e-01,  2.0956e-02],\n",
            "          [ 4.9556e-02,  4.6395e-02,  2.9056e-02],\n",
            "          [-3.8610e-02, -4.5116e-02,  7.4836e-03]],\n",
            "\n",
            "         [[ 5.9971e-02,  7.8443e-02, -1.0258e-01],\n",
            "          [-1.1875e-01, -1.2911e-01, -6.5527e-02],\n",
            "          [-1.1263e-01, -1.6952e-01, -4.1747e-02]],\n",
            "\n",
            "         [[ 1.5936e-03, -3.6743e-02, -1.6641e-02],\n",
            "          [ 1.1424e-01,  1.2807e-01, -2.5821e-02],\n",
            "          [-2.1199e-02, -8.7500e-02, -3.5061e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2367e-01,  1.0336e-01, -1.2069e-01],\n",
            "          [ 2.3733e-02, -5.9666e-02, -7.8940e-02],\n",
            "          [ 5.1815e-02,  1.1265e-01,  1.3941e-01]],\n",
            "\n",
            "         [[-5.7955e-02,  6.4399e-02,  5.6426e-02],\n",
            "          [-1.4383e-01, -3.5478e-02, -1.3815e-01],\n",
            "          [ 1.1502e-01,  9.4669e-02,  7.5179e-02]],\n",
            "\n",
            "         [[ 1.8718e-02,  2.7831e-05,  1.1596e-01],\n",
            "          [ 2.9829e-02, -1.3410e-01, -1.3777e-01],\n",
            "          [ 7.1735e-02,  1.0474e-02,  1.3602e-01]]],\n",
            "\n",
            "\n",
            "        [[[-8.9470e-02,  9.2069e-02,  2.9817e-02],\n",
            "          [ 1.0376e-01,  1.4219e-02, -1.0763e-01],\n",
            "          [-1.3864e-02,  2.2079e-02, -2.4238e-02]],\n",
            "\n",
            "         [[-1.2950e-01, -3.8061e-02,  4.8557e-02],\n",
            "          [-1.4775e-01, -4.6009e-03, -6.1489e-02],\n",
            "          [-1.0262e-01, -2.1631e-02, -7.3145e-02]],\n",
            "\n",
            "         [[-1.2507e-02, -6.9952e-02, -3.8305e-02],\n",
            "          [ 4.8669e-02, -1.2377e-01, -2.3920e-02],\n",
            "          [ 6.4244e-02,  9.3020e-02, -1.4970e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.8534e-02, -2.3011e-02,  8.1359e-02],\n",
            "          [-1.0604e-01, -8.8907e-02, -1.1521e-01],\n",
            "          [ 1.2587e-01, -1.4173e-02, -1.2371e-01]],\n",
            "\n",
            "         [[-6.5376e-02, -9.7252e-02,  1.3545e-01],\n",
            "          [ 1.3074e-01,  1.0194e-01, -1.1386e-01],\n",
            "          [-8.3296e-02,  1.5160e-01,  1.5298e-01]],\n",
            "\n",
            "         [[-3.0622e-02, -9.9185e-02,  3.3404e-02],\n",
            "          [-2.0580e-02, -1.3667e-01, -6.6632e-02],\n",
            "          [-7.4109e-02, -9.9269e-02,  6.8267e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.4261e-02,  1.4058e-01, -3.3253e-02],\n",
            "          [-1.3489e-01, -4.0783e-02,  8.2051e-02],\n",
            "          [-4.7909e-02,  1.0521e-03,  1.1578e-02]],\n",
            "\n",
            "         [[ 6.6622e-03, -6.1183e-02, -1.2283e-01],\n",
            "          [-1.1807e-01, -8.9841e-02,  4.7603e-02],\n",
            "          [-1.1647e-01,  2.0903e-02, -1.1604e-01]],\n",
            "\n",
            "         [[ 9.8237e-02, -1.1103e-01, -1.3432e-01],\n",
            "          [ 3.1267e-02,  1.2981e-02, -9.9810e-02],\n",
            "          [ 9.4065e-02,  4.5936e-02, -6.1614e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.4644e-02, -5.6667e-02,  1.2908e-01],\n",
            "          [ 7.1078e-02,  5.5293e-02,  4.1905e-03],\n",
            "          [ 9.2650e-02,  1.0466e-01,  8.8195e-02]],\n",
            "\n",
            "         [[-2.1858e-02, -1.2089e-01,  4.4487e-03],\n",
            "          [-2.5752e-02, -2.1683e-02, -1.5668e-03],\n",
            "          [-5.3297e-02,  1.7841e-02,  7.6890e-02]],\n",
            "\n",
            "         [[ 1.0918e-01,  1.0024e-01, -3.1364e-04],\n",
            "          [ 6.1939e-02, -1.3590e-01, -4.0558e-02],\n",
            "          [-1.0143e-01, -1.1474e-01, -1.4718e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 6.7485e-02, -1.1773e-01, -1.4458e-01],\n",
            "          [ 1.1021e-01, -1.1704e-01,  3.9172e-02],\n",
            "          [-1.8763e-03,  5.6014e-02,  7.8000e-02]],\n",
            "\n",
            "         [[-3.3405e-02,  6.5180e-02,  3.5575e-03],\n",
            "          [-2.4620e-03,  1.5228e-01,  1.2652e-01],\n",
            "          [ 9.3228e-02,  5.6512e-02,  1.5524e-01]],\n",
            "\n",
            "         [[ 9.5322e-02,  5.0887e-02,  4.1669e-02],\n",
            "          [-1.1634e-01, -1.3053e-01, -9.8478e-02],\n",
            "          [-1.3960e-01, -3.9644e-02, -9.1115e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.5852e-02,  3.8050e-02, -4.6713e-03],\n",
            "          [-3.3186e-03,  4.0299e-02, -7.1446e-02],\n",
            "          [ 5.5987e-02, -4.2730e-02,  3.5756e-02]],\n",
            "\n",
            "         [[-1.4823e-01, -8.1336e-02, -1.1444e-01],\n",
            "          [-1.5433e-01, -1.1236e-01, -8.1244e-02],\n",
            "          [ 3.4387e-02,  6.6586e-02, -6.3242e-02]],\n",
            "\n",
            "         [[-1.0429e-01,  8.6030e-02,  1.5196e-01],\n",
            "          [ 6.2143e-02,  5.4647e-03,  4.5922e-02],\n",
            "          [ 4.8259e-02,  3.9104e-02, -6.2901e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.8473e-02,  2.2392e-02,  2.8618e-02],\n",
            "          [-4.8713e-02,  8.9368e-02,  2.6880e-02],\n",
            "          [-1.5291e-02, -2.6812e-02, -1.1529e-01]],\n",
            "\n",
            "         [[ 9.6403e-02, -1.4745e-01, -1.4528e-01],\n",
            "          [-9.4208e-02, -5.9359e-04,  1.0285e-01],\n",
            "          [-1.1700e-01,  1.2367e-01,  5.4953e-02]],\n",
            "\n",
            "         [[-1.8957e-03,  4.7923e-02,  2.9419e-02],\n",
            "          [ 1.2845e-02, -3.4700e-02, -9.5918e-03],\n",
            "          [-9.5795e-02, -3.2094e-02,  5.3404e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.7173e-02,  1.1926e-01, -7.6206e-02],\n",
            "          [ 1.3342e-01,  1.1977e-01, -9.9444e-02],\n",
            "          [ 1.4403e-01, -8.8019e-02,  1.3313e-01]],\n",
            "\n",
            "         [[-1.9944e-02, -1.7896e-01, -4.4684e-03],\n",
            "          [ 8.6243e-02, -1.2281e-01, -5.8031e-02],\n",
            "          [-1.3616e-01, -2.6587e-02, -1.0315e-02]],\n",
            "\n",
            "         [[ 2.0210e-02,  9.1510e-02,  4.2787e-02],\n",
            "          [ 1.2278e-01, -8.7856e-02, -3.3120e-02],\n",
            "          [-1.1644e-02,  1.3411e-02, -1.0958e-01]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.6471,  0.2132,  0.5819,  0.5644,  0.7064,  0.3216,  0.9447,  0.0668,\n",
            "         0.0500,  0.3960,  0.8440,  0.1691,  0.4114,  0.4023,  0.0868,  0.7947,\n",
            "         0.3165,  0.4043,  0.3741,  0.2256,  0.6119,  0.9388,  0.0272,  0.3211,\n",
            "         0.3086,  0.6092,  0.6851,  0.3736, -0.1898,  0.1960,  0.8534,  0.8746],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0554,  0.0294,  0.0222,  0.0877,  0.1386, -0.0025,  0.1417,  0.0191,\n",
            "         0.0118,  0.0631,  0.0538,  0.0435,  0.0860,  0.0078,  0.0249,  0.1403,\n",
            "        -0.0105,  0.0343,  0.0333, -0.0134,  0.0702,  0.0323, -0.0138,  0.0969,\n",
            "         0.0782,  0.0413,  0.0232,  0.0954,  0.0848,  0.0522,  0.0903,  0.0036],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 8.3918e-02,  1.1764e-01,  1.1815e-01],\n",
            "          [-1.3601e-01, -1.3374e-01,  5.9850e-02],\n",
            "          [ 3.7721e-02, -4.2728e-02,  6.3344e-02]],\n",
            "\n",
            "         [[-4.4847e-03,  1.4318e-01,  1.1677e-01],\n",
            "          [-1.3349e-02, -1.2263e-01,  5.8292e-02],\n",
            "          [ 3.6771e-02, -6.5185e-02, -1.1597e-01]],\n",
            "\n",
            "         [[ 1.3450e-01, -1.4732e-01,  1.2327e-01],\n",
            "          [-1.0089e-01, -1.3802e-01,  1.3416e-01],\n",
            "          [-9.6480e-02, -6.2596e-02,  1.0032e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.8119e-02, -1.1966e-01,  1.2804e-01],\n",
            "          [-1.0067e-01, -3.9796e-02,  4.3161e-03],\n",
            "          [-4.3678e-02,  4.8209e-02, -5.9195e-02]],\n",
            "\n",
            "         [[-4.5554e-02,  1.1544e-01, -1.2218e-01],\n",
            "          [-5.5061e-02,  3.5865e-02,  1.2834e-03],\n",
            "          [-1.1024e-01,  1.4207e-01, -8.3665e-02]],\n",
            "\n",
            "         [[-4.1794e-02, -1.2985e-01, -4.7323e-02],\n",
            "          [-1.5354e-01,  3.7387e-02, -6.6957e-02],\n",
            "          [-1.0967e-03,  3.4794e-02,  6.2844e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6654e-01, -6.6546e-02,  6.5907e-02],\n",
            "          [ 4.4880e-02, -7.1819e-02, -5.6600e-02],\n",
            "          [-1.2718e-01, -1.3967e-01,  6.8786e-03]],\n",
            "\n",
            "         [[ 1.8266e-02,  4.6959e-02, -1.2810e-01],\n",
            "          [ 6.4507e-02, -3.1072e-02, -7.1845e-02],\n",
            "          [-1.4095e-01, -1.1177e-03, -1.1066e-02]],\n",
            "\n",
            "         [[-1.0547e-02,  9.3224e-02,  1.3252e-01],\n",
            "          [-1.9367e-02, -1.2392e-01, -9.3359e-02],\n",
            "          [ 1.0470e-01, -3.7153e-02, -1.3988e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3578e-01, -2.2654e-02, -4.2908e-02],\n",
            "          [ 1.0739e-01, -2.9302e-02, -2.8070e-02],\n",
            "          [ 1.2005e-01,  1.3371e-01, -3.4430e-03]],\n",
            "\n",
            "         [[ 3.4194e-02,  1.3014e-01, -1.1322e-01],\n",
            "          [ 8.8768e-02, -6.5281e-02,  9.3000e-02],\n",
            "          [ 8.9822e-02,  1.2887e-01, -2.6935e-02]],\n",
            "\n",
            "         [[ 1.5214e-01,  2.4797e-02,  1.0131e-01],\n",
            "          [-4.3269e-02,  6.9425e-02, -1.2310e-01],\n",
            "          [-6.2019e-02,  1.0891e-01, -1.2525e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 5.3997e-02, -4.2557e-02, -1.3277e-01],\n",
            "          [-8.2414e-02,  3.5255e-02,  7.6702e-02],\n",
            "          [ 8.5252e-02,  5.2564e-02,  1.1465e-01]],\n",
            "\n",
            "         [[-1.1905e-03, -5.8074e-02, -1.1500e-01],\n",
            "          [-4.6580e-02, -1.1524e-01, -4.6165e-03],\n",
            "          [ 5.2120e-02,  1.2400e-01, -1.0152e-02]],\n",
            "\n",
            "         [[ 2.0884e-02,  1.2072e-01,  1.0301e-01],\n",
            "          [ 1.0743e-01,  1.2837e-01, -7.7652e-02],\n",
            "          [ 6.0191e-02,  1.3079e-01,  1.1289e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.9472e-06,  3.5390e-02, -2.0474e-02],\n",
            "          [ 1.1015e-01, -7.8723e-02, -8.5527e-02],\n",
            "          [ 1.4322e-01, -1.0283e-01, -6.0451e-02]],\n",
            "\n",
            "         [[-5.8100e-02,  5.1311e-02, -4.3698e-02],\n",
            "          [ 1.1481e-01, -8.6214e-02, -4.6485e-02],\n",
            "          [-2.9389e-02, -1.1739e-01, -1.4823e-01]],\n",
            "\n",
            "         [[ 7.1192e-02, -6.5239e-02,  4.9477e-02],\n",
            "          [-2.4923e-02,  4.5261e-02, -4.5932e-02],\n",
            "          [-3.6986e-02, -8.0962e-02, -1.1169e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.8319e-03, -9.3580e-02, -2.3141e-02],\n",
            "          [-1.4164e-01,  7.1596e-02, -9.1702e-02],\n",
            "          [-5.6512e-02,  8.1613e-02,  7.4789e-02]],\n",
            "\n",
            "         [[ 9.3753e-02, -8.8491e-02,  8.5343e-03],\n",
            "          [-5.3244e-02, -3.5337e-02,  3.6307e-03],\n",
            "          [-7.7970e-02, -9.8420e-02,  4.1354e-02]],\n",
            "\n",
            "         [[-6.7972e-02, -1.4834e-01, -1.8059e-03],\n",
            "          [ 9.6741e-02, -1.5321e-03,  5.1422e-02],\n",
            "          [ 9.6146e-02,  1.0766e-01,  7.7256e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9468e-02, -7.3928e-03,  7.0680e-02],\n",
            "          [-1.2188e-01, -1.1068e-01,  1.8470e-02],\n",
            "          [-7.2526e-02, -1.3177e-01, -8.2510e-02]],\n",
            "\n",
            "         [[-1.1645e-01,  3.9713e-02, -3.3572e-02],\n",
            "          [-1.1828e-01, -6.7491e-02, -1.4019e-01],\n",
            "          [-1.4564e-01,  1.3197e-02, -1.1806e-01]],\n",
            "\n",
            "         [[ 5.2110e-02, -6.1983e-02, -1.7553e-02],\n",
            "          [-4.9289e-02, -1.5989e-01, -3.5020e-02],\n",
            "          [ 7.5996e-02,  4.3726e-02,  7.9164e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1106e-01, -9.5105e-02, -4.9662e-02],\n",
            "          [ 8.5144e-02, -1.0598e-01, -1.0925e-01],\n",
            "          [-2.5186e-02, -6.7408e-02,  6.4041e-02]],\n",
            "\n",
            "         [[ 1.0484e-01,  7.6543e-02,  2.7235e-02],\n",
            "          [-2.3412e-02, -1.3404e-01, -1.3818e-02],\n",
            "          [-6.6828e-02,  7.2643e-03,  1.4600e-01]],\n",
            "\n",
            "         [[ 3.5400e-02, -4.4158e-02,  1.1289e-02],\n",
            "          [ 1.2419e-01,  1.4571e-02,  7.3272e-02],\n",
            "          [ 1.1297e-01,  1.1534e-01, -5.8797e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.0996e-02,  1.6933e-02, -7.9623e-02],\n",
            "          [ 9.9865e-02, -1.1701e-01,  9.5675e-02],\n",
            "          [ 1.2318e-01, -1.0601e-01,  5.0285e-02]],\n",
            "\n",
            "         [[-4.2604e-02, -1.3312e-01,  4.2154e-02],\n",
            "          [-3.3827e-02,  3.6161e-02, -4.4291e-02],\n",
            "          [ 6.9818e-02,  6.9694e-02,  1.2470e-02]],\n",
            "\n",
            "         [[-9.4733e-02,  1.2103e-01,  1.3354e-01],\n",
            "          [-1.2769e-01, -1.0098e-01,  9.2500e-02],\n",
            "          [-5.6653e-02, -1.4012e-02,  5.7063e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2450e-01,  3.2646e-02, -8.8189e-02],\n",
            "          [ 3.8778e-02,  4.6789e-02,  5.0903e-02],\n",
            "          [ 7.6785e-02, -9.0483e-02,  1.9884e-02]],\n",
            "\n",
            "         [[-1.2255e-01,  8.8703e-02, -1.4055e-01],\n",
            "          [-7.6355e-03, -1.0941e-01,  1.0438e-01],\n",
            "          [-9.6116e-02,  8.2802e-02,  3.7758e-02]],\n",
            "\n",
            "         [[ 1.1528e-01,  6.6470e-02, -1.2305e-01],\n",
            "          [-1.3719e-01,  6.8646e-02, -2.7975e-03],\n",
            "          [ 1.1502e-01,  4.6841e-02,  9.4021e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.1055e-02, -1.2194e-01, -4.6985e-02],\n",
            "          [ 1.1728e-01,  6.8050e-02, -6.7136e-02],\n",
            "          [-1.2978e-01,  9.6975e-02,  6.8576e-02]],\n",
            "\n",
            "         [[-5.2287e-02,  7.4563e-03,  5.3215e-02],\n",
            "          [ 6.3521e-02, -1.6882e-01, -6.8694e-03],\n",
            "          [-5.8300e-02, -1.6952e-02, -1.1226e-01]],\n",
            "\n",
            "         [[ 1.1914e-01,  1.0975e-01,  7.9929e-02],\n",
            "          [-5.7249e-02,  1.3062e-01,  2.3780e-03],\n",
            "          [ 6.2060e-02,  1.6940e-02,  2.0009e-01]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.0183, 0.4664, 0.3670, 0.1090, 0.7508, 0.5572, 0.8795, 0.9662, 0.6121,\n",
            "        0.4169, 0.7414, 0.2644, 0.0048, 0.9820, 0.0028, 0.2563, 0.0124, 0.2729,\n",
            "        0.0309, 0.6685, 0.4898, 0.6874, 0.6993, 0.6517, 0.4333, 0.7896, 0.4354,\n",
            "        0.8381, 0.0532, 0.6572, 0.4897, 0.7003], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0295,  0.0728,  0.1308,  0.0142,  0.0065,  0.0238,  0.0342,  0.0856,\n",
            "         0.1077, -0.0324,  0.0006,  0.0132, -0.0118,  0.0579, -0.0110,  0.0105,\n",
            "         0.0073,  0.0200,  0.0127,  0.0611, -0.0051,  0.0328, -0.0064,  0.0387,\n",
            "         0.0721, -0.0082, -0.0460, -0.0076, -0.0064,  0.0003,  0.0065,  0.0399],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0831, -0.0997, -0.1105],\n",
            "          [ 0.0365,  0.1395,  0.1113],\n",
            "          [ 0.1283,  0.1039,  0.0365]],\n",
            "\n",
            "         [[-0.1124, -0.0179, -0.0474],\n",
            "          [ 0.0155, -0.0247, -0.1268],\n",
            "          [-0.0684, -0.0429, -0.0038]],\n",
            "\n",
            "         [[ 0.0759, -0.0650,  0.0481],\n",
            "          [-0.0997, -0.0874,  0.1052],\n",
            "          [-0.0266,  0.0356, -0.0617]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0363, -0.0471,  0.0560],\n",
            "          [ 0.0978,  0.1513,  0.1235],\n",
            "          [ 0.0854,  0.0354,  0.1339]],\n",
            "\n",
            "         [[ 0.0387,  0.0598, -0.0443],\n",
            "          [ 0.1355,  0.0240,  0.0454],\n",
            "          [ 0.1464,  0.1114,  0.1184]],\n",
            "\n",
            "         [[-0.0612,  0.0852, -0.0056],\n",
            "          [-0.0363, -0.0889,  0.0481],\n",
            "          [ 0.0958,  0.0104,  0.0378]]],\n",
            "\n",
            "\n",
            "        [[[-0.0195,  0.0302,  0.0446],\n",
            "          [ 0.0710,  0.1197,  0.1294],\n",
            "          [-0.0292, -0.0376,  0.1364]],\n",
            "\n",
            "         [[ 0.1046, -0.0317,  0.0226],\n",
            "          [ 0.1034, -0.0168, -0.0029],\n",
            "          [ 0.0258,  0.0935,  0.0920]],\n",
            "\n",
            "         [[ 0.0524, -0.0923, -0.0025],\n",
            "          [-0.0875,  0.0736, -0.1062],\n",
            "          [ 0.1185,  0.1314, -0.1229]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0735,  0.0362, -0.0607],\n",
            "          [ 0.1155,  0.1112, -0.0228],\n",
            "          [-0.0921, -0.1439, -0.0309]],\n",
            "\n",
            "         [[-0.0948, -0.0824, -0.0802],\n",
            "          [-0.0350, -0.1256, -0.0272],\n",
            "          [-0.0261,  0.0180,  0.0738]],\n",
            "\n",
            "         [[ 0.0215,  0.1429, -0.0584],\n",
            "          [-0.0176,  0.0758, -0.0641],\n",
            "          [ 0.0900, -0.0154, -0.0253]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0583, -0.1251, -0.0120],\n",
            "          [ 0.0684, -0.0379, -0.0507],\n",
            "          [ 0.0784,  0.0964, -0.0446]],\n",
            "\n",
            "         [[ 0.0597, -0.0461,  0.1011],\n",
            "          [-0.0125, -0.1141, -0.0847],\n",
            "          [ 0.0810,  0.0557,  0.0702]],\n",
            "\n",
            "         [[ 0.0727,  0.1026,  0.0999],\n",
            "          [ 0.1489,  0.0149,  0.1422],\n",
            "          [ 0.0542,  0.0210, -0.0245]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0137, -0.1143, -0.0778],\n",
            "          [-0.0964, -0.0115, -0.0885],\n",
            "          [ 0.0777,  0.0986, -0.0213]],\n",
            "\n",
            "         [[ 0.1006, -0.0268,  0.0537],\n",
            "          [-0.0859, -0.0356, -0.0273],\n",
            "          [ 0.1190, -0.0066,  0.0995]],\n",
            "\n",
            "         [[ 0.1297, -0.0280,  0.0885],\n",
            "          [ 0.0939, -0.0305,  0.0270],\n",
            "          [ 0.0988,  0.0261,  0.0594]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0886, -0.0156,  0.1395],\n",
            "          [ 0.1349, -0.1165,  0.0740],\n",
            "          [-0.0479,  0.1418, -0.0262]],\n",
            "\n",
            "         [[-0.0737,  0.1259,  0.1015],\n",
            "          [ 0.1213,  0.0519, -0.1081],\n",
            "          [-0.1486,  0.1179, -0.1147]],\n",
            "\n",
            "         [[-0.1055, -0.0536,  0.0583],\n",
            "          [-0.1281,  0.1276,  0.1080],\n",
            "          [-0.0575, -0.0126,  0.0918]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1284, -0.0071, -0.0943],\n",
            "          [ 0.1051,  0.0552, -0.1384],\n",
            "          [ 0.0442, -0.0213, -0.0296]],\n",
            "\n",
            "         [[ 0.0334, -0.0210, -0.0721],\n",
            "          [-0.1041,  0.0615, -0.0137],\n",
            "          [ 0.0312,  0.1168,  0.0033]],\n",
            "\n",
            "         [[ 0.1339, -0.0564, -0.0358],\n",
            "          [ 0.0725, -0.0847, -0.0227],\n",
            "          [ 0.0425,  0.1003,  0.0153]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0877,  0.1234,  0.0181],\n",
            "          [ 0.1031, -0.0822, -0.1266],\n",
            "          [-0.1399,  0.1458,  0.1172]],\n",
            "\n",
            "         [[ 0.1154,  0.0143,  0.0373],\n",
            "          [ 0.0835, -0.0407,  0.0862],\n",
            "          [-0.0749, -0.0368,  0.1001]],\n",
            "\n",
            "         [[-0.1144, -0.0054,  0.0660],\n",
            "          [ 0.0188, -0.0316,  0.0939],\n",
            "          [ 0.1385, -0.0467,  0.1466]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0244,  0.1152,  0.0281],\n",
            "          [ 0.1126,  0.1105, -0.1125],\n",
            "          [ 0.0722, -0.0152, -0.0514]],\n",
            "\n",
            "         [[ 0.0995,  0.0133, -0.0520],\n",
            "          [ 0.0554, -0.0195,  0.1372],\n",
            "          [-0.0502,  0.1611, -0.0729]],\n",
            "\n",
            "         [[ 0.0029, -0.0553,  0.0978],\n",
            "          [ 0.1249, -0.0212,  0.1007],\n",
            "          [ 0.1538,  0.1055, -0.0723]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0142, -0.1079, -0.0438],\n",
            "          [-0.0911,  0.0139,  0.1082],\n",
            "          [-0.0499,  0.1118, -0.0391]],\n",
            "\n",
            "         [[-0.0603,  0.1728,  0.0443],\n",
            "          [-0.0780,  0.0731,  0.0811],\n",
            "          [ 0.0338,  0.0277, -0.0438]],\n",
            "\n",
            "         [[ 0.0966,  0.0608,  0.1336],\n",
            "          [ 0.1229,  0.1349, -0.0057],\n",
            "          [-0.1000,  0.0112,  0.0310]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0677,  0.1250, -0.0601],\n",
            "          [-0.0881, -0.0168,  0.1347],\n",
            "          [-0.1108, -0.1101, -0.0543]],\n",
            "\n",
            "         [[ 0.0202, -0.0026,  0.0427],\n",
            "          [ 0.0489,  0.0183,  0.0856],\n",
            "          [ 0.0651, -0.0684,  0.0578]],\n",
            "\n",
            "         [[ 0.0792,  0.0113, -0.0581],\n",
            "          [ 0.1108, -0.0454,  0.0410],\n",
            "          [-0.0730,  0.0237, -0.0683]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.9222, 0.1720, 0.3679, 0.3349, 0.4667, 0.1741, 0.4537, 0.6279, 0.1226,\n",
            "        0.6257, 0.3119, 0.8824, 0.9090, 0.5993, 0.0241, 0.6069, 0.5068, 0.5099,\n",
            "        0.5194, 0.0824, 0.0501, 0.5398, 0.4252, 0.9474, 0.7727, 0.3145, 0.9423,\n",
            "        0.0457, 0.8278, 0.1940, 0.6979, 0.8538], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0535,  0.0224,  0.0550,  0.0430,  0.0384,  0.0035,  0.1269,  0.0151,\n",
            "         0.0365,  0.0526,  0.0715,  0.0451,  0.0278,  0.0348,  0.0038,  0.1027,\n",
            "        -0.0038,  0.0281,  0.0031,  0.0188,  0.0191,  0.0335,  0.0108,  0.0815,\n",
            "         0.1049, -0.0322,  0.0565,  0.0627,  0.0753,  0.0291,  0.1106, -0.0119],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0950, -0.0303,  0.0670],\n",
            "          [-0.0328, -0.0046, -0.0328],\n",
            "          [ 0.1251,  0.0231,  0.0781]],\n",
            "\n",
            "         [[-0.1420, -0.0879, -0.0557],\n",
            "          [ 0.0182, -0.1528, -0.0110],\n",
            "          [-0.1237, -0.1309,  0.0009]],\n",
            "\n",
            "         [[ 0.0036, -0.0515, -0.0974],\n",
            "          [-0.1592, -0.1491, -0.1101],\n",
            "          [ 0.0275, -0.0453, -0.0942]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0882,  0.1213, -0.0942],\n",
            "          [ 0.0644,  0.0635, -0.1202],\n",
            "          [-0.0341, -0.0166,  0.0011]],\n",
            "\n",
            "         [[ 0.0797, -0.0780, -0.0082],\n",
            "          [-0.1209, -0.0713,  0.0805],\n",
            "          [-0.0384, -0.0776, -0.0337]],\n",
            "\n",
            "         [[-0.0555,  0.0563,  0.1040],\n",
            "          [-0.1209,  0.0336, -0.0742],\n",
            "          [ 0.0152, -0.0634,  0.0288]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0313, -0.1372,  0.0006],\n",
            "          [ 0.0250,  0.0820, -0.1361],\n",
            "          [-0.0314,  0.0137,  0.1102]],\n",
            "\n",
            "         [[-0.0140, -0.1403, -0.0423],\n",
            "          [-0.0370, -0.0155,  0.0203],\n",
            "          [ 0.0011, -0.1124,  0.0345]],\n",
            "\n",
            "         [[ 0.0708,  0.0343,  0.0648],\n",
            "          [-0.0533, -0.0705,  0.0659],\n",
            "          [ 0.0964,  0.0258, -0.0777]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0449, -0.0346, -0.0573],\n",
            "          [-0.0556, -0.1162,  0.0817],\n",
            "          [ 0.1124,  0.0538, -0.0501]],\n",
            "\n",
            "         [[-0.0578, -0.0141, -0.1295],\n",
            "          [ 0.0973,  0.1279,  0.1204],\n",
            "          [ 0.1215,  0.0125, -0.0175]],\n",
            "\n",
            "         [[-0.1015, -0.0663, -0.0337],\n",
            "          [ 0.0623, -0.0266,  0.0424],\n",
            "          [-0.0838,  0.0045,  0.0679]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1167, -0.0923,  0.0194],\n",
            "          [-0.0286,  0.0393, -0.1365],\n",
            "          [-0.0636,  0.1548, -0.0311]],\n",
            "\n",
            "         [[ 0.0090,  0.0132,  0.1072],\n",
            "          [ 0.1289,  0.0489,  0.0313],\n",
            "          [-0.1298, -0.1247, -0.0290]],\n",
            "\n",
            "         [[-0.1282, -0.1546, -0.1676],\n",
            "          [-0.1456, -0.1336, -0.1169],\n",
            "          [-0.0976, -0.0229,  0.0804]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0888,  0.0757,  0.1467],\n",
            "          [-0.1280, -0.0736,  0.0174],\n",
            "          [-0.0296,  0.1027, -0.0446]],\n",
            "\n",
            "         [[ 0.0582, -0.0505,  0.0284],\n",
            "          [-0.0213, -0.1195,  0.1322],\n",
            "          [ 0.0638, -0.0115,  0.0208]],\n",
            "\n",
            "         [[-0.0062,  0.0048,  0.1268],\n",
            "          [ 0.0701, -0.1370,  0.0712],\n",
            "          [ 0.0540, -0.0330, -0.0332]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1010, -0.0051,  0.0784],\n",
            "          [-0.1108,  0.0034, -0.1348],\n",
            "          [ 0.1341, -0.1235, -0.1244]],\n",
            "\n",
            "         [[ 0.1276, -0.0685,  0.0326],\n",
            "          [ 0.0621, -0.0962,  0.0416],\n",
            "          [ 0.0817, -0.0606,  0.1171]],\n",
            "\n",
            "         [[ 0.0640, -0.0347,  0.0698],\n",
            "          [-0.1137, -0.0709, -0.0654],\n",
            "          [ 0.0569,  0.0558,  0.0210]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0683,  0.0055,  0.0938],\n",
            "          [-0.0611,  0.0724,  0.0058],\n",
            "          [ 0.0738,  0.0009,  0.1117]],\n",
            "\n",
            "         [[ 0.1636,  0.1375, -0.1156],\n",
            "          [ 0.0931,  0.1119, -0.0809],\n",
            "          [-0.0462, -0.0574,  0.0335]],\n",
            "\n",
            "         [[-0.0301,  0.0216,  0.0283],\n",
            "          [ 0.0199,  0.0969, -0.1379],\n",
            "          [-0.0207,  0.0993, -0.0222]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0363,  0.0112, -0.1108],\n",
            "          [-0.0722,  0.0251, -0.1059],\n",
            "          [-0.0463,  0.0422,  0.0145]],\n",
            "\n",
            "         [[-0.0384,  0.1339,  0.0246],\n",
            "          [ 0.0950, -0.1343, -0.1268],\n",
            "          [-0.1219, -0.0976,  0.0121]],\n",
            "\n",
            "         [[-0.0476,  0.1157, -0.0085],\n",
            "          [ 0.1250,  0.1071,  0.1019],\n",
            "          [-0.0300, -0.1451,  0.0519]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0585,  0.0836, -0.0709],\n",
            "          [ 0.1089, -0.1318, -0.1362],\n",
            "          [-0.1058, -0.1002,  0.1336]],\n",
            "\n",
            "         [[-0.1210, -0.1243, -0.0965],\n",
            "          [-0.1233,  0.0376, -0.1028],\n",
            "          [-0.0729,  0.0260, -0.0438]],\n",
            "\n",
            "         [[ 0.0736, -0.0131, -0.1141],\n",
            "          [-0.0320,  0.0062, -0.1376],\n",
            "          [ 0.1334,  0.1376,  0.1246]]],\n",
            "\n",
            "\n",
            "        [[[-0.0967, -0.0740, -0.1416],\n",
            "          [-0.0308, -0.0140,  0.1357],\n",
            "          [ 0.1133, -0.0457,  0.0774]],\n",
            "\n",
            "         [[ 0.0929, -0.1155,  0.1066],\n",
            "          [ 0.0421,  0.0973, -0.0115],\n",
            "          [-0.0103,  0.0898, -0.0170]],\n",
            "\n",
            "         [[-0.0832, -0.0484, -0.1105],\n",
            "          [ 0.0336,  0.0725, -0.1530],\n",
            "          [-0.0129,  0.1374,  0.1239]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0040, -0.1391, -0.0217],\n",
            "          [-0.1511, -0.0310,  0.1362],\n",
            "          [-0.1354,  0.0504,  0.0035]],\n",
            "\n",
            "         [[-0.1433,  0.0637, -0.0943],\n",
            "          [ 0.0064,  0.0721, -0.0489],\n",
            "          [-0.1247,  0.0686,  0.0868]],\n",
            "\n",
            "         [[-0.1289,  0.1246,  0.1076],\n",
            "          [-0.0170, -0.0173,  0.0486],\n",
            "          [ 0.1016,  0.0299,  0.0123]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.8932, 0.1425, 0.6219, 0.8556, 0.8200, 0.1485, 0.4424, 0.0153, 0.5941,\n",
            "        0.8193, 0.9545, 0.7961, 0.4925, 0.2677, 0.2878, 0.5119, 0.1824, 0.5993,\n",
            "        0.1466, 0.2344, 0.0023, 0.7786, 0.4866, 0.6198, 0.0332, 0.2135, 0.6618,\n",
            "        0.4457, 0.6195, 0.9899, 0.1601, 0.6205, 0.5080, 0.7209, 0.7175, 0.8437,\n",
            "        0.7611, 0.3152, 0.7701, 0.1347, 0.9901, 0.8169, 0.8683, 0.5463, 0.1439,\n",
            "        0.5770, 0.7799, 0.5167, 0.7313, 0.7125, 0.5008, 0.9358, 0.8247, 0.8155,\n",
            "        0.2509, 0.9092, 0.4413, 0.0824, 0.3163, 1.0246, 0.5161, 0.9801, 0.1131,\n",
            "        0.1630], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0411, -0.0171, -0.0031, -0.0197,  0.0533,  0.0201, -0.0099,  0.0113,\n",
            "        -0.0209, -0.0492, -0.0212, -0.0278,  0.0728, -0.0124,  0.0321, -0.0120,\n",
            "         0.0742, -0.0252,  0.0089,  0.0379, -0.0125, -0.0285,  0.0037, -0.0202,\n",
            "         0.0497, -0.0034, -0.0416, -0.0523,  0.0175,  0.0414, -0.0283, -0.0108,\n",
            "         0.0561, -0.0049, -0.0003,  0.0400, -0.0279, -0.0083,  0.0280,  0.0312,\n",
            "        -0.0049,  0.0027,  0.0141,  0.0039, -0.0090,  0.0676, -0.0226,  0.0234,\n",
            "         0.0345, -0.0282, -0.0192, -0.0081,  0.0153,  0.0140,  0.0436,  0.0113,\n",
            "         0.0430,  0.0329, -0.0052, -0.0095,  0.0386,  0.0411,  0.0251, -0.0545],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0837,  0.0293,  0.0488],\n",
            "          [-0.0036, -0.0388, -0.0632],\n",
            "          [-0.0108, -0.0382, -0.0258]],\n",
            "\n",
            "         [[ 0.0279,  0.0571,  0.0584],\n",
            "          [ 0.0775, -0.0127, -0.0770],\n",
            "          [-0.0164, -0.0091,  0.0878]],\n",
            "\n",
            "         [[ 0.0150, -0.0387,  0.0015],\n",
            "          [-0.0489,  0.0533,  0.0658],\n",
            "          [ 0.0581, -0.0549,  0.0440]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0633,  0.0435, -0.0745],\n",
            "          [ 0.0289,  0.0163, -0.0495],\n",
            "          [-0.0692, -0.0078, -0.0709]],\n",
            "\n",
            "         [[ 0.0182,  0.0634, -0.0829],\n",
            "          [-0.0560,  0.0159,  0.0560],\n",
            "          [ 0.1043, -0.0435,  0.0704]],\n",
            "\n",
            "         [[-0.0941,  0.0792,  0.0820],\n",
            "          [ 0.0107, -0.0235, -0.0703],\n",
            "          [-0.0296, -0.0781, -0.0640]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1245, -0.0170,  0.0453],\n",
            "          [ 0.0649,  0.0731,  0.0297],\n",
            "          [-0.0914, -0.0475, -0.0990]],\n",
            "\n",
            "         [[-0.0779, -0.0700,  0.0479],\n",
            "          [ 0.0561,  0.0335,  0.0195],\n",
            "          [-0.0115, -0.0380, -0.0505]],\n",
            "\n",
            "         [[ 0.0822,  0.0752, -0.0203],\n",
            "          [-0.0531,  0.0953, -0.0627],\n",
            "          [-0.0277, -0.0733, -0.0131]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0747,  0.0801, -0.0692],\n",
            "          [-0.0896, -0.0979,  0.0410],\n",
            "          [-0.0442, -0.0079,  0.0100]],\n",
            "\n",
            "         [[ 0.0334, -0.0610, -0.0490],\n",
            "          [ 0.0520,  0.0451,  0.0102],\n",
            "          [-0.0958,  0.0826, -0.0794]],\n",
            "\n",
            "         [[ 0.0018,  0.0601, -0.0964],\n",
            "          [ 0.0373,  0.0611, -0.0826],\n",
            "          [ 0.0673,  0.0764, -0.0123]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0074,  0.0368, -0.0889],\n",
            "          [ 0.0051,  0.0606, -0.0144],\n",
            "          [ 0.0642, -0.0325,  0.0400]],\n",
            "\n",
            "         [[ 0.0931,  0.0379, -0.1037],\n",
            "          [-0.0181,  0.0118, -0.0109],\n",
            "          [-0.0719,  0.0074,  0.0692]],\n",
            "\n",
            "         [[ 0.0078, -0.0437,  0.0744],\n",
            "          [-0.0107,  0.0204,  0.0177],\n",
            "          [-0.0808,  0.0832,  0.0580]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0334,  0.0715,  0.0277],\n",
            "          [ 0.0165, -0.0611,  0.0259],\n",
            "          [-0.0291,  0.0589,  0.1050]],\n",
            "\n",
            "         [[-0.0631, -0.0946, -0.0754],\n",
            "          [-0.0431, -0.0466, -0.0047],\n",
            "          [ 0.0826, -0.0381,  0.0399]],\n",
            "\n",
            "         [[ 0.0125,  0.0543,  0.0666],\n",
            "          [ 0.0128,  0.0751,  0.0886],\n",
            "          [-0.0539,  0.0707,  0.0574]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0380, -0.0534, -0.0769],\n",
            "          [ 0.0263, -0.0649,  0.0256],\n",
            "          [ 0.0403, -0.0411,  0.0688]],\n",
            "\n",
            "         [[ 0.0556, -0.0169, -0.0951],\n",
            "          [-0.0755, -0.1022,  0.0632],\n",
            "          [ 0.0814,  0.0989, -0.0337]],\n",
            "\n",
            "         [[-0.0084,  0.0010, -0.0613],\n",
            "          [ 0.0538,  0.0756,  0.0038],\n",
            "          [-0.0199, -0.0403, -0.0534]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0588, -0.0353, -0.0761],\n",
            "          [ 0.0776,  0.0722, -0.0306],\n",
            "          [ 0.0289, -0.0012,  0.0775]],\n",
            "\n",
            "         [[-0.0909,  0.0288,  0.0042],\n",
            "          [-0.0517,  0.0519,  0.0395],\n",
            "          [-0.0873, -0.0564,  0.0314]],\n",
            "\n",
            "         [[-0.0818, -0.0545,  0.0659],\n",
            "          [ 0.0801,  0.0404,  0.0858],\n",
            "          [-0.0073, -0.0291,  0.0427]]],\n",
            "\n",
            "\n",
            "        [[[-0.0507,  0.0279, -0.0395],\n",
            "          [-0.0242,  0.0238, -0.0728],\n",
            "          [-0.0333, -0.1191,  0.0562]],\n",
            "\n",
            "         [[ 0.0686,  0.0683,  0.0972],\n",
            "          [ 0.0710, -0.0878,  0.0204],\n",
            "          [ 0.0876, -0.0919, -0.0735]],\n",
            "\n",
            "         [[-0.0079,  0.0177,  0.0682],\n",
            "          [ 0.0922,  0.0467,  0.0349],\n",
            "          [ 0.0638,  0.0462, -0.0329]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0098,  0.0890,  0.0921],\n",
            "          [-0.0655,  0.0517, -0.0602],\n",
            "          [ 0.0807,  0.0430,  0.1081]],\n",
            "\n",
            "         [[-0.0330, -0.0800,  0.0504],\n",
            "          [-0.0362, -0.0200,  0.0813],\n",
            "          [ 0.0758,  0.1016, -0.0183]],\n",
            "\n",
            "         [[ 0.0177,  0.0550,  0.0938],\n",
            "          [-0.0132,  0.0396,  0.0533],\n",
            "          [-0.0238, -0.1024, -0.0963]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0351, -0.1043,  0.0960],\n",
            "          [ 0.0462,  0.0703,  0.0272],\n",
            "          [-0.0455,  0.0174, -0.0338]],\n",
            "\n",
            "         [[ 0.0146,  0.0333,  0.0960],\n",
            "          [-0.0872,  0.0300,  0.0237],\n",
            "          [-0.1037, -0.0164, -0.0636]],\n",
            "\n",
            "         [[-0.0427,  0.0112, -0.0291],\n",
            "          [ 0.0857,  0.1077, -0.0984],\n",
            "          [ 0.0995, -0.0802, -0.0010]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0240,  0.0725,  0.0156],\n",
            "          [ 0.0730, -0.0220,  0.0403],\n",
            "          [ 0.0360,  0.0082, -0.0068]],\n",
            "\n",
            "         [[ 0.0145,  0.0084, -0.0535],\n",
            "          [-0.0454, -0.0386,  0.0908],\n",
            "          [-0.0773,  0.0666, -0.0628]],\n",
            "\n",
            "         [[ 0.0359,  0.0076,  0.0173],\n",
            "          [ 0.0340, -0.0199,  0.0361],\n",
            "          [ 0.0011, -0.0455, -0.0607]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.6280,  0.5989,  0.4503,  0.6074,  0.0878,  1.0234,  0.0072,  0.1061,\n",
            "         0.1773,  0.1877,  0.2784,  0.3747,  0.5639,  0.3647,  0.1335,  0.9958,\n",
            "         0.4868,  0.7845,  0.6870, -0.0152,  0.1238,  1.1355,  0.1597,  0.7758,\n",
            "         0.1060, -0.0070,  0.9467,  0.9372,  0.8554,  0.5618,  0.8737,  0.2429,\n",
            "         0.1277,  0.7014,  1.0399,  0.2101,  0.4278,  0.1408,  0.9788,  0.9072,\n",
            "         0.2751,  0.6532,  0.9257,  0.9427,  0.0838,  0.5723,  0.2504,  0.7577,\n",
            "         0.8125,  0.1321, -0.0293,  0.0456,  0.4002,  0.7106,  0.3857,  0.2667,\n",
            "         0.0039,  0.1562,  0.7356,  0.6980,  0.6694,  0.1733,  0.5031,  0.1223],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0936,  0.0613,  0.0188,  0.0643,  0.0056,  0.1346,  0.0274,  0.0072,\n",
            "         0.0506,  0.0198,  0.0960,  0.0194, -0.0040,  0.0933, -0.0144, -0.0006,\n",
            "         0.0492,  0.0578,  0.0399,  0.0184,  0.0165,  0.1245,  0.0041,  0.0101,\n",
            "         0.0128,  0.0915,  0.0339,  0.0676,  0.0447,  0.0190,  0.0760,  0.0375,\n",
            "        -0.0088,  0.0728,  0.0831,  0.0527,  0.0740,  0.0490,  0.1074,  0.0432,\n",
            "         0.0379,  0.1019,  0.0264,  0.0356,  0.0168,  0.0800,  0.0608,  0.0218,\n",
            "         0.0637,  0.0012,  0.0206, -0.0539,  0.0297,  0.0230,  0.0090,  0.0213,\n",
            "         0.0276,  0.0088,  0.0349,  0.0617, -0.0194,  0.0347,  0.0009,  0.0304],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.4379]],\n",
            "\n",
            "         [[-0.1937]],\n",
            "\n",
            "         [[-0.1584]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3753]],\n",
            "\n",
            "         [[ 0.1456]],\n",
            "\n",
            "         [[-0.1081]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4017]],\n",
            "\n",
            "         [[ 0.1364]],\n",
            "\n",
            "         [[ 0.4219]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1048]],\n",
            "\n",
            "         [[ 0.2781]],\n",
            "\n",
            "         [[ 0.0524]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2213]],\n",
            "\n",
            "         [[-0.0455]],\n",
            "\n",
            "         [[ 0.3258]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.4189]],\n",
            "\n",
            "         [[ 0.3942]],\n",
            "\n",
            "         [[ 0.1981]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.3959]],\n",
            "\n",
            "         [[-0.0057]],\n",
            "\n",
            "         [[ 0.2600]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3158]],\n",
            "\n",
            "         [[ 0.2861]],\n",
            "\n",
            "         [[-0.3701]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1183]],\n",
            "\n",
            "         [[-0.3281]],\n",
            "\n",
            "         [[ 0.3894]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1021]],\n",
            "\n",
            "         [[-0.3986]],\n",
            "\n",
            "         [[-0.3905]]],\n",
            "\n",
            "\n",
            "        [[[-0.1745]],\n",
            "\n",
            "         [[ 0.3885]],\n",
            "\n",
            "         [[-0.1002]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0663]],\n",
            "\n",
            "         [[ 0.2862]],\n",
            "\n",
            "         [[ 0.0537]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1941,  0.6544,  0.3160,  0.2490,  0.2740,  0.9197,  0.4126,  0.3007,\n",
            "         0.3049,  0.7850,  0.1372,  0.2667,  0.6918,  0.8447,  0.6667,  0.8533,\n",
            "         0.7587,  0.5659,  0.7504,  0.2605,  0.3670,  0.9736,  0.0164,  0.3258,\n",
            "         0.4327,  0.5600,  0.0248,  0.5744,  0.1230,  0.2347,  0.1233,  0.8426,\n",
            "         0.7201,  0.6221,  0.0154,  0.7692,  0.1467,  0.2143,  0.6358,  0.5862,\n",
            "         0.6408,  0.1804,  0.9128,  0.9047, -0.0170,  0.2737,  0.2632,  0.0950,\n",
            "         0.9703,  0.7763,  0.6392,  0.3313,  0.6286,  0.1303,  0.9010,  0.2892,\n",
            "         0.7782,  0.8523,  0.2815,  0.2530,  0.6789,  0.2360,  0.6905,  0.3863],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1107,  0.0693,  0.0212,  0.0690,  0.0054,  0.1120,  0.0274,  0.0074,\n",
            "         0.0523,  0.0202,  0.0939,  0.0211, -0.0044,  0.0895, -0.0140, -0.0007,\n",
            "         0.0546,  0.0629,  0.0387,  0.0162,  0.0145,  0.1283,  0.0043,  0.0113,\n",
            "         0.0118,  0.0913,  0.0377,  0.0668,  0.0423,  0.0203,  0.0759,  0.0387,\n",
            "        -0.0083,  0.0706,  0.0870,  0.0570,  0.0689,  0.0474,  0.1049,  0.0517,\n",
            "         0.0399,  0.1009,  0.0258,  0.0345,  0.0178,  0.0883,  0.0589,  0.0220,\n",
            "         0.0660,  0.0013,  0.0191, -0.0479,  0.0292,  0.0250,  0.0085,  0.0209,\n",
            "         0.0275,  0.0093,  0.0307,  0.0571, -0.0182,  0.0354,  0.0009,  0.0302],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.0828e-02,  1.9858e-02, -1.0339e-01],\n",
            "          [-3.2113e-02,  8.0415e-02,  7.7979e-02],\n",
            "          [-4.9936e-02,  5.7925e-03, -2.7158e-02]],\n",
            "\n",
            "         [[-1.0243e-01,  7.0545e-02, -1.1191e-01],\n",
            "          [-6.5160e-02, -7.2692e-02, -4.9076e-03],\n",
            "          [ 2.1114e-02,  9.1205e-02, -4.9896e-02]],\n",
            "\n",
            "         [[-6.8751e-03, -5.1208e-02, -8.9948e-02],\n",
            "          [ 7.9122e-02, -9.7741e-02,  1.3155e-02],\n",
            "          [-5.7629e-02,  4.2900e-03, -4.3402e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.9599e-02,  6.9137e-03,  5.3245e-02],\n",
            "          [ 8.9953e-02, -3.4983e-02, -3.7878e-02],\n",
            "          [-8.6252e-02,  5.3869e-02, -9.8218e-02]],\n",
            "\n",
            "         [[-4.9628e-02,  5.0643e-02,  5.4642e-02],\n",
            "          [ 1.5221e-02,  5.4307e-02,  6.3433e-02],\n",
            "          [ 5.6371e-02,  7.0980e-03,  6.6511e-02]],\n",
            "\n",
            "         [[ 1.8142e-02, -7.7631e-02, -4.6797e-02],\n",
            "          [-3.4432e-02,  8.8136e-02,  9.5570e-02],\n",
            "          [ 3.9792e-02,  5.7989e-02,  5.7051e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.0069e-02, -1.2744e-02, -8.4208e-02],\n",
            "          [-6.0179e-02,  2.5866e-02,  2.4719e-02],\n",
            "          [-3.6211e-02, -4.8742e-03, -2.4396e-02]],\n",
            "\n",
            "         [[-8.3258e-05, -1.1523e-02,  2.4571e-02],\n",
            "          [ 6.6058e-02,  6.6693e-02,  3.5344e-02],\n",
            "          [ 9.6788e-02,  1.2094e-01,  1.0089e-01]],\n",
            "\n",
            "         [[ 5.2653e-02, -7.9153e-03, -6.8420e-02],\n",
            "          [ 9.4535e-02, -5.3292e-02,  2.3122e-02],\n",
            "          [ 8.4706e-03,  8.6187e-02,  5.4680e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4386e-02,  1.1117e-02,  6.2697e-02],\n",
            "          [ 7.6084e-02, -7.0532e-02,  3.6399e-02],\n",
            "          [-9.8118e-02, -7.6999e-02, -1.5387e-02]],\n",
            "\n",
            "         [[ 1.1350e-01,  5.5434e-02,  4.5258e-02],\n",
            "          [-2.9045e-02,  3.5125e-02,  7.9823e-02],\n",
            "          [ 4.9463e-02, -6.8008e-02, -3.8758e-02]],\n",
            "\n",
            "         [[-2.3449e-02, -3.7547e-02, -1.7533e-02],\n",
            "          [-1.1004e-02, -1.0690e-01,  2.2463e-02],\n",
            "          [-1.0680e-02, -1.8450e-02,  9.6167e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.6275e-02, -7.2066e-02, -6.3804e-02],\n",
            "          [-3.2829e-02,  9.3542e-02,  4.1047e-02],\n",
            "          [ 1.1186e-03,  1.0079e-01,  7.3038e-02]],\n",
            "\n",
            "         [[-8.5264e-02,  8.7821e-03, -9.7970e-04],\n",
            "          [-5.0988e-02, -6.8656e-02,  5.4697e-02],\n",
            "          [ 1.7924e-02, -8.4790e-02,  6.2510e-02]],\n",
            "\n",
            "         [[-3.7207e-02,  4.4259e-02, -4.0314e-02],\n",
            "          [-3.7226e-02,  4.3985e-03, -5.0838e-02],\n",
            "          [ 6.1407e-02,  8.1284e-02,  9.8888e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.3183e-02, -7.5410e-02,  9.9608e-02],\n",
            "          [ 2.6243e-02, -3.8473e-02,  5.2221e-02],\n",
            "          [-7.6760e-02, -8.4583e-02, -1.7966e-02]],\n",
            "\n",
            "         [[ 9.9035e-02,  5.0536e-02,  8.2919e-02],\n",
            "          [ 4.7781e-02, -4.4696e-02,  1.6816e-03],\n",
            "          [-6.0009e-03,  1.6595e-02, -3.6181e-02]],\n",
            "\n",
            "         [[-8.4877e-02, -7.4710e-02,  6.2875e-02],\n",
            "          [-6.4800e-02, -7.7475e-02, -1.0321e-01],\n",
            "          [-3.1196e-02,  8.1984e-02, -2.9162e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-5.7512e-02, -7.8738e-03,  1.3480e-02],\n",
            "          [-2.4651e-02, -3.9280e-02,  4.5624e-02],\n",
            "          [-1.0725e-01, -8.0855e-02, -9.9296e-02]],\n",
            "\n",
            "         [[ 1.9158e-02,  5.6375e-02,  6.1680e-02],\n",
            "          [ 2.9432e-02, -5.8934e-02,  3.5557e-02],\n",
            "          [ 2.8213e-02,  2.8765e-02, -5.8957e-02]],\n",
            "\n",
            "         [[ 6.1860e-02, -6.3899e-02,  6.1348e-02],\n",
            "          [-5.0077e-02, -9.3061e-02, -3.9568e-02],\n",
            "          [ 7.1220e-03, -6.7478e-03,  4.2271e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.7917e-02,  3.7024e-02, -4.9539e-02],\n",
            "          [-5.4252e-02,  4.1235e-02, -7.4058e-03],\n",
            "          [-3.3719e-02,  5.3608e-03,  3.9679e-02]],\n",
            "\n",
            "         [[ 1.0671e-01,  5.2199e-02, -5.5609e-02],\n",
            "          [-2.2598e-02, -4.3364e-03,  6.0552e-02],\n",
            "          [ 9.2666e-03, -7.6640e-02,  2.2270e-02]],\n",
            "\n",
            "         [[-1.6699e-02, -2.5500e-02,  1.1608e-02],\n",
            "          [ 8.8489e-02,  5.6874e-02,  8.1017e-02],\n",
            "          [-8.0865e-02,  8.6085e-02,  5.4993e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.5554e-02, -9.2066e-02, -1.0925e-01],\n",
            "          [ 4.4664e-02,  2.2084e-02, -6.8390e-02],\n",
            "          [ 2.7099e-02,  6.0661e-02, -6.9456e-03]],\n",
            "\n",
            "         [[-7.1811e-02, -2.4048e-02, -4.6426e-02],\n",
            "          [-8.9848e-02, -7.0523e-02, -3.9123e-02],\n",
            "          [-2.2467e-02, -3.6007e-02,  1.8670e-02]],\n",
            "\n",
            "         [[ 2.4168e-02, -6.8084e-02,  6.3012e-02],\n",
            "          [-3.2950e-02, -4.7501e-02, -5.3957e-02],\n",
            "          [-2.6694e-02,  9.0242e-02, -8.1906e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.8073e-02,  7.6332e-03, -4.6150e-02],\n",
            "          [-2.8097e-02,  3.3557e-02,  7.0270e-02],\n",
            "          [-7.6442e-02, -2.7805e-02, -1.1222e-02]],\n",
            "\n",
            "         [[ 2.2105e-02,  4.7719e-02, -1.6587e-02],\n",
            "          [-5.9778e-02, -1.0163e-01,  2.1613e-02],\n",
            "          [-7.2232e-02,  5.4749e-03,  8.3114e-02]],\n",
            "\n",
            "         [[-8.2052e-02, -6.5950e-02, -9.7161e-02],\n",
            "          [-8.0347e-02, -7.3330e-02, -1.9687e-02],\n",
            "          [ 5.1106e-02,  1.8054e-02, -9.9651e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4464e-02,  9.5999e-02, -8.4998e-02],\n",
            "          [-4.0914e-02,  4.3434e-02, -5.2580e-02],\n",
            "          [-7.2744e-02,  3.7779e-02,  7.4124e-02]],\n",
            "\n",
            "         [[ 6.7026e-02,  7.6080e-02, -2.3775e-02],\n",
            "          [ 7.2467e-02,  3.0895e-02, -6.2778e-02],\n",
            "          [ 8.7370e-02,  4.7453e-02,  4.8170e-02]],\n",
            "\n",
            "         [[ 7.0607e-02,  3.1476e-02, -7.2531e-02],\n",
            "          [ 3.3562e-02, -7.3980e-02, -6.3427e-02],\n",
            "          [ 2.1761e-02,  6.8129e-02, -5.8241e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8388e-02,  7.7336e-02, -1.0740e-01],\n",
            "          [-9.1861e-02, -1.1930e-02, -2.8941e-02],\n",
            "          [-4.8286e-02, -5.1505e-02,  2.3316e-02]],\n",
            "\n",
            "         [[ 9.1934e-02, -5.7758e-02,  8.3262e-02],\n",
            "          [-8.3313e-02,  9.1093e-02,  6.0779e-02],\n",
            "          [-7.9446e-02,  4.8066e-02,  2.3358e-02]],\n",
            "\n",
            "         [[ 6.0657e-02, -6.3980e-02,  3.5820e-02],\n",
            "          [ 1.1055e-02,  7.2416e-02, -2.6754e-02],\n",
            "          [ 7.8438e-02,  3.7544e-02,  3.7620e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4026, 0.6016, 0.0472, 0.3265, 0.1878, 0.2792, 0.3653, 0.0957, 0.0874,\n",
            "        0.8740, 0.7202, 0.1873, 0.6354, 0.8177, 0.7357, 0.9349, 0.2959, 0.0105,\n",
            "        0.3996, 0.1573, 0.2915, 0.4702, 0.9072, 0.4527, 0.1461, 0.9166, 0.6481,\n",
            "        0.0891, 0.5707, 0.4219, 0.1923, 0.0656, 0.1209, 0.8226, 0.0964, 0.7721,\n",
            "        0.2367, 0.7261, 0.1225, 0.1058, 0.2044, 0.1689, 0.5781, 0.7330, 0.0826,\n",
            "        0.0383, 0.1722, 0.4130, 0.3000, 0.0747, 0.1520, 0.4252, 0.0516, 0.1566,\n",
            "        0.0332, 0.9902, 0.5911, 0.6579, 0.9686, 0.4089, 0.2492, 0.8934, 0.2916,\n",
            "        0.7672], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0288, -0.0093, -0.0070,  0.0086,  0.0134,  0.0054, -0.0289, -0.0234,\n",
            "        -0.0032, -0.0761,  0.0042, -0.0084,  0.0515,  0.0022,  0.0226,  0.0462,\n",
            "         0.0033, -0.0102,  0.0060, -0.0071, -0.0199, -0.0164, -0.0381, -0.0086,\n",
            "        -0.0205,  0.0073, -0.0152, -0.0344,  0.0154,  0.0335,  0.0050,  0.0133,\n",
            "        -0.0112, -0.0596,  0.0023,  0.0160, -0.0075, -0.0260,  0.0247, -0.0085,\n",
            "         0.0192,  0.0259,  0.0247, -0.0377, -0.0061, -0.0100, -0.0080, -0.0365,\n",
            "        -0.0018,  0.0210, -0.0234, -0.0266, -0.0060, -0.0297,  0.0008, -0.0013,\n",
            "         0.0538, -0.0334, -0.0937, -0.0216, -0.0026,  0.0314, -0.0304, -0.0606],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-8.6818e-02,  1.1427e-02, -4.7906e-02],\n",
            "          [-5.4232e-02, -9.9321e-02, -9.5150e-02],\n",
            "          [-1.5554e-02, -3.9938e-02, -6.2616e-02]],\n",
            "\n",
            "         [[ 9.8318e-02,  3.2744e-02,  2.7941e-02],\n",
            "          [-7.5418e-02, -6.1390e-02, -6.3732e-03],\n",
            "          [-4.3548e-02,  4.3280e-02, -9.2135e-02]],\n",
            "\n",
            "         [[ 7.9256e-02,  5.7926e-02,  4.6491e-02],\n",
            "          [ 6.6811e-02,  5.4486e-03, -8.1364e-03],\n",
            "          [ 8.4482e-02,  1.3994e-02,  6.2651e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5820e-02,  2.3632e-02, -1.0351e-02],\n",
            "          [-4.2693e-02, -1.5200e-02, -3.2438e-02],\n",
            "          [-2.8796e-03, -3.8677e-02, -4.9781e-02]],\n",
            "\n",
            "         [[-2.2034e-02, -6.2167e-02,  6.7146e-02],\n",
            "          [ 9.5386e-02,  1.6840e-02,  5.1511e-02],\n",
            "          [-7.1782e-02,  7.3052e-02, -8.4904e-02]],\n",
            "\n",
            "         [[-6.0024e-03, -3.9302e-02, -9.1355e-02],\n",
            "          [-9.5153e-02,  5.1819e-02, -1.1601e-02],\n",
            "          [-1.5530e-02, -1.9490e-02, -4.0471e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.5353e-03, -2.7719e-03,  4.3534e-02],\n",
            "          [ 6.1055e-02,  8.8348e-02, -8.0255e-02],\n",
            "          [-2.7922e-02,  2.8815e-02,  8.9809e-02]],\n",
            "\n",
            "         [[-3.6295e-02, -1.0843e-01, -4.2233e-02],\n",
            "          [-1.1381e-01, -2.5568e-02, -4.5829e-03],\n",
            "          [-8.8499e-02, -9.4046e-02,  2.1866e-02]],\n",
            "\n",
            "         [[-1.7687e-02,  2.2543e-02,  1.3742e-02],\n",
            "          [ 1.5337e-02, -1.1855e-02, -5.1363e-02],\n",
            "          [ 6.8153e-02, -5.9913e-02, -3.3409e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1803e-04,  6.5903e-02,  3.4223e-02],\n",
            "          [ 2.3443e-02,  2.3911e-03, -5.0150e-02],\n",
            "          [ 4.7007e-02,  8.7032e-02,  7.2173e-02]],\n",
            "\n",
            "         [[ 7.0828e-03, -3.7118e-02,  5.0492e-02],\n",
            "          [-3.5336e-02, -4.4578e-02, -1.4440e-03],\n",
            "          [ 3.7301e-02,  8.9320e-02, -8.2577e-02]],\n",
            "\n",
            "         [[ 1.0225e-01,  8.7863e-02, -9.0457e-02],\n",
            "          [ 5.8801e-03,  1.0456e-01, -6.4580e-03],\n",
            "          [ 4.2844e-02,  3.1600e-02,  5.4661e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5688e-02,  8.0342e-02, -5.7043e-02],\n",
            "          [ 4.9541e-02,  7.9332e-02, -5.6608e-02],\n",
            "          [ 6.7685e-02,  5.8741e-03, -5.2806e-05]],\n",
            "\n",
            "         [[ 2.5564e-02,  2.8988e-03,  6.6318e-02],\n",
            "          [ 3.5606e-02,  7.4727e-02, -2.8989e-03],\n",
            "          [-4.1621e-02,  4.0348e-02,  9.0520e-02]],\n",
            "\n",
            "         [[-2.4166e-02, -1.8406e-02, -1.9791e-02],\n",
            "          [-1.9828e-02,  7.2115e-02, -2.0089e-02],\n",
            "          [ 5.1536e-02, -1.1199e-02,  4.7021e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2089e-02, -8.8068e-02,  8.8355e-02],\n",
            "          [ 5.1303e-02,  4.4869e-02,  4.9244e-03],\n",
            "          [ 3.9131e-02, -8.2652e-02, -1.1842e-01]],\n",
            "\n",
            "         [[ 9.0530e-02, -7.0253e-02, -1.5381e-02],\n",
            "          [ 1.0011e-01, -5.5029e-02, -6.8581e-02],\n",
            "          [-6.5040e-02, -5.0139e-02, -2.3215e-02]],\n",
            "\n",
            "         [[-1.2388e-02, -9.2275e-02, -7.7865e-02],\n",
            "          [ 7.6682e-02, -2.7422e-02, -3.9289e-02],\n",
            "          [ 5.5070e-03,  5.6816e-03,  3.7660e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 9.3106e-02,  2.7359e-03,  6.5503e-02],\n",
            "          [ 5.5645e-02, -4.3558e-02, -7.1496e-02],\n",
            "          [-4.3144e-04, -2.1034e-02,  9.3575e-02]],\n",
            "\n",
            "         [[ 1.0683e-01,  4.6549e-02, -2.5424e-02],\n",
            "          [ 9.4112e-02, -1.0054e-02,  8.0833e-02],\n",
            "          [-5.6660e-02, -9.3431e-03, -9.3307e-02]],\n",
            "\n",
            "         [[-1.4209e-02,  2.6691e-02,  8.3841e-02],\n",
            "          [-2.1724e-02,  9.3190e-02,  3.0540e-02],\n",
            "          [ 7.4491e-02, -1.5577e-02, -3.8477e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.0267e-02, -4.3310e-02,  6.1166e-03],\n",
            "          [-4.8449e-02, -7.2712e-02,  4.5047e-02],\n",
            "          [-8.4038e-02, -3.1941e-02, -3.1551e-02]],\n",
            "\n",
            "         [[-8.5456e-02, -4.2550e-02,  6.5972e-02],\n",
            "          [ 3.3138e-02,  7.1317e-02, -9.9725e-02],\n",
            "          [-7.1896e-02, -6.5147e-02,  8.7841e-03]],\n",
            "\n",
            "         [[ 7.9391e-02, -1.2060e-02,  2.7421e-02],\n",
            "          [ 6.7861e-02, -1.0008e-01, -2.0491e-02],\n",
            "          [-7.4067e-02, -5.8999e-03, -8.3927e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2498e-02,  5.4272e-03,  6.1699e-02],\n",
            "          [ 2.7008e-02,  2.6601e-02, -7.5468e-03],\n",
            "          [-4.7520e-02, -1.0354e-01,  8.7688e-02]],\n",
            "\n",
            "         [[-3.3629e-02, -2.0475e-02,  4.5785e-02],\n",
            "          [ 3.5059e-02, -1.4910e-02, -3.8013e-02],\n",
            "          [ 1.1001e-01, -5.0495e-02,  7.7933e-03]],\n",
            "\n",
            "         [[-3.6838e-02,  5.3616e-02,  2.2233e-02],\n",
            "          [-5.9855e-02,  3.1644e-02, -8.6121e-02],\n",
            "          [-4.4656e-02,  7.0966e-02,  5.8040e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4496e-03,  6.0522e-02, -7.4740e-02],\n",
            "          [ 7.1103e-02,  3.1264e-02,  7.3984e-02],\n",
            "          [ 1.5384e-02,  1.0480e-01, -2.2744e-02]],\n",
            "\n",
            "         [[-9.8359e-02,  3.1270e-02, -1.5260e-02],\n",
            "          [-8.6911e-02,  3.0888e-02,  2.2563e-02],\n",
            "          [-4.3636e-02,  1.0848e-02,  2.0108e-02]],\n",
            "\n",
            "         [[-4.9463e-04,  9.5473e-02, -3.5967e-02],\n",
            "          [-9.2702e-02,  2.1324e-02,  7.5774e-02],\n",
            "          [ 1.9102e-04,  5.6395e-02,  3.7679e-02]]],\n",
            "\n",
            "\n",
            "        [[[-8.3222e-02,  6.7859e-02, -4.8567e-02],\n",
            "          [-4.1501e-02, -3.8688e-02,  1.2735e-02],\n",
            "          [ 1.1525e-01, -4.8965e-02, -2.5712e-02]],\n",
            "\n",
            "         [[ 2.6025e-02,  4.7354e-02, -1.8853e-02],\n",
            "          [-5.5409e-02, -8.4117e-02,  4.9749e-02],\n",
            "          [ 5.7812e-02,  5.3677e-02,  4.8731e-02]],\n",
            "\n",
            "         [[-2.2973e-02, -9.9643e-02,  5.2040e-02],\n",
            "          [-1.8179e-02,  1.3707e-02,  2.0643e-02],\n",
            "          [-6.7246e-02,  3.1110e-02, -5.5654e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.2972e-02,  1.4501e-02, -8.0169e-02],\n",
            "          [-6.9468e-02,  3.8961e-02, -7.2766e-02],\n",
            "          [-2.1079e-02, -6.6086e-02,  3.1542e-02]],\n",
            "\n",
            "         [[-9.5130e-04,  4.2356e-02, -3.0669e-02],\n",
            "          [-1.4336e-02, -2.9731e-02,  3.1042e-02],\n",
            "          [ 1.6465e-03,  8.4971e-02, -7.2710e-02]],\n",
            "\n",
            "         [[-3.7731e-02,  1.1009e-02, -3.3058e-02],\n",
            "          [-3.0258e-02,  1.9646e-02,  3.1189e-02],\n",
            "          [-4.0445e-02, -3.0215e-02,  6.6741e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.7809, 0.7359, 0.1948, 0.4743, 1.0132, 0.6507, 0.5045, 0.3647, 0.6077,\n",
            "        0.0850, 0.8131, 0.0844, 0.1013, 0.6050, 0.5566, 0.8990, 0.8492, 0.9076,\n",
            "        0.3429, 0.9798, 0.8436, 0.6470, 0.7520, 0.5423, 0.4010, 0.4695, 0.7229,\n",
            "        0.5728, 0.1959, 1.1539, 1.0898, 1.0553, 0.3807, 0.6865, 0.0130, 0.8522,\n",
            "        0.8346, 0.5725, 0.3568, 0.2672, 0.7025, 0.1063, 0.9741, 0.8589, 0.0426,\n",
            "        0.0243, 0.0956, 1.1220, 0.0332, 0.8448, 0.2590, 0.3031, 0.3370, 0.7905,\n",
            "        1.1386, 0.9655, 0.7616, 0.3015, 0.2025, 0.6307, 0.7586, 0.3515, 0.5137,\n",
            "        0.6120], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0374,  0.0129,  0.0229,  0.0966,  0.0478,  0.1468,  0.0766,  0.0511,\n",
            "         0.0482,  0.0524,  0.0864,  0.0215,  0.0288,  0.0994,  0.0226,  0.0637,\n",
            "         0.1040,  0.0582,  0.0340,  0.0155,  0.0774,  0.1522,  0.0545,  0.0385,\n",
            "         0.0723,  0.1032,  0.0245,  0.0705,  0.0539,  0.0435,  0.0851,  0.0663,\n",
            "        -0.0039,  0.0871,  0.1266,  0.0712,  0.1201,  0.0719,  0.0935,  0.0064,\n",
            "         0.0865,  0.1035,  0.0556,  0.0710,  0.0434,  0.0492,  0.0197,  0.0512,\n",
            "         0.0707,  0.0478,  0.0702, -0.0135,  0.0179,  0.0092,  0.0532,  0.0346,\n",
            "         0.0963,  0.1018,  0.0210,  0.0707,  0.0534,  0.0011,  0.0667,  0.0485],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0730, -0.0472, -0.0540],\n",
            "          [ 0.0407,  0.0273, -0.0629],\n",
            "          [ 0.0508,  0.0922, -0.0897]],\n",
            "\n",
            "         [[-0.0593,  0.0272,  0.0494],\n",
            "          [-0.0854, -0.0182,  0.0398],\n",
            "          [ 0.0607,  0.1033,  0.0480]],\n",
            "\n",
            "         [[ 0.0910, -0.0498, -0.0311],\n",
            "          [ 0.0093, -0.0687,  0.0400],\n",
            "          [ 0.0230, -0.0819,  0.0263]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0542, -0.0519,  0.0432],\n",
            "          [ 0.0154,  0.0233, -0.0757],\n",
            "          [ 0.0863,  0.0029,  0.0872]],\n",
            "\n",
            "         [[-0.0349,  0.0889, -0.0618],\n",
            "          [-0.0035,  0.0144,  0.0027],\n",
            "          [-0.0282,  0.0459,  0.0732]],\n",
            "\n",
            "         [[ 0.0896, -0.0251, -0.0005],\n",
            "          [ 0.0704,  0.0798,  0.0348],\n",
            "          [-0.0176,  0.0213, -0.0670]]],\n",
            "\n",
            "\n",
            "        [[[-0.1042, -0.0017, -0.0833],\n",
            "          [ 0.0557,  0.0212, -0.0062],\n",
            "          [ 0.0531, -0.1006,  0.0076]],\n",
            "\n",
            "         [[ 0.0796, -0.0515,  0.0401],\n",
            "          [-0.0865, -0.0350,  0.0316],\n",
            "          [-0.0111,  0.0189,  0.0084]],\n",
            "\n",
            "         [[-0.1016, -0.0438, -0.0808],\n",
            "          [-0.0403,  0.0979,  0.0606],\n",
            "          [-0.0924,  0.0171, -0.0746]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0946,  0.0531, -0.0628],\n",
            "          [ 0.0373,  0.0556, -0.0102],\n",
            "          [-0.0678, -0.1109,  0.0821]],\n",
            "\n",
            "         [[-0.0520, -0.0847, -0.0851],\n",
            "          [-0.0082,  0.1204,  0.0058],\n",
            "          [ 0.0794,  0.0728,  0.0997]],\n",
            "\n",
            "         [[-0.0125,  0.0378, -0.0364],\n",
            "          [-0.0372,  0.0943, -0.0444],\n",
            "          [ 0.0035,  0.0351,  0.1050]]],\n",
            "\n",
            "\n",
            "        [[[-0.0176,  0.0014,  0.0777],\n",
            "          [ 0.0274, -0.0600, -0.0392],\n",
            "          [ 0.0493, -0.0359,  0.0108]],\n",
            "\n",
            "         [[ 0.0528,  0.0590, -0.0970],\n",
            "          [-0.0158,  0.1054,  0.0460],\n",
            "          [-0.0046,  0.0309, -0.0100]],\n",
            "\n",
            "         [[-0.0818,  0.0504,  0.0218],\n",
            "          [ 0.0930,  0.0308, -0.0349],\n",
            "          [ 0.0193, -0.0442, -0.0058]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0822, -0.0376,  0.0970],\n",
            "          [ 0.0923, -0.0008, -0.0278],\n",
            "          [ 0.0363, -0.0789,  0.0895]],\n",
            "\n",
            "         [[-0.0181,  0.0206, -0.0160],\n",
            "          [-0.0066,  0.0847,  0.0085],\n",
            "          [ 0.0099, -0.0755, -0.0735]],\n",
            "\n",
            "         [[-0.0380,  0.0221,  0.0359],\n",
            "          [ 0.0057,  0.0007, -0.1096],\n",
            "          [-0.0866,  0.0068,  0.0227]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0139,  0.0406,  0.0416],\n",
            "          [-0.0741, -0.0257,  0.0299],\n",
            "          [-0.0225,  0.0541, -0.0188]],\n",
            "\n",
            "         [[-0.0815, -0.0384, -0.0857],\n",
            "          [ 0.0808, -0.0295,  0.0158],\n",
            "          [-0.0940, -0.0652,  0.0649]],\n",
            "\n",
            "         [[ 0.0221, -0.0298, -0.0428],\n",
            "          [ 0.0534, -0.0406,  0.0866],\n",
            "          [-0.0472, -0.0580,  0.0980]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0530,  0.0262, -0.0890],\n",
            "          [-0.0461, -0.0396, -0.0377],\n",
            "          [-0.0169,  0.1012, -0.0383]],\n",
            "\n",
            "         [[-0.0629, -0.0194,  0.0592],\n",
            "          [-0.0688, -0.0811,  0.0867],\n",
            "          [ 0.0321,  0.0287, -0.0112]],\n",
            "\n",
            "         [[-0.0469,  0.0289,  0.0859],\n",
            "          [ 0.0211,  0.0346,  0.0842],\n",
            "          [ 0.0587,  0.0218, -0.0270]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0223, -0.0417, -0.0786],\n",
            "          [-0.0371,  0.0086, -0.0837],\n",
            "          [-0.0588, -0.0663, -0.0485]],\n",
            "\n",
            "         [[ 0.0823,  0.0216,  0.0496],\n",
            "          [-0.0689,  0.0076, -0.0765],\n",
            "          [ 0.0473,  0.0469,  0.0667]],\n",
            "\n",
            "         [[ 0.0454,  0.0504, -0.0074],\n",
            "          [ 0.0957,  0.0438, -0.0740],\n",
            "          [-0.0499,  0.0679, -0.0606]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0979, -0.0160, -0.0232],\n",
            "          [-0.0948,  0.1040, -0.0305],\n",
            "          [-0.0594,  0.0448, -0.0733]],\n",
            "\n",
            "         [[ 0.0875,  0.0780,  0.0248],\n",
            "          [-0.0004,  0.0521, -0.0069],\n",
            "          [ 0.0468, -0.0402, -0.0411]],\n",
            "\n",
            "         [[-0.0756, -0.1011, -0.0723],\n",
            "          [-0.0755, -0.0392, -0.0265],\n",
            "          [-0.0987,  0.0755, -0.0238]]],\n",
            "\n",
            "\n",
            "        [[[-0.0809, -0.0082,  0.0311],\n",
            "          [ 0.0367, -0.0305, -0.0732],\n",
            "          [-0.0153,  0.0811,  0.0217]],\n",
            "\n",
            "         [[-0.0157, -0.0924, -0.0736],\n",
            "          [-0.0643,  0.0622, -0.0597],\n",
            "          [ 0.0014,  0.0748,  0.0679]],\n",
            "\n",
            "         [[-0.0711, -0.0795,  0.0564],\n",
            "          [-0.0473, -0.0323,  0.0111],\n",
            "          [ 0.1009, -0.0640,  0.0255]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0853, -0.0980,  0.0933],\n",
            "          [ 0.0366,  0.0863, -0.0137],\n",
            "          [ 0.0577,  0.0360, -0.0020]],\n",
            "\n",
            "         [[-0.0898,  0.0956, -0.0365],\n",
            "          [-0.0243, -0.0231,  0.0182],\n",
            "          [ 0.0399, -0.0742,  0.0412]],\n",
            "\n",
            "         [[ 0.0520,  0.0760,  0.0065],\n",
            "          [ 0.0847,  0.0984,  0.0753],\n",
            "          [-0.0900, -0.0528,  0.1010]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.5878, 0.8131, 0.3031, 0.9124, 0.7820, 0.7345, 0.6776, 0.0461, 0.4875,\n",
            "        0.3716, 0.9487, 0.8476, 0.1696, 0.6199, 0.7612, 0.1989, 0.4767, 0.6470,\n",
            "        0.2860, 0.5586, 0.3781, 1.0250, 0.6461, 0.6502, 0.5363, 0.1181, 0.9450,\n",
            "        0.6816, 0.2165, 0.5446, 0.3574, 0.6870, 0.4089, 0.2832, 0.6845, 0.8209,\n",
            "        0.9325, 0.4448, 0.3512, 0.8556, 0.9567, 0.3360, 0.4904, 0.5506, 0.9451,\n",
            "        0.8319, 0.9536, 0.4246, 0.0543, 0.8746, 0.9390, 0.7696, 0.7173, 0.9388,\n",
            "        0.0774, 0.2170, 0.8490, 0.6773, 0.5789, 0.4285, 0.8299, 0.5069, 0.1657,\n",
            "        0.3103], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0265,  0.0017, -0.0092, -0.0363, -0.0451,  0.0235, -0.0468, -0.0102,\n",
            "         0.0165,  0.0199,  0.0097, -0.0281, -0.0117, -0.0285, -0.0330,  0.0236,\n",
            "        -0.0075, -0.0328,  0.0312, -0.0100, -0.0230,  0.0214,  0.0026,  0.0153,\n",
            "         0.0364,  0.0066,  0.0133,  0.0005,  0.0022, -0.0190, -0.0022,  0.0075,\n",
            "         0.0133, -0.0170,  0.0038, -0.0426, -0.0205, -0.0102, -0.0121, -0.0097,\n",
            "        -0.0080,  0.0115, -0.0282,  0.0008,  0.0297, -0.0331, -0.0018,  0.0102,\n",
            "         0.0023, -0.0387,  0.0588, -0.0134, -0.0249, -0.0376,  0.0047,  0.0037,\n",
            "        -0.0141, -0.0188, -0.0088, -0.0209, -0.0145,  0.0005, -0.0160,  0.0069],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-8.8893e-02,  2.7340e-02,  8.1739e-02],\n",
            "          [ 3.8701e-02, -5.2502e-03,  4.2225e-02],\n",
            "          [-2.6406e-02, -8.2387e-03, -1.2757e-02]],\n",
            "\n",
            "         [[ 9.4280e-02, -7.5199e-02, -9.2759e-02],\n",
            "          [ 6.1714e-02,  8.9627e-02, -6.2648e-02],\n",
            "          [-1.5923e-02,  8.8672e-02, -5.2003e-02]],\n",
            "\n",
            "         [[ 3.1726e-03,  2.2926e-02,  1.2058e-02],\n",
            "          [-7.6320e-02,  2.0968e-02, -8.0284e-02],\n",
            "          [-7.0313e-02, -8.1400e-02,  8.7474e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.6195e-02, -3.9061e-03, -9.0913e-02],\n",
            "          [-9.7457e-02,  3.6557e-02,  6.5806e-02],\n",
            "          [ 1.8130e-02,  5.1591e-02,  5.6000e-02]],\n",
            "\n",
            "         [[-6.5376e-04, -6.5642e-02, -6.7999e-02],\n",
            "          [ 3.1562e-02, -4.0275e-02, -8.6124e-02],\n",
            "          [-6.0148e-02, -9.2129e-02, -6.7216e-02]],\n",
            "\n",
            "         [[ 9.3631e-02, -4.8421e-02,  8.6608e-02],\n",
            "          [-2.9745e-02, -7.5557e-03, -1.0043e-01],\n",
            "          [-3.3369e-02, -1.9870e-02, -1.4867e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.8277e-02,  5.8077e-02,  9.1470e-02],\n",
            "          [-1.6769e-02,  8.9678e-02, -4.6779e-02],\n",
            "          [-8.4257e-02,  5.1066e-02,  4.5368e-02]],\n",
            "\n",
            "         [[ 9.8114e-02,  4.9100e-02,  8.3393e-03],\n",
            "          [ 5.0068e-02, -5.1423e-02,  6.3391e-02],\n",
            "          [ 3.3483e-02, -8.6375e-02,  3.8904e-02]],\n",
            "\n",
            "         [[-1.0929e-01,  4.0431e-02, -7.2757e-02],\n",
            "          [-4.1682e-02,  6.3527e-02,  1.4155e-02],\n",
            "          [-3.7548e-02, -8.5739e-02,  6.6405e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1826e-02,  2.4016e-02,  9.0760e-02],\n",
            "          [-4.8315e-02,  3.4894e-02, -7.3994e-02],\n",
            "          [ 4.2613e-02, -8.0673e-02,  8.9931e-02]],\n",
            "\n",
            "         [[-4.7815e-02,  1.0287e-01, -6.2620e-03],\n",
            "          [ 1.4925e-02, -7.1458e-02, -3.9983e-02],\n",
            "          [-6.4717e-02,  2.5430e-02,  3.5590e-03]],\n",
            "\n",
            "         [[ 8.4055e-02, -4.5884e-02,  7.3470e-02],\n",
            "          [-3.2764e-02,  7.4938e-02,  9.0381e-02],\n",
            "          [ 7.6567e-02, -5.8907e-02,  4.2852e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6265e-02, -9.4552e-02, -2.8121e-02],\n",
            "          [ 5.9124e-02, -3.1088e-02,  5.6004e-03],\n",
            "          [-1.0253e-01, -2.1058e-02, -6.1758e-03]],\n",
            "\n",
            "         [[-8.4426e-02, -1.0168e-01,  7.0340e-02],\n",
            "          [-9.5207e-02,  3.6287e-02,  4.0523e-02],\n",
            "          [-2.6480e-02,  2.4017e-02,  1.0270e-01]],\n",
            "\n",
            "         [[ 8.4686e-02, -4.9401e-04,  1.0361e-01],\n",
            "          [ 6.5177e-02,  7.6030e-03, -8.4421e-02],\n",
            "          [ 4.6132e-02, -3.4227e-02,  3.1182e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1590e-03,  9.6360e-02,  2.2610e-02],\n",
            "          [ 1.5906e-02, -4.7120e-02,  6.9505e-02],\n",
            "          [ 8.0210e-02,  5.5363e-02,  2.5869e-02]],\n",
            "\n",
            "         [[ 1.5833e-02,  4.6309e-02,  6.4993e-02],\n",
            "          [ 8.8030e-02,  5.9004e-02,  6.7174e-02],\n",
            "          [ 2.5284e-02, -9.0484e-02,  4.3538e-02]],\n",
            "\n",
            "         [[-4.1500e-02,  1.6483e-02,  3.7622e-02],\n",
            "          [-5.1351e-02, -2.3514e-02,  4.0392e-02],\n",
            "          [ 2.1119e-02,  8.3740e-02,  5.1940e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.7783e-02,  1.9386e-02, -1.6537e-02],\n",
            "          [-7.9446e-02, -2.7863e-02,  3.7120e-02],\n",
            "          [ 1.0243e-02,  7.2668e-02,  3.6054e-06]],\n",
            "\n",
            "         [[-8.4458e-02,  9.1324e-02,  1.0927e-02],\n",
            "          [-5.9573e-02,  1.5003e-03,  9.3048e-03],\n",
            "          [-6.5948e-02, -1.0064e-02, -1.5576e-03]],\n",
            "\n",
            "         [[-6.9235e-03, -1.0127e-01, -3.4975e-02],\n",
            "          [ 5.6556e-02,  4.0783e-02, -5.1582e-02],\n",
            "          [-1.0720e-01, -6.1920e-02, -5.2611e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.3448e-02,  7.7685e-02,  2.1998e-02],\n",
            "          [ 2.0175e-02,  4.3324e-02,  7.4781e-02],\n",
            "          [ 6.2929e-02, -4.3709e-02,  6.8528e-02]],\n",
            "\n",
            "         [[-8.1932e-02,  6.6333e-02,  7.7101e-02],\n",
            "          [-8.1285e-02,  4.6168e-03,  8.6806e-02],\n",
            "          [ 2.0925e-02,  6.8032e-02, -7.0257e-02]],\n",
            "\n",
            "         [[-2.6099e-02,  8.8338e-02, -4.8360e-02],\n",
            "          [-6.9208e-02,  3.1006e-03, -7.1624e-02],\n",
            "          [-7.6128e-02, -4.4510e-02, -6.0263e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.5998e-02, -5.5184e-02,  5.5137e-02],\n",
            "          [-2.0125e-02, -9.6971e-04, -2.1188e-02],\n",
            "          [-4.3476e-02, -5.7339e-02, -5.9656e-02]],\n",
            "\n",
            "         [[ 6.7297e-03,  9.5375e-02,  1.0761e-01],\n",
            "          [ 1.2648e-02,  7.1523e-02,  1.0179e-01],\n",
            "          [ 1.6131e-02,  5.5761e-02,  4.9215e-02]],\n",
            "\n",
            "         [[ 7.2006e-02,  9.2984e-02,  7.4524e-02],\n",
            "          [-1.8908e-03,  7.3498e-02,  4.7985e-02],\n",
            "          [ 4.7822e-02, -3.6926e-02,  3.6100e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.1737e-02,  2.4287e-02,  3.9998e-02],\n",
            "          [ 2.8551e-02, -6.5255e-02, -8.2598e-03],\n",
            "          [ 6.6954e-02,  7.6252e-02,  7.3751e-02]],\n",
            "\n",
            "         [[-8.0022e-02, -1.7766e-02,  5.7446e-02],\n",
            "          [ 7.4520e-02, -8.7311e-02,  8.5213e-03],\n",
            "          [-5.8123e-02, -1.1874e-03, -4.7351e-04]],\n",
            "\n",
            "         [[ 2.5572e-02, -7.6246e-02, -3.0540e-02],\n",
            "          [ 6.5146e-02,  3.4799e-02, -1.3767e-02],\n",
            "          [-5.0314e-02, -7.5929e-02,  7.4017e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.2021e-02,  9.1438e-02,  5.9851e-02],\n",
            "          [-9.4594e-02, -6.7057e-02,  2.6643e-02],\n",
            "          [ 3.7360e-02, -9.6113e-02,  2.0946e-02]],\n",
            "\n",
            "         [[ 1.6829e-03, -4.9812e-02, -4.7245e-02],\n",
            "          [ 5.9117e-02, -1.1837e-02, -4.5306e-02],\n",
            "          [-2.1126e-02,  4.1204e-02,  4.0800e-02]],\n",
            "\n",
            "         [[ 1.9352e-02, -1.6765e-02,  3.8970e-04],\n",
            "          [-3.7234e-02, -5.1284e-02, -5.9600e-02],\n",
            "          [-4.0192e-02, -5.3321e-02,  4.4797e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4804e-02,  6.4502e-02, -8.4007e-03],\n",
            "          [ 8.3067e-02, -3.0872e-02,  1.1521e-01],\n",
            "          [ 5.9833e-02,  4.8045e-02,  6.8831e-03]],\n",
            "\n",
            "         [[ 6.1215e-02,  2.9474e-03,  5.8626e-02],\n",
            "          [-8.9076e-02,  6.4592e-02, -1.3761e-02],\n",
            "          [-8.3660e-02, -3.2324e-02, -6.0255e-02]],\n",
            "\n",
            "         [[ 6.8825e-02, -5.4625e-02,  1.5231e-02],\n",
            "          [-6.5532e-03, -8.4055e-02, -7.1841e-02],\n",
            "          [-9.2062e-03, -8.8333e-02,  4.9683e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1812,  0.2429,  0.3345,  1.1293,  0.2481,  1.3111,  0.9502,  0.6561,\n",
            "         0.7514,  0.6513,  0.8504,  0.1731,  0.7912,  1.1376,  0.0391,  0.9961,\n",
            "         1.1302,  0.7577,  0.8379,  0.4840,  0.4652,  1.1830,  0.5916,  0.5892,\n",
            "         0.3590,  0.7371,  0.2011,  0.3540,  1.0572,  0.3876,  0.8732,  0.4677,\n",
            "         0.5232,  0.8254,  1.0185,  0.8386,  0.8697,  0.8964,  0.8509,  0.1602,\n",
            "         0.5341,  1.1965,  0.2158,  0.3684,  0.9666,  0.9379,  0.4128,  0.7587,\n",
            "         0.5948,  0.4502,  0.7911,  0.0626,  0.4065,  0.2990, -0.0130,  0.1197,\n",
            "         0.5634,  0.8650,  0.2867,  0.9318,  0.3193,  0.4952,  1.0114,  0.4758],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.9844e-02,  4.5083e-03,  3.2755e-02,  1.1582e-01,  5.1300e-02,\n",
            "         1.4556e-01,  1.0717e-01,  7.4547e-02,  7.2394e-02,  6.9205e-02,\n",
            "         1.5434e-01,  2.1188e-02,  3.2747e-02,  1.0134e-01,  2.9831e-03,\n",
            "         1.1383e-01,  1.1080e-01,  7.6181e-02,  7.3548e-02,  3.9090e-02,\n",
            "         7.0725e-02,  1.7498e-01,  6.7157e-02,  6.2006e-02,  8.0109e-02,\n",
            "         1.0462e-01,  3.8163e-03,  1.0878e-01,  6.2908e-02,  4.0630e-02,\n",
            "         9.3779e-02,  6.7443e-02, -3.4771e-03,  9.4395e-02,  1.3226e-01,\n",
            "         7.2403e-02,  1.7155e-01,  5.0730e-02,  1.0182e-01,  2.7922e-02,\n",
            "         8.9653e-02,  1.0132e-01,  4.1119e-02,  7.5940e-02,  7.6997e-02,\n",
            "         4.1770e-02,  3.3582e-02,  2.3862e-02,  6.7456e-02,  3.8766e-02,\n",
            "         1.0528e-01, -7.7662e-03,  1.3061e-02,  2.8369e-02, -1.5843e-04,\n",
            "         6.4630e-03,  9.7332e-02,  1.2118e-01,  7.2496e-02,  3.7479e-02,\n",
            "         6.5137e-02,  1.0860e-02,  6.2508e-02,  5.2607e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.2950,  0.0009, -0.1196,  0.3883,  0.2060, -0.3458,  0.3387,  0.0035,\n",
            "         -0.2517,  0.0148,  0.2874, -0.0251,  0.1752, -0.1403, -0.2306,  0.4820,\n",
            "         -0.2939, -0.3680,  0.0413,  0.3912,  0.0963, -0.4149,  0.3016,  0.2413,\n",
            "         -0.1383,  0.4016, -0.0265,  0.0474,  0.0752, -0.0973, -0.1740, -0.2956,\n",
            "         -0.0641,  0.1869,  0.1855, -0.2313, -0.1434, -0.3675,  0.2354,  0.1983,\n",
            "          0.4777, -0.1124, -0.2191, -0.3167, -0.2828,  0.4483,  0.1645, -0.0977,\n",
            "          0.0691, -0.3405,  0.4918, -0.1035, -0.0978,  0.3525, -0.3732, -0.0311,\n",
            "          0.0277,  0.0069,  0.1520, -0.0147,  0.0972, -0.1546,  0.3224, -0.1744],\n",
            "        [ 0.1624,  0.0644, -0.2815,  0.5718, -0.3129, -0.0173,  0.5341,  0.1245,\n",
            "          0.3039,  0.1293,  0.3745, -0.0953, -0.0958, -0.4392, -0.0633, -0.4732,\n",
            "         -0.0473, -0.0061,  0.4201,  0.2927,  0.2669, -0.3961, -0.0561,  0.0670,\n",
            "         -0.3968,  0.0421, -0.3664, -0.3858,  0.5676, -0.2983, -0.3647, -0.1940,\n",
            "          0.0188, -0.2602, -0.4130,  0.0233, -0.3452,  0.2532, -0.4981, -0.2371,\n",
            "         -0.4085, -0.1986,  0.2854,  0.1064,  0.4619,  0.1512, -0.3105,  0.2407,\n",
            "          0.0335,  0.2868, -0.0946, -0.1465, -0.1622,  0.1889,  0.3690,  0.0973,\n",
            "         -0.0350, -0.3707,  0.3435, -0.1797,  0.1557,  0.2989,  0.4264,  0.1681],\n",
            "        [-0.0446, -0.1582, -0.2321, -0.3084,  0.2684, -0.3658, -0.2181, -0.0181,\n",
            "         -0.1391, -0.1576,  0.2394, -0.2684, -0.3706,  0.0356, -0.2946,  0.2621,\n",
            "         -0.1432,  0.4574,  0.3398,  0.3332,  0.1429, -0.1467,  0.2330, -0.2920,\n",
            "          0.1192, -0.1364,  0.0347,  0.2698, -0.3294,  0.2735,  0.0335,  0.3021,\n",
            "          0.2489,  0.0581,  0.4991,  0.3289, -0.4391, -0.2406,  0.5239,  0.2355,\n",
            "         -0.3847, -0.3263,  0.0609,  0.2282, -0.2296,  0.0606,  0.2060, -0.2712,\n",
            "         -0.3657, -0.0661, -0.3164,  0.2849,  0.3542, -0.0332, -0.2773,  0.2352,\n",
            "          0.2544,  0.0995, -0.1220,  0.2555,  0.2643, -0.0887,  0.1600, -0.1522],\n",
            "        [ 0.0121, -0.3750, -0.2245, -0.1156,  0.3912,  0.0836,  0.1723,  0.3377,\n",
            "         -0.2895, -0.2240,  0.0709, -0.2927, -0.1785,  0.0519, -0.1147, -0.3656,\n",
            "          0.4382, -0.2665, -0.3182, -0.2650,  0.1264,  0.2285,  0.3539, -0.1394,\n",
            "         -0.1926, -0.0825, -0.1630,  0.2524, -0.0620, -0.0260,  0.1285, -0.0920,\n",
            "         -0.1327,  0.1881, -0.2057,  0.0848,  0.3547, -0.2130,  0.1983,  0.0683,\n",
            "          0.0249,  0.5191,  0.0151,  0.0580,  0.2025, -0.2349, -0.1094, -0.1876,\n",
            "         -0.1130,  0.3451, -0.0352,  0.2703, -0.0718, -0.1808,  0.1879,  0.3818,\n",
            "          0.1751,  0.2242,  0.1380, -0.2353,  0.1771, -0.2679, -0.2614,  0.2336],\n",
            "        [-0.0758,  0.0244,  0.0345, -0.1572, -0.0673,  0.3067, -0.3324, -0.2427,\n",
            "          0.0224,  0.2780,  0.1859,  0.1885, -0.3159, -0.0422, -0.2961,  0.0541,\n",
            "         -0.5404,  0.6210, -0.2937, -0.0260, -0.2048,  0.8253, -0.2346,  0.0955,\n",
            "          0.3098,  0.2167, -0.0550, -0.2715, -0.2704, -0.2026,  0.3136,  0.3425,\n",
            "         -0.3064, -0.0180,  0.1738, -0.2522, -0.1807, -0.3767,  0.0083, -0.1390,\n",
            "         -0.1344, -0.3965, -0.2252, -0.0606, -0.3006,  0.0859,  0.2211, -0.3670,\n",
            "          0.4007, -0.1383,  0.3032, -0.1874, -0.3311, -0.2494, -0.0684,  0.3596,\n",
            "          0.2200,  0.2297,  0.1587, -0.0533,  0.1143, -0.0762,  0.0162,  0.1457],\n",
            "        [-0.0188,  0.2640, -0.0702, -0.3629,  0.1190,  0.8020, -0.1313,  0.3396,\n",
            "         -0.2003, -0.1322, -0.2727,  0.0410, -0.0566,  0.0455,  0.2999, -0.0051,\n",
            "          0.1286, -0.1739,  0.0895,  0.0639, -0.4250, -0.0653, -0.2880, -0.4308,\n",
            "          0.0227, -0.2388,  0.2461,  0.3108, -0.1966, -0.2399,  0.3351, -0.2427,\n",
            "         -0.2583, -0.4057,  0.1273,  0.2258,  0.5495, -0.1424,  0.1050, -0.3516,\n",
            "         -0.0316,  0.0547,  0.0541,  0.1902, -0.2075,  0.1887,  0.2357, -0.1353,\n",
            "         -0.2828,  0.2385, -0.3280, -0.0688,  0.2923, -0.3175, -0.1301,  0.2927,\n",
            "         -0.1979, -0.4066, -0.0290, -0.0578,  0.4683, -0.1462, -0.1201, -0.2280],\n",
            "        [-0.0395, -0.0513, -0.3830,  0.0528, -0.2536, -0.3503, -0.1158,  0.0647,\n",
            "         -0.2862, -0.4665,  0.5571, -0.2584, -0.3589,  0.6489, -0.2633,  0.0311,\n",
            "          0.1552, -0.1376,  0.1197,  0.0214,  0.3043,  0.4534,  0.1354,  0.1977,\n",
            "         -0.2786, -0.3904, -0.1671, -0.1341, -0.0311,  0.2690,  0.2672,  0.1216,\n",
            "         -0.2064, -0.2031,  0.1890, -0.1765,  0.4283, -0.2438, -0.2162, -0.3253,\n",
            "          0.0073,  0.2533,  0.1567,  0.2558, -0.1077, -0.1641, -0.2412,  0.1014,\n",
            "          0.1926, -0.3273, -0.1659,  0.0237,  0.0201,  0.1563, -0.0112, -0.2256,\n",
            "          0.3353, -0.3542, -0.3191, -0.0982, -0.4093,  0.1457, -0.4186, -0.2465],\n",
            "        [-0.2459, -0.3103,  0.0656, -0.4469,  0.3368,  0.6093, -0.1375,  0.1107,\n",
            "          0.1004,  0.3579, -0.3615, -0.0073, -0.0414, -0.2119,  0.2963,  0.4558,\n",
            "         -0.5052, -0.0836, -0.2444, -0.3135, -0.1758, -0.5054, -0.0749,  0.0910,\n",
            "          0.2318, -0.2550,  0.1316, -0.1603, -0.5195,  0.0441, -0.1210, -0.0881,\n",
            "          0.1672,  0.3905,  0.3774,  0.2881,  0.1836,  0.1466, -0.2814, -0.2780,\n",
            "          0.1340, -0.0502,  0.4176,  0.1432, -0.0247, -0.2079,  0.0078,  0.2683,\n",
            "          0.2465, -0.2619,  0.1477,  0.0802, -0.1858, -0.0443, -0.1689, -0.0710,\n",
            "         -0.4105,  0.3591,  0.1107, -0.3523,  0.0732,  0.2890,  0.1811,  0.3221],\n",
            "        [ 0.1783,  0.1175,  0.2830,  0.2940, -0.1182, -0.4604,  0.2750, -0.1402,\n",
            "         -0.3112, -0.1596,  0.1737,  0.1170, -0.1000, -0.3379,  0.1951, -0.3571,\n",
            "         -0.1274, -0.1578, -0.3984,  0.0907,  0.4266,  0.0308,  0.2910,  0.0547,\n",
            "         -0.1840,  0.5522,  0.3163, -0.4613, -0.0897, -0.1755,  0.7126, -0.2957,\n",
            "          0.0514, -0.4761, -0.1129,  0.2477, -0.0289,  0.2581, -0.3801,  0.0461,\n",
            "          0.1511, -0.2760, -0.4772, -0.4238, -0.0272,  0.1073, -0.2258,  0.6159,\n",
            "         -0.2907, -0.3077,  0.2260, -0.0934, -0.0123, -0.0638, -0.1325,  0.3920,\n",
            "          0.0821,  0.0777,  0.1161,  0.4201,  0.2442,  0.1538,  0.2198,  0.2924],\n",
            "        [-0.2171,  0.3280,  0.3260,  0.3332,  0.0484, -0.4031,  0.1232, -0.3011,\n",
            "          0.4981, -0.0499, -0.4399,  0.0686,  0.0516,  0.2249, -0.0657,  0.0377,\n",
            "          0.6491, -0.1312,  0.2328,  0.0223,  0.0934, -0.5656, -0.1599,  0.1472,\n",
            "          0.0627,  0.3443,  0.3426, -0.4242,  0.0026, -0.1327,  0.0741,  0.4396,\n",
            "          0.1281, -0.0693, -0.2367, -0.4895, -0.1062,  0.3107, -0.2242, -0.2356,\n",
            "          0.3530, -0.1071,  0.0661, -0.0711, -0.2434,  0.3776,  0.1985,  0.1740,\n",
            "         -0.1782,  0.2323, -0.1753,  0.1938, -0.0718, -0.0778,  0.2875,  0.0028,\n",
            "         -0.4345, -0.1989, -0.3246, -0.3520, -0.2539, -0.1638,  0.5279,  0.3107]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0134, -0.1618,  0.0033, -0.0195,  0.0395, -0.0571,  0.0166,  0.1196,\n",
            "        -0.0108, -0.0090], device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twkPKC459Vm7"
      },
      "source": [
        "# Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r94SvQM1Ryl",
        "outputId": "7a2cfa2e-4d31-4cc1-cdf8-0e9e50813185"
      },
      "source": [
        "#First load models, dataset\n",
        "\n",
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "import sys\n",
        "sys.path.append('/content/open_lth/')\n",
        "\n",
        "from foundations import hparams\n",
        "from models import registry\n",
        "\n",
        "model_hparams = hparams.ModelHparams(\n",
        "    'cifar_resnet_20',\n",
        "    'kaiming_uniform',\n",
        "    'uniform'\n",
        ")\n",
        "\n",
        "model_A = registry.get(model_hparams).cuda()\n",
        "model_B = registry.get(model_hparams).cuda()\n",
        "\n",
        "from datasets import registry\n",
        "\n",
        "dataset_hparams = hparams.DatasetHparams(\n",
        "    'cifar10',\n",
        "    128 #batch size\n",
        ")\n",
        "\n",
        "#set the platform\n",
        "from platforms import base\n",
        "import platforms.platform\n",
        "import platforms.local\n",
        "\n",
        "platforms.platform._PLATFORM = platforms.local.Platform()\n",
        "\n",
        "train_set = registry.get(\n",
        "    dataset_hparams,\n",
        "    train = True\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_A = nn.CrossEntropyLoss()\n",
        "loss_B =  nn.MSELoss() #nn.KLDivLoss() #nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.SGD(model_A.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer_B = optim.SGD(model_B.parameters(), lr=1e-3, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'open_lth' already exists and is not an empty directory.\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7KmJC3N1bNm",
        "outputId": "11c41be1-a9cb-4ace-ace7-28dc42a0d012"
      },
      "source": [
        "nb_epochs = 4\n",
        "batch_size = 128\n",
        "#output_vectors = torch.empty(nb_epochs, len(train_set), batch_size, 10)\n",
        "a_i_A = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_A = torch.zeros(len(train_set),128)\n",
        "\n",
        "model_B_on_A_acc = torch.zeros(len(train_set),128)\n",
        "model_B_on_A_acc_tilde = torch.zeros(len(train_set),128)\n",
        "\n",
        "a_i_B = torch.zeros(len(train_set),128) #batch size = 128, about 390 batches\n",
        "a_tilde_i_B = torch.zeros(len(train_set),128)\n",
        "\n",
        "forget_matrix_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of A\n",
        "forget_matrix_B = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt image classification\n",
        "forget_matrix_B_on_A = torch.zeros(nb_epochs, len(train_set),128) #forget statistics of B wrt A logits\n",
        "\n",
        "softmaxfunc=nn.Softmax(dim=1) #softmax to check classification\n",
        "batch_tracker = 0\n",
        "\n",
        "#initialize previous logits\n",
        "model_A.eval()\n",
        "#first_x, first_y = next(iter(train_set))\n",
        "\n",
        "l_A_previous = torch.zeros(nb_epochs, len(train_set), 128, 10)\n",
        "\n",
        "model_A.eval()\n",
        "model_B.eval()\n",
        "\n",
        "#initialize previous logits\n",
        "# for batch in train_set:\n",
        "#     x, y = batch\n",
        "#     x=x.cuda()\n",
        "#     l_A = model_A(x)\n",
        "#     with torch.no_grad():\n",
        "#         l_A_previous[0, batch_tracker, 0:len(l_A)] = l_A #set previous logits equal to current logits to start with\n",
        "#     batch_tracker += 1\n",
        "\n",
        "model_A.train()\n",
        "model_B.train()\n",
        "batch_tracker = 0\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    losses_A = list()\n",
        "    accuracies_A = list()\n",
        "\n",
        "    losses_B = list()\n",
        "    accuracies_B = list()\n",
        "\n",
        "    for batch in train_set:\n",
        "        x, y = batch\n",
        "        x = x.cuda()\n",
        "\n",
        "        l_A = model_A(x)\n",
        "        l_A_prime = model_A(x)\n",
        "        l_B = model_B(x)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            l_A_previous[epoch, batch_tracker, 0:len(l_A)] = l_A #set previous logits equal to current logits to start with\n",
        "    \n",
        "        #batch_size_current = len(batch[1])\n",
        "        #output_vectors[epoch, batch_tracker, 0:batch_size_current] = l_A\n",
        "\n",
        "        #check if model A's prediction is correctly classified\n",
        "        l_A_softmax = softmaxfunc(l_A)\n",
        "        if epoch >1:\n",
        "            l_A_previous_softmax = softmaxfunc(l_A_previous[epoch-1, batch_tracker])\n",
        "\n",
        "        for k in range(len(l_A_softmax)):\n",
        "            if torch.argmax(l_A_softmax[k])==y[k]: #if it is, mark as '1'\n",
        "                a_i_A[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_A[batch_tracker, k] = 0 #otherwise 0\n",
        "\n",
        "            if a_i_A[batch_tracker, k] < a_tilde_i_A[batch_tracker, k]: #and if we find that the marker has decreased, we've forgotten the example\n",
        "                forget_matrix_A[epoch, batch_tracker, k] += 1\n",
        "                \n",
        "                #if we forgot the example, build new logits which involve previous ones\n",
        "                #we will use this to rewind the logits\n",
        "                #try:\n",
        "                if epoch > 1 and torch.argmax(l_A_previous_softmax[k])==y[k]:\n",
        "                    #print(f\"Forgot example! Replacing {l_A_prime[k]} with {l_A_previous[epoch-1,batch_tracker, k]} \\n\")\n",
        "                    #print(f\"Was it classified correctly before? {torch.argmax(l_A_previous_softmax[k])==y[k]}\")\n",
        "                    l_A_prime[k] = l_A_previous[epoch-1,batch_tracker, k]\n",
        "                #except IndexError:\n",
        "                    #print(f\"batch_Tracker {batch_tracker} k {k}\")\n",
        "\n",
        "            a_tilde_i_A[batch_tracker, k] = a_i_A[batch_tracker, k]\n",
        "    \n",
        "        #now do the same with model B\n",
        "        l_B_softmax = softmaxfunc(l_B)\n",
        "\n",
        "        for k in range(len(l_B_softmax)):\n",
        "            if torch.argmax(l_B_softmax[k])==y[k]:\n",
        "                a_i_B[batch_tracker, k] = 1\n",
        "            else:\n",
        "                a_i_B[batch_tracker, k] = 0\n",
        "\n",
        "            if a_i_B[batch_tracker, k] < a_tilde_i_B[batch_tracker, k]:\n",
        "                forget_matrix_B[epoch, batch_tracker, k] += 1\n",
        "\n",
        "            a_tilde_i_B[batch_tracker, k] = a_i_B[batch_tracker, k]\n",
        "\n",
        "            #also measure how much it forgets w.r.t. the task it is trained for\n",
        "            #(how much it forgets model A's logits)\n",
        "            if torch.argmax(l_B_softmax[k])==torch.argmax(l_A_softmax[k]):\n",
        "                model_B_on_A_acc[batch_tracker, k] = 0\n",
        "            else:\n",
        "                model_B_on_A_acc[batch_tracker, k] = 1\n",
        "            \n",
        "            if model_B_on_A_acc[batch_tracker, k] < model_B_on_A_acc_tilde[batch_tracker, k]:\n",
        "                forget_matrix_B_on_A[epoch, batch_tracker, k] += 1\n",
        "            \n",
        "            model_B_on_A_acc_tilde[batch_tracker, k] = model_B_on_A_acc[batch_tracker, k]\n",
        "\n",
        "\n",
        "\n",
        "        #2. compute objective function for A and B\n",
        "        J_A = loss_A(l_A, y.cuda())\n",
        "        #print(J_A)\n",
        "\n",
        "        #we need to first see if the example got forgotten\n",
        "        #if so, we need to feed in l_A_previous to model_B's loss function\n",
        "        #otherwise, use the current logits\n",
        "\n",
        "        #model B's loss is to compute cross entropy between its output and the output of model A\n",
        "        \n",
        "        #J_B = loss_B(l_B, l_A_prime)\n",
        "        J_B = loss_B(l_B, l_A_prime)\n",
        "\n",
        "        #3. cleaning the gradients\n",
        "        model_A.zero_grad()\n",
        "        model_B.zero_grad()\n",
        "\n",
        "        #reset previous logits\n",
        "        #l_A_previous[] = l_A\n",
        "\n",
        "        #4. accumulate partial derivatives\n",
        "        J_A.backward() #what happens without retain_Graph?\n",
        "\n",
        "        #J_A.backward(retain_graph=True)\n",
        "        J_B.backward() #bug!\n",
        "\n",
        "        #5. step in opposite direction\n",
        "        optimizer_A.step()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        #6. monitor loss\n",
        "        losses_A.append(J_A.item())\n",
        "        losses_B.append(J_B.item())\n",
        "        accuracies_A.append(y.eq(l_A.detach().argmax(dim=1).cpu()).float().mean())\n",
        "        accuracies_B.append(y.eq(l_B.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "\n",
        "\n",
        "        batch_tracker+=1\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch +1}, train loss A, B: {torch.tensor(losses_A).mean():.2f} , {torch.tensor(losses_B).mean():.2f} \\n\")\n",
        "    print(f\"Training accuracy for A, B: {torch.tensor(accuracies_A).mean():.2f} , {torch.tensor(accuracies_B).mean():.2f} \\n\")\n",
        "    batch_tracker = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss A, B: 2.20 , 0.08 \n",
            "\n",
            "Training accuracy for A, B: 0.21 , 0.15 \n",
            "\n",
            "Epoch 2, train loss A, B: 1.98 , 0.06 \n",
            "\n",
            "Training accuracy for A, B: 0.29 , 0.22 \n",
            "\n",
            "Epoch 3, train loss A, B: 1.86 , 0.07 \n",
            "\n",
            "Training accuracy for A, B: 0.34 , 0.25 \n",
            "\n",
            "Epoch 4, train loss A, B: 1.79 , 0.07 \n",
            "\n",
            "Training accuracy for A, B: 0.36 , 0.27 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WI1w1SnEEHX",
        "outputId": "8b3940e6-9cab-4254-cd9c-cb7b1eab958e"
      },
      "source": [
        "print(torch.flatten(forget_matrix_noise)[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([18., 11., 11., 11., 11., 17., 15.,  7., 16.,  4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSEzBUhDSxHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "46a51cc8-e1d6-42b9-9354-805235f02b77"
      },
      "source": [
        "import numpy\n",
        "#plt.hist(torch.flatten(forget_matrix_noise)[0:10], label = \"Events\")\n",
        "plt.hist(torch.flatten(forget_matrix_noise)[0:10], label = \"Events\", weights=numpy.ones(len(torch.flatten(forget_matrix_noise)[0:10])) / len(torch.flatten(forget_matrix_noise)[0:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.1, 0. , 0.1, 0. , 0. , 0.4, 0. , 0.1, 0.1, 0.2]),\n",
              " array([ 4. ,  5.4,  6.8,  8.2,  9.6, 11. , 12.4, 13.8, 15.2, 16.6, 18. ],\n",
              "       dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmFLnK-3GOOi"
      },
      "source": [
        "\n",
        "model_1_cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw7GwKwdIgD3",
        "outputId": "e415a64c-907b-43ac-9203-6b3322b5ac68"
      },
      "source": [
        "torch.tensor(accuracies).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4420)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB4idPHaHa6s"
      },
      "source": [
        "ep = 3\n",
        "sys.stdout.flush()\n",
        "print(\"ep: {}\".format(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nOrZ9JLIPWH"
      },
      "source": [
        "import test\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9nSyX1zHQqu"
      },
      "source": [
        "torch.tensor(losses).mean()\n",
        "print(f\"Epoch {epoch +1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VAVjdJG_Iw"
      },
      "source": [
        "print(f\"Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyO_-aOBDbOL",
        "outputId": "6b840dc6-5fbd-4b15-daa7-007d012f7073"
      },
      "source": [
        "images, labels = next(iter(train_set))\n",
        "model_1.eval()\n",
        "loss(model_1(images),labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3900, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv-K5oxwzOBJ"
      },
      "source": [
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "#transforms.ToTensor()(testplotimages).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8KoohS0XhUq"
      },
      "source": [
        "testplotimages = []#torch.empty(10)\n",
        "for i in range(10):\n",
        "    testplotimages.append(\n",
        "        torch.transpose(torch.transpose(images[i]*0.45 + 0.23, 0, 1), 1, 2) #unnormalize\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "34upjgD0eyeY",
        "outputId": "162f72db-1dc4-4c79-ff28-f80de0b7d838"
      },
      "source": [
        "gridlist = [images[0]*.23 + 0.45,images[1]*0.23 + 0.45,(images[3]*.23)+0.45,(images[4]*.23)+0.45]\n",
        "blah = utils.make_grid(gridlist)\n",
        "show(blah)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO19abBl11Xet+88v3keul/Pas2yLCQLHGNwxQYHuxyXY8cFojClSgWCSZEKNvwgqUqloEhBoAIkKiA2KWIDxoOMbWJZlqxZWLakVqvnbvXwut883Xne+bHWPmt1q5/0+vWj+91kf1Vdfd8+556zp3vOWutbg7HWwsPDw8Oj8xC62R3w8PDw8Ngc/APcw8PDo0PhH+AeHh4eHQr/APfw8PDoUPgHuIeHh0eHwj/APTw8PDoU1/UAN8a83xhz3Bhzyhjzma3qlIeHh4fH28Ns1g/cGBMGcALA+wBMA/g+gE9Ya49sXfc8PDw8PNZD5Dq+ex+AU9baMwBgjPkigA8BWPcBbozxUUMeHh4e145Fa+3AlY3XY0IZA3BB/T3NbR4eHh4eW4tzV2u8Hgl8QzDGPAzg4X/s+3h4eHj8/4breYBfBDCh/h7ntstgrX0EwCOAN6F4eHh4bCWux4TyfQB7jTFTxpgYgI8DeHRruuXh4eHh8XbYtARurW0aY34ZwP8BEAbw59ba16/9Sok3tfzub/xr6pwS2GPxGH1oNwEAxraDY0k+1mhUg7ZSqQgAKBSLQVur1aL/0bj8mgBgwgCAtZV80JRKZQAAYYSDtmqZ7pFIUr/j8XhwrB2OAgBWVtek3xEDAOjt6ZZ7tWgMzVoFAFBvNYJDiyvU3/nlUtB2fpE+L9dN0Hbs8Eu4HG+eR4+NoHqVNj+Xm8OVc+nncXO42p68Oq7LBm6t/SaAb17PNTw8PDw8Nod/dBJzM7h04lUAQKtZC9oiUZJuUwn6H22RwC1LsLZdD9rKZZJaw2EZYpwl7kqT3nB1LT2TcI7FxaWgLRqK8T3TQVutRn1qsSYQiYgVqtIkCbnZEs0hGqbja0m5VypBn42la5mwSNamRuMqz80HbSuzBQDA2cUKPDw8PBx8KL2Hh4dHh8I/wD08PDw6FNvShFLIrwIA0ulk0NZokpmk1SZCMZNJBceKeSINW4psjKayAABj5B2VyLIphM0gOo1ArUZmlXRKyMZ4jO4fjUSDtnSWrltvlAEAkaiYPypL1BaNirkkziRmU5GuBUeERmn6k3EZZzJMfRoalPMblsYgLcDxuY0THR5bh94BMaclk7RuxtAaR0Ky/0IhWrOms83RiQCA1VUhyhNhMtOlQ7QXClUxk4XStI+SimxPp+n+Xd09QdvKMpn96iXe16q/jToT5LJNEY5QP2NR+W10pYlwHBnsBQBcnJ0NjpXqNIZcTu7ZbNBdSiUh7E+cOgOPGwsvgXt4eHh0KLalBL7KkkQmI5JH3Ek3TEDWCkJwhhyhGRIxo94gQjOupNsWk50xlkAaVXWNJpGSmYRI9mV2RWwqmSbJJGoyTu++bE4ksrU1ksCbbXELzKZz1FYXySq/SgRrhYWXWrkcHAuHaCxRRdIO5eiekYhIQMfnVuBx4xENi5TdYq2w3aK1MjHZrzXeT07apRNof3bnZI/lWKKuF2hPtMtCxKeitHe7UrKHU+y+momJVrhYoX3cZlI8kRANcGCgHwCwsiL7xbnAjo4OBW1hOM2PJPBoUlwA3zhP8XkxpW1295CLbUa2Lk6cgscNhpfAPTw8PDoU/gHu4eHh0aHYliaUiwukly2tShRlJkVd7U6T6tiTERUvxAxNLCrDSScS3JYL2socqZnk77ZqQgS2mWQs14R0qrGvd6EipFO6SmpyNkVqcKUq5hJHGLVaYv6I2Ax/EJUXTGymWA120ZoAYJhYDStVPZOkezZDcq/N4g++9mTwefro9wEA82eOcr9l/oYnbwEATO65JWjrGdkBAEgk5bwTh58FAJw7Sb779YKsWYSvl+vpkjY2Ud33Y+8J2vbsp3tU14iMe/3QD4Njbfbtr6so2yOHDwEA1lYXgjbnn9+o07wtL4luXyzRd5stMZkNsKng8W98GdeCmPL7dwR5T38fAKBUkXtGW9SPJptSAFnbkeHBoG14kEwcb5wk+0N/ROZqeGwYABBqyD1DbIbJKRNHXxcR6zbMJpduIeJTaZrvcEj6MTBMWUkTygzjHAcalvZYd7f0Y6zJe1I9LSJRaouHxVxzJf7H18WmYq5y3O1xR/gCQgi788PKfBlyJlLF0gbnm6vcgc8Lt+W3FDgdGPkttcOX3xMAopbbmm13o+CYW46Wso41Qc+NiJHffoj3W71K+69SUyZbHvNDH3znm/t9DfASuIeHh0eHYltK4KfPk2QVVjlCdk6SVGFa7Nqn3AOzGSKCktn+oC1iSYKYm5W33rkFSqmb7CapJGpFKsmX6F4N5faVS7BErVwAS86ZjwmssBXJ0DoBoSVEVL1a4GMi7Zg2iQbRKF0rqcivEL9To4r8qtVpDMmEeuVvEvlliTTt6ybJ0Q4QmWUjoq2M7NgFAGgpQjbUJgmzXZZ5q64s0ncrNA/j/SJdTk7uBQBM7N0ZtI2OjwMABgeHg7ZolKVVXpeJ8ZHgWLNJc1mtinS7ukJzurAgY4nEWSLlnDY9fSIZJtJEIK/lhciLJza39btyWbkuS8GDQzR/84uLcoyJxLXl1aBtaIDmJh6XdUwmaV+MTdJ8pNOZ4FijTvMcgyLzY3TdckVI8YkxXj/eTzrHT71O89ffL1J5hInyWk3y7TgyvsL5eQpry8GxGmulff2yP5Jpmr+IWV8Cb6nfktOSr1YBrK0IeyeZmkDYVhK4u4b6ruE2nefU3SPEjg8tJVsvc56idlueC5kg55E8U/JVRwjT8yaclDm1fL12W27a4g67+QaARon2aZTdNSNxeQY0m8q99DrgJXAPDw+PDoV/gHt4eHh0KLalCWXX7lEAwFBOiJo4RyhaJrOMevckWTXpSomKUmR+K1KXa9x94C4AwLOvE3m3li8Exxps/4gbUW0GWQ1vKu7wIvts10P0fyYu548zIdWdE1XTsAmiq0tUWGf+qbMjeK0s6lwq6XyEFbHDH7PKL37TaMhg6jX6XGbf4537x4NjxRKp15o87B2gMURUBN/effsBAO964D4AwNiwXKOri0wGjYjMUSpBY4hoIootMhVWOWuqj6kkqfY93WKa2b37VgDAkSPH1UVoDLVame/dGxyK8rSt5eeCNgtRda8F/QN9wWen+juSamhE/KpTrHrHFfM3MkDkYaMh5qClRepTlveMntt2nWMCFMkdCtHEVcoSAeksBCE2sdVUzEGtTn2Lx+V3UMwTKZ9W0cwu1fLSMplO4lGJb3DmjHpd9mmhSGsVuio9yf1RZk531tVMKLrNfXakZFtfPiAS1fnuXooIdZ8rvC6nTkk1stNnTgAAYjG5Ri7GZrG2XKPcoLGms2ReCcVkHWPskKDNNjU2yRSUma5Vpd/QxDjtXb13otEt+C3DS+AeHh4eHYu3lcCNMX8O4IMA5q21t3FbL4C/ArATwFkAH7PWbllo4J5xkvRGekWSvXSJosEizvVOEUEJl2K2JdJis0FSSLkg0pzhlK6RkHvLyys0xcUY3rVXJMiP3rsbADA9I65xf/gtkt4vlkliSUUVocd5IfbuFDJ1dIDe4I2GSEURTjGb6urifoiYUeKoOqOWJpVlSUm5IW0WTZVrwzCR4nK+rC0ICdc3QvMwedveoG1wkjQjnesFTDI2OEL22CUhFsunSQ1qhETaPX7oFQDAOw/eGrS9+0dIenfSVz4v0uX5s5cAALGoSJCxGM3bwIDU0D53niUrdlMslmXN8nkal85bk+sS6fNaEFIZaZzk3eJ1aYaUCyqTrm6tASC/StKtgZxnWfK9eInG2ZUVkjQVoXnO14QIdXMUS4i22XBrwBKyUdJom9e4HZZ9Gnfug0qCLDMJHYvTvOj5TiVYO1Xk6Noq/dzXVpQmcAUuk7avInm/1XecJ64NKYLTuQUqF1vDroUlFc28yPv49BnKzXL+0qw6n66XhsxfpUptVeVCXGF3ytI8rUvYyt5JmAi3Sb9bYbpGQxHDzudgLU+/iTbkWZRRZPX1YCMS+OcAvP+Kts8AeNxauxfA4/y3h4eHh8cNxNtK4Nbap4wxO69o/hCA9/DnzwN4EsCvb1WnpjjjW6Uib/e+HpIMXPCBtnm1OZPbalXe1tN5euueXZAgnPYKuyjxqLMRkcKGusl+uTcnttYEu9wNRkSCzCXoHuE2XcSqd+Bynt7gpy+IFDo0TO54sZi8wVdZarEceKSzDDbZDa5aE4kpyW/6iA4G2iRqZZEQMuwGl+ujMd9z193BsYndJHkXVCDK8dMXAAB5Je0UOcfG0iqN+dKMuJ91Obt1SDSHr//vvwEARD8h8/ZP3vUAtbHUNzwskjUsSfGry8JX/PAHFDQUUZpAOkdSuSumUS+IQuiE4IEBsYu3mpuzgRsltsbivAdYamwqt9caazo9SZG0oiGXtVDWscqBRzEOPKvXlAvqGq1VLCu5UGLscmqiOicLu6Oy9tFQrmxZziCYSIhEbZjncXZs+k6Lr5t40/mON9FcTatGkxqLrC9JNpV7YODupwNuXNDaZU32svPbysZe5724Oi8BXAvsunn+wnTQtspBSc0GjSmkspQmWJstKnt+q0lrVSxJW83Sdy2L/T1p4QRi/NhMqOvWQd+NRmStQuymXGcezKqxrBa3pjjLZm3gQ9baGf48C2DorU728PDw8Nh6XLcXirXWGm1MvgLGmIcBPHy99/Hw8PDwuBybfYDPGWNGrLUzxpgRAPPrnWitfQTAIwDwVg/6y77DarCOO3S5R5wSYpThwTi3M+3Ww+qWVUxDhrWbVFCoQdTEgV76vFAS88Bjl0jJiMTEjNDDEWu7ORKtos6vcuRcSREZR06TKedWdrcDgEwPqcF1dvGyym0uxBGYEZ0LJUNqardKqA88jc0grqLBGmEizCqs5r+xJmN55akXAQDLS0IGTl+ksUQVMRdl4q4WREyK+j46SHM0N3M2aMsxEVZQ5NeJM+cBACOj5GYXVTltRiYoKnN0QqIzz8/Q+ccPnQ/ahkbou2+cZyK2IfvDueO1lDtjIr5+BOFbQZvuLEfiJTnfSFXlwYjx/miVVOENtt2NDInC2lzi/cnzl45Jv2oFMnF0DYv7WVmZrxz6h8hUVSvSNcIq6tcRzrpoSKVCaxqPiQkxFKP9tsZmhEZD5ircon2t1xacXySZWL/yfKOlXWHplxvS7rGOqVR2gCjXvq2xGUibNU6/cRoAcIkJXwAoFWk+aqq/LXbFbLE5JqdMbdU6tRUUYemiJ2vKSSDG65DlIjDaCcHVyEjGZOxtU+ExyxwFz6pgz6h8KspEej3YrAnlUQAP8eeHAHxtS3rj4eHh4bFhbMSN8AsgwrLfGDMN4LcA/DaAvzbGfArAOQAf28pOuSxvOkjAlatyEpAmahpMQkSUq1QiQm/TsSGRRm45QK5xU5Pktnb6hDj412tEwrVa4rKVZ6Elp4jNvTl6/e7mwIhjp4Q8qXIgSlxJJSuLRKaVxDsR/SwtGiZpo+o96oJDmkrDiDuXsetPhYJUSqS/+VWar5PniZx8/fBrwbEQS8GtmmgHlTxpFmHl2lWp0RhWC0QWF4oisZ+9cAQAkE6KO+j+PQfogyIRn33qCQDAjl1TAIB9B0Rb6esjl1Kdu6Sri6SjUFOk+BKTahWW2CqrQtC12L00kVSBXmty/FpwcV7u6Qi3NJPnmW4h9KpMCmbCIvmOj5AGFU/Lvg4z19qTivH/sneyI+SOWguJFnl8ltxpdbbAWomI4yrnqImqezby1KYz4bWZKA8rIrRYpPXjnxLqLbnnAOeo6VMa4Ik8ScN9vUIMX4lGW+bbrV4oqh85XJZNubauzJCWd2Ga9mShKvO9uEyKfkiVrnPPiogmdd1vh5tKKmirzpJ6pSHz0eS5iRi5Rtzl52FX0WpF9ks8QevcVuviPunAI/exylqTdqvU+V+uBxvxQvnEOod+Ykt64OHh4eGxKfhITA8PD48OxbbMhRKkg1TqSKvB5hHOIWCVL63zoQ0bFanYJFPIsIrmvPOugwCA/gFSBfsHhdw4c/R1AEB3WsiyCzNEYmZ1Kk4uItDTRWrU6OhdwbGLx4n4SyVEVTp9kf1Jm6KClar03py+QNfvSkkfM0zW6nSXRdZr7cY44LdEd+9A8PnUecolMvMGRaylonLPNVbLi2vCTxtW+1YLMpZVjuBzqTL7h8VEk8zSvI1NyRxNcnjamVeeDdrCnMekwVGJOk3s7XfQmu3Ztztomxghk1bmXfcEbYeOkjmsVqW9UI0KSdUGzW9bpQ+enRUi7FpQa4rqu8x5Q1JlmoPehqjqUecrnFVmlTKZKYoqHa/Ts8NMutcKQnoO5Oi7x05ItXdXszWbFDOJK+3aO0Jkp2mptKXsu62z5+arNDcJlR9ldo5MM+B0zZkuMZdUuVBFU5HtySStYza9fk6PYlXGYji1bFUVUSmyH/rFi0JGL7CPd4uLqYQSKu0qk8Rt5W8fYTtJyOhHGZPWHPFabuj5dv7l0hYLCqzIWJyThHM0iKjLu6jWilGmGe5T5SrR0lX2Ob+MAH/TWZuDl8A9PDw8OhTbUgJ3r6eIfmO5N6fLkaC6Xmu68kfyXuvmxPtRlVVtmvN0WJY006oC/eQuIowGB4RtnDxIUZRtVcpsdXWMzyNJc3FJ8ix0xYnQ2T0ubl/Vb1G5shMX3wjaki2SgtfyJFkvLYlUsm/XTgDA6KBIQC3OXldvXv97+/TpF4LPR09RyatLl+j/VkEl+O+iedu/b2fQdvvB2+n8BSGdzs3TdwZGqCDBzj1Tco0+mqO5FSE27QLNw/mzSuriKE6XHuV9+w8Gx0ocsaZqA8Aygf36c88FbXsPUBTp0BhJ/S+8+L3g2MwsSb4NJYlVK5vLKzPYK7lKmlUaVzZDmpxVxGyYS68ldSEAXr6Sune9SRKkI2lvOXAgODY7Q5kKazVFKHJRCB312ea8Hil2eauXREsIJ+m3EVZ5WkpcZGKtLIR9F2dDLJZdLhK5fpxd+xoqKnd8cpLvvX42wpdefib4XGVpvFySPeYyJTatLhXI0ao8f7auchld5VbNFo9PMfwt/m6TC7EY7abI7rkJq8qsMSkaNppY5H5wFsKmkthd5sF6VRGh/GCqKELWldNzpeDCZusft14C9/Dw8OhQ+Ae4h4eHR4diW5pQopywqq3UEZeQvsTRjg1Vjy7B5EM0Iurq5E7yJc72CynZNUJqYjpOallKsaQ9XDm9HRY1uDfN5ChEbc5NkIrbP0KmlMVnvin9ZuKne0zMCGPjNIZzM0KaXTpD5GWLVdOKSsJ19hyZFtJhqRmZZqIotAVJ4J9/8tvB58gQjWX3wTsAAMm69OPgrfsAAPtVkYdWlVRNG5J1KcGlauW5Covpp9Ek00KxIKRkN69fU/kZn58jZ+hEhnzqu5S/8S42yeikYZVVUsOPvSDV622FTAS3feCnAAC337krOFauky/x6ZNng7ZUStb0WpBRdUlv2UNmhGSKTBchVbxh9jytd7MpanY6S+aPlaKYzMKGk1Oxyl5Q/unzTOg1Lsu7RXtG+9u32QThzBOFvFy/i8dZV6lMLVfQCCsTZS6b47FwrUtVkzXHhVXCyv/a+TGfOSemsCtx9o1X5I8wj1Ndw+WHbYd00itCEFipfaeZ5NamlChHhBpVjKHBZH+Dv9tW9rcY76OUWiu2wKKl+uYIR8uJvxpWJbpqvrlWboNNPUablNiv3HAq2pYyFV2tsMVm4CVwDw8Pjw7FtpTAq+yu1NbEhHth8ZszrPJx1LmS9uCkuMjd98EPAwCS3SKBN9ok6XWH6W1aXpbUpyGW+nNDQkC2mP2IxSUSMwciPpemye0qE5FItEMnSVQKpcXtcOzO91DfLonkWzxHEmmyh6SeVSWRlYskLUbCklI1EIbM5lKgasyfl6INd99FKWPjcZq3PiUcjYxR35ZVROOFkzRf9ba4X4ZYQglH2HXLqtwfTY7mVJqUbbmoRSl6scTkaSjGEW6XSSf8WfFLmQRpSztHJ4O2BOe8CYEk09tvFwm8u5sk+q+V/z5om53ZXP2RTEwmKc3VzGO8d7p6ZC8wd4iVJZnvw6+T22ZTSYvxOF2jL03fvcT7CgCWuDBBtSnufmtOQlfMnBMEV7hghJbYXXraVEr67aJbjbpGlQs/uPwumoyznCq1qUhMlzdEV56/Eo1ZKXkX6qG1MnFxKgg6riTwcJRLJ7Zo79TVI8q4HElKBHeutSHlmBdmgjLBGmutIsSp07SzMZGGz505BgBIZpQ7bw/tT8Ohla2KjN1FW1bU03N2mZwZ6jXpWzLDWrTTdEJyjeZbzNu1wEvgHh4eHh0K/wD38PDw6FBsSxNKnaPdQop4qdZJDbJMDGhCz8ZIbQ+JBoTRfUTQJTMSwbc0Sz7Ii/NUP7GrZ0dwLMYRkBFVzdxVqoGRC1cXiVi6eJr8jFOqAngPVyd59YWTQdtHfvEXAQDvrIvKtPq33wAAXFoi9XZe+fk6U1HLKFMEMzrtxvUvVyqjqrXzvVZXKdoy0SumnzKvgQqmQ7KHCLG4ztjDUX2Wu1ZVFddd8qiQMv246kmZPjERxSyZlMJJMnVYZaZoG1p305KIRkcWRjOyB5Lsi92skYlhaVr88/sytI4f/uAHgraXXqG98NTjj+FaMKEiTZ35oKeH+h1WyZCiHO07Migmue88/iSNqS3ndWdpLmcu0UQP94q5pJuTY63OyR5bnCMCvLtXklk5krubo46zaVnjbBetaTqrozNpjc6clGRuYXYAKLHJpa6SxdU59ao2W7q6nm+VTjZ/SWIfuhO8t1Sq25CLtoTyneb0sNEo7TUTEnNdk02qdaN++2kac9+gkP4pNlkk2NSiIyCjKfbZL4lTwR7+bkvtazcfrgJ9Qsm68/Pkn9+MiOlngCt6wao9yema8xzJuqLSTLfWd5+/JngJ3MPDw6NDsS0lcOf2Fc+KNNc1SG/wCBcECMek66E2ESMq6BL5Ar0lkxkhNudmDgMAnnuecpbcfvs7g2N79tIbtF2W69Y5d0YsolJxVqltmJPor56dCY7tGqd7rRyWFLN5Tp5/y4MPBG1LHPlov8+kVkmkjMUlTj1ZUfkeOLKs1rr+JPAjO4Tcc4nmq5yyczYvY48xidNoqvwQTNaVC+LC1rCczpMrqDfDMpZUF0lHQxUhDO0SSVt1FRXpXMCSKcrDoT3NXP6SVks0mBCn+rRKIiyW8nwtOi+uq8HPz/L1RRp+94N3AgB+D9cGq1zHXHEMJ5k2VJRhnAs92qiIWi0eZ0jl8Ql6yZGPO3bK+vQP0H6amJH5dvfMdYlGEuZ7zc/TvnvX/T8SHBseI02nqcjl/CJpXC7VMQAsrVDfo0wGD/aLhN9mYrOt1qCbi4wsv0Va3ojSoNPhFt9nLmircF6hnMp3Um/Q9bqz9HvMZUUrXDR0z3pG3ExzY0TEX8zL3K+eoJqp4SKRulP7bw2OFTk9cnFWtOTdXSQpr5VEEygX2S2QXR3vGhHNa3mVIljfWJI8QXaEtPl4WOWLqdH9x0bJkaIN0VaWy1Kr93rgJXAPDw+PDsW2lMBzAyRR5HpEUglHSIKos42zVVX2KkOSm42JCN4skdSyNHcqaLtwkQI/+vs5t0NJJJAfPv1dulZU8qP0DE8AAIbGxF0tydXlU5znI5YUaafeT/eaVNLAuVOU5fDguz8VtN35brK/La3QG795Vuy13TG2p4ZkLC22xzWhc0ZsDlbZaV1ukHKeC1EkZez5NXYZVPke3HlKqEQuTRL3ACf2z/WKZDjQneT+ixRVidM9l3aOBm21FmsxbD/XFePbzo1LB3uwBN6tigm0WzTnLQ4U6uqWscTY1Ww1rzSBxuYKOpy7cCH4nEnTWAu83t1x0VZc4IwL1gKAVJaDapRL2uAgjSEeorHv3i2BU/G4C+BSY+HiHklVnMLl2rAVkupqeaUhddMe6xsRiTrE2S13TE7IvRKkheVLJF3GlIYb4RweOhthmH1bW7X1XVubJZEy66skeV+6IFJrc5kk8FJSuQCyXbmdoTHYoZ3BMTNOv8PRA3cGbXm+/cV5cddEmdZ2oosk3qU10ZJrbRpLqSLS9iqfFx5R+4m1zRZrgCsx2X877r0NAHDhpZeCtsgwaUshK9pEo0B9WlmjsQ/3iBS/qoLbrgdvK4EbYyaMMU8YY44YY143xnya23uNMY8ZY07y/z1vdy0PDw8Pj63DRkwoTQC/Zq09COB+AL9kjDkI4DMAHrfW7gXwOP/t4eHh4XGDsJGSajMAZvhzwRhzFMAYgA+BamUCwOcBPAng17eiU0OjRH7l54QMXFvkeoxMpCgPHjRZnetPCmGZjbPKqHJzTO2kwgIRVrfPHj8bHJs9Q/eqVkW9bXH+hv5xcTfM5bhGI1dyv/Ud9wXHujjVaOrQEel3YE4R9Wz8DnJn2z1NY3nplf8WHJscpetXVNRWMc91/K5SkfyaocwTEU6L2cW1ASa7xDZyYDf1I6PMKmGO3CutiSmiWibVO5km9Xr/PhnnxE5S0UPRnUFbcZW+OzEqJpQDZ0jFzLELXa8qwhFh85FKfRO4LCbS0rcmr5srUxhVrmNVkPmtb0DynxQV4XgtKJcVoc15L+ocodg7ICRpm13kqoqMnuAUrEdeOxa0RbmYwCgXqRgYkP3q0psqKwxicRp8StXODEdcaCCRZZW8mC6W2eXNhoTETCbMm66Ry9LE5ctkOrMqXW0yQRvEqFxDDQ73zKVkDa5EU+3XhQtUQ7O5IqYLR9zqYgxuPlwdyZqVPo7fSo4As8q1cHmFcrGEWiraso9++73d1N9zy2L26hmg33IqKpHAYzvYgUFVbWgsc0Q0m7Gsqnq/e3InAKC0R0xV82xKikRl39XY3BVhQrQ/K/uvt0SfxXi6OVyTDdwYsxPA3QBeBDDED3fXj6F1vvMwgIc330UPDw8Pj6thww9wY0wGwN8C+FVrbd7ofATWWmOuXu/LWvsIgEf4GhtKwcWFxXH8rCrJVN2DUQ8AAB2OSURBVKGvZvm+iaTKBcFkXL8iNvMrdJFe9dYb33EvAODEoacAAG+cPRscW5qht/TUuAQE5AtE6Bx//kTQlojT9YosWcUhfRzNkSawWpJK2hF2UVo6L2Tq0C4qWPCun34fAKBaEU3j1EuUBL9VF0nPuMCV1PVzzu958N7g865bSSO5NE33HxsTCXIflzAbHhCthj3MUCiIhFdj4tEwkeaIPQDIZNjNTgQmRNnls1IUMuue23cCAHbup8yDjbZoCZaDN5qqwIBltzntStqosqsb74VQRAWdsMQJ1VZrbI4QDkVEHK5VqJ9xlkxras3iCXYZbIiq2OKgr8KyFFJwuW+mJmm+k3HZw5k07Z2uXlVl3mXCa6niEezG2N9P58/Pi5Q7s0AS9Q9eezVo27N3B58n63hphtajyXlPXEEUAIhyIpq4KsHmtN6ajvS6AirhHyIcrJNOyHyUCjSGclET1HTfwTGaj9Hb3h0cK9TonjOzZ4O22iKVm4urIJkhdjkOczX6iX7RCtNdtBlNXEjdPUOkuURUbphsnfZThvPddEVlE49zIZiJd4gb8tmLFBQVicug4wlyccyytr5jtwQVlr7xVQCA6Oqbw4bcCI0xUdDD+y+ttV/m5jljzAgfHwEwv973PTw8PDy2HhvxQjEA/gzAUWutjnt4FMBD/PkhAF/b+u55eHh4eKyHjejkDwL4WQCvGWNchvbfAPDbAP7aGPMpAOcAfGyrOtVkAiiVEEIny7kf2k1S2Uo1FTXFldGnVNrX2QUib85eEsUgw+aUC2dI3dERhU3O/bCs0n+ODJE5pa2IzTbnCKlxrca1WUlo3+KcFasrkqY2myCzxKkjh4O2pTXq765ddP277hWzxrnD5FvaqIgZxjJJZragpt477rwl+HzrPeRPW7l9DwAg3SW+506ptcrn20VI9qq0m5YT0DhJoK3JZaflK1fhWo39nfcKMZzkFKMV9kG2ITVOJvKssr65dLMtZcZz0YJ19u9ttVXuFCbGQkpeKSxtjhAeVgVCEkxYpZjoSqalP002cUQV+5pL0DruHhe6qIdJwNEh2uu6YESOC4pUQ8oPnCNN82tihkmkycQSTZF5Z3ZB9vUFHuexkxIBOTtH+29tTfmLN+jzrQdHuB9iKmo54lblcHEFCRIxxbBegWpLzo9zFHFPTvZYLEtE9uCw1AHdz7+F3gnakwtNMX+sXeIUzvOSwyVTJn/qSUX+FufoNzmyi/bY+PDO4JjhvZWMi0lkkHMA9eRkX+/jVMUZnttUQvZkksecTMg17vtRMkc6EhYAorx3sxwrEY7LfHz179c3PV0LNuKF8gywbuXSn9iSXnh4eHh4XDO2ZSTmrbcRmRVqSUL4Cifqr7J7Xagmbj2ZEr3V84o8fP67RBJAuf/EI/Q2LXLEmlWRjVl2l2uorIEuoX5Y5b9wkYnDQ+SGlFaSSoRdqzJpeZNHudRYaUUk+0aNxnD4WSrysDx7RvrBZdlWuJwWAER5mcKR6898kEyLBJRhTirNJcGghCknOOryVa4CnXbpc3xjcL7KY+L0FuXRB8tEUUYVP2hykYeWk/BUVjjLWe9Cqvydq4HVUoSidToDi/2mLVpTnKPvoi3pSLqqOnoNsGowCd4zUZbEo3HlupinfdJQ+6+by5b13i0ubMmY5WuQFB9Rrnotp86oEnZxlv4cQQwAMSY+rRun6uPrR8llsVRWpC273NVq0hYLc+ZIdtGzWrvhivZ5Fb1YKJMEGQmvX+av0RZSvHuEShxO3S5ut70H76f+9orjQJQLM1QKFE0ZKQg5uStLG/bAwb1B29QIS+wpmY+Xnn8WAHDbFD1HduySEocJ1njS6ncAdpnMZkRrS2foeCpF40uryNelJZL6u3tUTpZ++hxSv40m5zsp5Un7mT4jv/Pikmjp1wOfC8XDw8OjQ+Ef4B4eHh4dim1pQuln80SqXyLWZvLkp91Ic3RaSFS3dIVIiLU5SSAfq5K5JBcXf9YW6/sBH2F1BBinRVWvNFcDsKVqAYaYBLGs6s3PCUk6NUYE0IE7hJRcrVB/VxdUQh2Q3/X5Y0RsVlVRiCGO5utKijpnWkwiqYROm0W2W0wXXDoQjqOyNbl+jSPLSqr6eZ1NRLWqqN5NJnUb7FfdUAUZyxxNV1ZJjZy5JNsnCa5c0YHuHK17IibkUIujRWHUGrBxJpsVv+SlObpvldP3ttui3jpjgC5DmMsq5/RrgE6Dmy/R+LqzpJZXViRBlvPXTqVk/4V5z64uiqmvxv7Ga0yKN1rSb8umtsuIMd5/5Zb4nLOVCfUKtaWUL/LsDCVO0xGNNU55GlNFUcJMnpbLdLGmKugQj9F5a6pO5gynorVY3xT1T/7Fvwo+D+xiwjwq6z7NzgeVY+INneBEVL3s6z/ZL3EIoxP0eahPCPCeNN1fh0j8yL5PAAC6eF3iOfktpdg0Ui7LWBY43fAtB+S6EZ4bw8+MdktFkHLCtrQiPZ0ltVSQ89Y43eziDMWAFJblWdGtft/XAy+Be3h4eHQotqUEXmEpIJ0Tsqc4R7kUXJGFoQF5gyVYpJ6+KJLewilys4pbaevpofOmRkkKzahyUGEmfnTl7SITKDrNaoPdpyJMiO6/S0gZl/shrtKKlpdIK6ir9JEhfqt3sVtZOqFKRDVIihrsF5fIUp4I0FJtc/k7NL76lW8Gn1tcBX6FK2oX14RodWRMVaULnZul81qKxewdGOL/SXOIq/C7EkccHj8hElaeXTcndom0E2ZpJ5eja+xSx8YniOCaUmlWe5m0yyoCud3tct+QRNZQxS8c+RtWUY5DUyLZXQsWVyQPzOgg7c8CS+KNtriG9XH0X2FN3BUbTc7voaRbN5XHThLBFTKiBcU4wnJySvLGhJh5rpZEnWjx9ZqsyeliFqsrJO2fmBbXu6lB0hR7sxKNGOH8ISXO27HSVNHEHPGar8j4VpjQbNv1ZcDwsKzZM2fItW8xfzRoy9Zof9yZlbU6uJPc96ZGad0zymV1eYULQERlPvbtpIIVyYjsySwTjlGW4hFVle15fyTCsneSUdJ6SitSZi14DvBvtdVQZd/C9Dw4fkpIyaefeBIAMHdR8q7cfTsRtxNDNLdh5TQx2idzfz3wEriHh4dHh8I/wD08PDw6FNvShHLqMBF+U7tEBdufIALg4jz5T5qGkFCml9ShwV6VLjRLppP8rKg+ea77l2hT27494h9q2IG5rOrilYpkzkhmhfi75VaqxpEZIDWu3JZ+jE2SOjevElc54mJ4SHxd19aoH+kmqcGNhphGmuyTWlYqW5vrTbar11+R59vffSb43D1BEXC2SWaNl599PDi2Y4JSwfb3iRnr4gUyoTQVG5jqJfWzxgTrrKpY85P3vwsAcNedtwVt5Rqp4SGl1r5x7iwA4PgJmrfXDv1Q+siV2T/6sX8etD14G6mmMaW+j49Qf+usIhvlN+4iNxuQfociitG8Bly4KGp2lCsDNTkqeHJSojRLnJFtrSgmlGaT+hFWRT9L7Ld+5CSZCCPq2KXzdK/+Ptl/XUz4njwhNR0dof4z/+xBAEDcitmhp5t+E8m87J2lFTJdtOtirnFjWSuSeUCb68p1GkNIkctVTtJlwutHYs7zugLAQIr2yVBWTB23TVLbqDJ/3HbPHXRdNm9+/YufD469+DwloesfFP/yX//srwEA9u6Q6kKVIu0xl4TLtt5M/sdUJGaZx7qwJNGqSU5Y1eRkbdWyENQrnDTv0ceeCtqiLAs/893HgrbZ87RG995G0c9ri0JiHjq/NamjvATu4eHh0aHYlhL48gwRRQ+8Q3IkhPbTm7mxRK53+RmRKOosLKQUeXjnHiJDIlMyxConuq9ylfSZS+LaFxQOUIJZkl19wiofapFzUMxzpGRbEXqFCZLAzhwTCbw7wXUQVSGACkeMGg59bDZVGlyu8t1syvgMRyja1ubqOGp87JM/H3yOD+0DAJTzJFmfPPRKcGxkmOYvpKL6kgmS7Opt0Q7230FRcT1ctbs8INLiBzldbiorJLDLYdPWgZUc6VrlPDfzcxKldu4M5b9IpYX0mZ0msvXsYZFCQ+zidobTot73fkn1uWOKNCNNbIYS60cQvhWaVtZ7aZXW0eUs0dJ2mIsDtJWbXalCG/WyyFSey2ySzptXOVpePkTEYzopUbniwqnITnYBPHriLABgKCVaU5YjNoeHpW2Ja7Aa5Z44v0D3GJ+g81pqgWqsOZSLsv+arlap2gtXIheTa6Sz7KbblMHnwi4vifxG2xx9+torLwMAvvLVv5JrJOm8+aWLQdsf/9HvAwB+9Zel5myMtYOMWwMr615nUl47Kywwib+8KvsuzJpciVMn6wjto7z/BiYkIvTffPqXAQD/OSzX/YfnnwMAOEWxpTSBhZXrd0gAvATu4eHh0bHYlhL40jy9nWYuSsGhCZYM9kyQNHX8iEjP7QInnFfvo3ScbXPq7ds7yNJwi5z5V1eVRME5K7S90YboGnNz4gK4xNWvs+zONTQo5587+gM6f1pczdY4cGF5SQUNxTkpP0tR1YqqZM0eZsWi9DsWLvG91pd2Nop4TOboxNHXAAD5VZpnq6TLBrumFVUgjyvikVDue40SzeHaAn139pzYwL/1LXJZXFElvta4gEG2S+y0XT1k00znSNOZnhYJa7CfeJBEl2Twe/rr3wAALJ+QIgXOle7kDNkxL5Rkbfdx7owulQmvq3dzblw9ihPIcfbGBNc8W1bjTCbpWKOupD/eYxG1BjHWGuvMfcwtS7+rLK32ZiX4ZWL3AF9LBRQVaL+d5YrvsUFVsZ73fyYlGofhzIe5pKxBcZX6fvYsub3u3i+unHVOSVnXwSwsTGqp/Eo0K+KWemKOtKWG4pjS+4jLGFNcV6lA6/fc92iNewdknR5+mKTsV14WTfGF578HAHj6e08GbQNJ+p0bzlukA+Xq7KZbVuXeimXa47Wm5phoP1fZXbKsXChP8u/7F+5/r5zN2TJHJmQssUOkwVfZrTOcEE10bIq09ZcPyx7eDLwE7uHh4dGh8A9wDw8Pjw7F25pQjDEJAE8BiPP5X7LW/pYxZgrAFwH0AfgBgJ+11tbXv9LG8epxUgXjcYng+8hHqCL1zr3k+jc9K6pbnQmGWOTNtQOtYiVrZdL7ihwNGI2oqtxcu9KoYgIuiq7dENOC5Zwc9Tpdf35R1ETDdRsjUbnnWoVcwUJRMbVEG1wvkbVaTahU61zlPSP37B8mFbYnff3TW1gSs9TjX/k6AODCDOVmCTVErTz0KpsDVFrRgFhV0YLffpRcD2NcM/Cud7wjOFaP0Zzma3Ld0+dobZeWZG3rVbrepRlS38+9IdF699xDeWU+/Sv/Lmh78Tkih5oqcjRfI9W4wqrvme9LoY2nX6I1SEdERY7GNpdOtqBU7zav9+gwmXdiSTHRlLk/6ZSYAAy7LpqwrG00xu54bC4pV5TpLEn7M9Mv7rF1TqfcjMh5iW66b5vT6xYK0sd9u3fS+TOqeAmbMdaKQtrt3UuE9vR5qv/aaMoeNvyYKKqo0jbLfpm3qEpfVtXgbYnMgHVltlzJUX9PQZGHl+hH8frrZI78yIc/FBz75M/9HABgapfUlnzhOXLle+w73wvaJgfJZTfUcmYQ6XeFye6WGl+J0+rWlFnK7XrnXllVBWQuLtBvY1GlhJ2bIxI4r4jsNLt8RuM0R3V1TxvanBvrldiIBF4D8F5r7Z0A7gLwfmPM/QB+B8DvW2v3AFgB8Km3uIaHh4eHxxZjIxV5LAD3+o7yPwvgvQD+Jbd/HsB/APAnW9GpE+eJJFhbEjLw7neShJDlqucXVNkoR4ilVFBBm8kKLd2ucV6INr9xR4ZFOnKO+E2Vcc3WibhQ1bxg+byAkFLuaHG+fzQqknLMcEGCmJyXSHDVbCY32hCpuDtFfcv1SL+7++m8XHL9oImNYmRIgk327twFQIohRFS2wzBL3iGVV8Oyy2QsoZLhR0nrGR0jcvnHP/CB4FCWpbMuVRrv9deIgDpxUlwth8dJq6pyekSjcs8cPk7S+JHjUtwjNXUQAHDpoly3h5PrD3KwSSoj2tgyVzFfmha3w4UFCdq4FqTS0rcWB+G4CveRqEj1UdZInDsagTUvtYyR6OVBJrW27B3Dld/TXbJ38uzWllRztLBAJHskQr+DnqSsWYqLi2QSKuPlIO37xTn5faW4IMLgEBHKhbzkQnEedLqmRq6b5jubk3m+Eq++8GzwucU/onZF3GmXThCJ3tev8hpxtsDlZZJoh0ckAK7AJHF/vxDJYS4oceyE5HqZ5iCZMBcPaSjJusGas/pJBzVf6qoISIILsYyMMnmutXt+HP7w1UNB265b9nHba9Jf1oTyTKY2WzpwamtKqm20Kn2Y62HOA3gMwGkAq9YGLh7TAMbW+e7DxpiXjDEvbUWHPTw8PDwIG3qAW2tb1tq7AIwDuA/Agbf5iv7uI9bae62197792R4eHh4eG8U1+YFba1eNMU8AeABAtzEmwlL4OICLb/3tjaOfSRmoyMNCkdS9U8foNk88cTo4duBOyoOQ2C2+wnFLelFhWdTEepXU017OmRJLyvBdnUpVJwIpVl1tQVQryz6xhvOtRhRJ2mRVOh0Tv92wcRXXFYnUIvUpkyUVuWdAiI9MjsmhjBCsiTSpYPEtqIm5vCDEywMPUO6MB3/8PXR9VTU7wqq/jsRsc8RkWEUXugi1CofDLl2QFJvLHDWo73mGTSczc2I2ygyy8hanMZu4mAec3+63n3w6aNuxh6JyJ/pE6Usw+Zxi00WtInvnzBpF72ZU+tQWK4/zhWszpSSUP3XI0OcKm9riqmq7q3puoEhJZ2IJiy0ixwU2qmtksqhHxMQQidN8l+uibruo4Iaq51Av016cqZDZoW9cfJEbl8ickFR2wESW5mpA+dYvLpEJoreL50jZeYpNutn+EZnvtnUFINbPz6MLRpSbtTcdr3AU59y8Mlsu0O+1Zem6+TXxrV9eJFPRknIcqHCEc6Em1y/UriT7tenRzYM2XbnPsi51/i0Xpyne5LLcOi3a8888/VzQdmGWiPKlZelbhCNBk1xrU9ejbZk352fZDN72iWCMGTDGdPPnJID3ATgK4AkAH+XTHgLwtS3pkYeHh4fHhrARCXwEwOcNpesLAfhra+3fGWOOAPiiMeY/AXgZwJ9tVacm+4h4yaUkUmy4m8i3C8fIzWl1SSTas8fJXSnXFkm2j8mVsnIhiiaorcWZxSpWJcUP070ayo3QsPQZU5nzWs49kd/kTVXZ273cTUgkgDBLVMm4SFEceIiefjqvWwR2ZNJ0r5bKFNdsOPfH6ycx0ynJ67KUpz798FWiJ3RU6RCXtXOl0gBghQs0QBG9EXalG9tF0tlEj7i8XTxOUonL6ggAg8NESqX6lZbCOVZctNvIiEQBznKC/EVVhmxklKR9oyJHi24dOHNjQ2lGcXbviyuXyPrS5rLBxRSpm0rRdVsskYVVtsMwS9mtli4/R+tt1TUKeZJkK0wa6mu4QiV1tQYNdjMsr8qcxphIznJmSKisgQ12ewzHZK7irB1Yta9r/HuJM3Ha0ycFL2bWOAOocn2rcrGTSlk0jCtRLsueD+iyyzIDkoTeVNKwjgYGgMOvi7vpgz9GGuOzz0pGzdUg+lWTxfaKNvnduMhXTS6HorRW2rXU5bKJsEbntC0AiLA0HlGalCt8MjYhe9dprHHOrJjMKPIfW+NGuBEvlEMA7r5K+xmQPdzDw8PD4ybAR2J6eHh4dCi2ZTKrWw5QkpvxMfEP3bmXqlrnq0RwvW9Vzi8xQdjTr6pEJ0nlaaVE1awxAZmMkjrXlZbzC21Sc1pKVUoykZhNyHvORXDVOXlSIilkY4jVTxilSkfqfC3t103Hu/vIFJFOqfdom5ZkdV7Uz/ISqYS1tvi/AsexGcSV33GtSpP43DPfAQBYVUQixz7c2ofWRbRF1Ht/xxSlnb3tgVsBALt3CNG1ysUdZk4KsRNLkkq6u1/80Rfmaf1uP3A7XesOcXL6wl98ju8p69Io0dzUVZIi66LcOGFQWCXs38mRe/MXjgVtCG0uEjMdk/WOMOnlZiOhaqwWOcmTVtVjTNImlS+5KyyQDJH5qrImpPvwIKnjFaVu93Dq2uiAqqPKS9qA8zeWNXMEWlSRr67DDWVSGhik82K8/8IRMTvEud/Wym8plWJiLrV+Wl5bL16t9SotysE8mE36/9lnxZe8xmTu2TNClOd6ctxHWe8w/w6jTKLqY5k0mfhcnU9Ago01Ye9iNELsZx6JyZo5EjOkHeP5nlanSeYYlBYT2Y22mMJCWyQ6ewncw8PDo0NhriQN/lFvZsyNu5mHh4fH/zv4wdViabwE7uHh4dGh8A9wDw8Pjw6Ff4B7eHh4dCj8A9zDw8OjQ3Gj3QgXAZT4/05GPzp7DJ3ef6Dzx9Dp/Qc6fwyd1P8dV2u8oV4oAGCMeanTMxN2+hg6vf9A54+h0/sPdP4YOr3/gDeheHh4eHQs/APcw8PDo0NxMx7gj9yEe241On0Mnd5/oPPH0On9Bzp/DJ3e/xtvA/fw8PDw2Bp4E4qHh4dHh+KGPsCNMe83xhw3xpwyxnzmRt57MzDGTBhjnjDGHDHGvG6M+TS39xpjHjPGnOT/e97uWjcTXJT6ZWPM3/HfU8aYF3kd/soYs35KuW0AY0y3MeZLxphjxpijxpgHOnAN/i3vocPGmC8YYxLbeR2MMX9ujJk3xhxWbVedc0P4Qx7HIWPMPTev54J1xvC7vI8OGWO+4qqN8bHP8hiOG2P+6c3p9bXhhj3AuaLPHwH4AICDAD5hjDl4o+6/STQB/Jq19iCA+wH8Evf5MwAet9buBfA4/72d8WlQGTyH3wHw+9baPQBWAHzqpvRq4/gDAH9vrT0A4E7QWDpmDYwxYwB+BcC91trbQKViPo7tvQ6fA/D+K9rWm/MPANjL/x4G8Cc3qI9vh8/hzWN4DMBt1to7AJwA8FkA4N/1xwHcyt/5Y35mbWvcSAn8PgCnrLVnrLV1AF8E8KEbeP9rhrV2xlr7Q/5cAD04xkD9/jyf9nkAH745PXx7GGPGAfw0gD/lvw2A9wL4Ep+y3fvfBeDd4JJ91tq6tXYVHbQGjAiApDEmAiAFYAbbeB2stU8BWL6ieb05/xCAv7CEF0AFz0dwk3G1MVhrv22D+m54AVSQHaAxfNFaW7PWvgHgFDqg4tiNfICPAbig/p7mto6AMWYnqLTciwCGrLUzfGgWwNA6X9sO+K8A/j2k9HYfgFW1ibf7OkwBWADwP9kM9KfGmDQ6aA2stRcB/BcA50EP7jUAP0BnrQOw/px36m/7FwB8iz935Bg8ibkBGGMyAP4WwK9aa/P6mCU3nm3pymOM+SCAeWvtD252X64DEQD3APgTa+3doFQMl5lLtvMaAADbij8EehmNAkjjzap9R2G7z/nbwRjzmyAT6V/e7L5cD27kA/wigAn19zi3bWsYY6Kgh/dfWmu/zM1zTkXk/zdX4vwfHw8C+BljzFmQyeq9IHtyN6vywPZfh2kA09baF/nvL4Ee6J2yBgDwkwDesNYuWGsbAL4MWptOWgdg/TnvqN+2MebnAXwQwCet+FF31BgcbuQD/PsA9jLzHgMRBo/ewPtfM9he/GcAjlprf08dehTAQ/z5IQBfu9F92wistZ+11o5ba3eC5vu71tpPAngCwEf5tG3bfwCw1s4CuGCM2c9NPwHgCDpkDRjnAdxvjEnxnnJj6Jh1YKw3548C+Dn2RrkfwJoytWwrGGPeDzIp/oy1tqwOPQrg48aYuDFmCkTI/sPN6OM1wVp7w/4B+CkQ83sawG/eyHtvsr8/ClITDwF4hf/9FMiO/DiAkwC+A6D3Zvd1A2N5D4C/48+7QJvzFIC/ARC/2f17m77fBeAlXoevAujptDUA8B8BHANwGMD/AhDfzusA4Asge30DpAV9ar05B2BAHmanAbwG8rbZrmM4BbJ1u9/zf1fn/yaP4TiAD9zs/m/kn4/E9PDw8OhQeBLTw8PDo0PhH+AeHh4eHQr/APfw8PDoUPgHuIeHh0eHwj/APTw8PDoU/gHu4eHh0aHwD3APDw+PDoV/gHt4eHh0KP4v7sB02KbQRvYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "qjJGrcheeAg6",
        "outputId": "197c750a-8d57-4245-b2bd-70561ff9c63f"
      },
      "source": [
        "for k in range(3):\n",
        "    plt.imshow(utils.make_grid(testplotimages[k]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "d3wbJqSvYVJy",
        "outputId": "2c43d16c-8c24-499d-c99a-e8fff7caf9ce"
      },
      "source": [
        "testimage2.size()\n",
        "plt.imshow(testimage2*.5+0.,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb0cd0c8050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "cmuJlJZoDRAT",
        "outputId": "d8629860-b83c-4ea6-9e5f-dfcd22b14515"
      },
      "source": [
        "images, labels = next(iter(train_set))\n",
        "model_1.eval()\n",
        "plt.imshow((0.5*images[3].reshape(32,32,3))+0.5)\n",
        "plt.imshow(images[3])\n",
        "#img=images[3].view(images[3].size(0), -1)\n",
        "#softmax=nn.Softmax(dim=1)\n",
        "# with torch.no_grad():\n",
        "#     print(model_1(images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-00685f8544ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#img=images[3].view(images[3].size(0), -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#softmax=nn.Softmax(dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 32, 32) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbLSySZu75rb",
        "outputId": "cbd5ed06-9943-4b83-de4d-0d9844989d80"
      },
      "source": [
        "32*32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndrkqsfn78MA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AglGSm4WzhQU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYeTGXn3GAEw"
      },
      "source": [
        "#we need to define an index function that determines whether an example has been seen before\n",
        "#one idea: go through train set, assign each example a unique identifier\n",
        "#when training, find the example in the list with the unique identifier\n",
        "#update forget array at that location\n",
        "#there are no repeats in each batch?\n",
        "#since there are no repeats in each batch, we can get away with something simpler.\n",
        "#we just have to keep track of it at each epoch.\n",
        "\n",
        "images, labels = next(iter(train_set))\n",
        "l = model_1_forget_cuda(images.cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3i8dZiw_cg"
      },
      "source": [
        "softmaxfunc=nn.Softmax(dim=1)\n",
        "testt =softmaxfunc(l[1]).round()\n",
        "print(testt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsUyh--VxE6y",
        "outputId": "26d2befe-04f2-4957-9eb4-ed12fd80ffe3"
      },
      "source": [
        "images, labels = next(iter(train_set))\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6,\n",
              "        2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2,\n",
              "        2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9,\n",
              "        6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6,\n",
              "        1, 1, 4, 1, 8, 3, 9, 6, 6, 1, 8, 5, 2, 9, 9, 8, 1, 7, 7, 0, 0, 6, 9, 1,\n",
              "        2, 2, 9, 2, 6, 6, 1, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAcpvseb0nja",
        "outputId": "540c5584-1d4e-4c1b-f85a-80d11647e141"
      },
      "source": [
        "randlist = torch.rand(round(0.2*len(labels)))\n",
        "newvals = torch.randint(0, 9, (len(labels),))\n",
        "indices = torch.randint(0, len(labels), )\n",
        "for i in range(len(labels)):\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 4, 3, 5, 2, 0, 8, 2, 3, 2, 1, 5, 7, 3, 7, 6, 5, 4, 1, 4, 6, 7, 0, 5,\n",
              "        8, 1, 6, 7, 8, 3, 7, 3, 0, 2, 6, 7, 8, 6, 8, 2, 3, 2, 4, 3, 1, 3, 1, 6,\n",
              "        3, 8, 2, 7, 6, 8, 5, 1, 0, 5, 4, 0, 6, 7, 8, 3, 0, 8, 8, 0, 6, 5, 8, 4,\n",
              "        7, 7, 4, 0, 1, 1, 5, 6, 6, 0, 7, 7, 7, 6, 4, 0, 7, 1, 0, 8, 7, 1, 4, 5,\n",
              "        5, 6, 7, 1, 7, 1, 4, 1, 4, 8, 3, 6, 8, 5, 2, 1, 3, 5, 6, 8, 7, 1, 1, 0,\n",
              "        8, 2, 2, 5, 0, 5, 8, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ2lyrDk3NsD",
        "outputId": "dc794233-fb8f-4221-b7d6-e6933a32806b"
      },
      "source": [
        "import random\n",
        "random.randint(0,9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI9rHkfV0q7m",
        "outputId": "bba234aa-4829-4101-8011-26647aaaf7df"
      },
      "source": [
        "indices = torch.randint(0, len(labels), (round(0.2*len(labels))+1,))\n",
        "print(indices)\n",
        "newlabels = torch.clone(labels)\n",
        "\n",
        "for idx in indices:\n",
        "    newlabels[idx] = random.randint(0,9)\n",
        "\n",
        "print(labels)\n",
        "print(newlabels)\n",
        "print(labels-newlabels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([124,  72,  40,  67,  11,   7, 121,  91,  93,   3,  75,  16, 116, 125,\n",
            "         65,   7,   6,   5,  35,  88,  93, 114, 106, 103,  61,  57,  35])\n",
            "tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6,\n",
            "        2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2,\n",
            "        2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9,\n",
            "        6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6,\n",
            "        1, 1, 4, 1, 8, 3, 9, 6, 6, 1, 8, 5, 2, 9, 9, 8, 1, 7, 7, 0, 0, 6, 9, 1,\n",
            "        2, 2, 9, 2, 6, 6, 1, 9])\n",
            "tensor([6, 9, 9, 4, 1, 7, 3, 7, 8, 3, 4, 0, 7, 2, 9, 9, 4, 3, 2, 6, 4, 3, 6, 6,\n",
            "        2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 2, 3, 7, 3, 3, 7, 2, 2, 7, 1, 1, 1, 2,\n",
            "        2, 0, 9, 5, 7, 9, 2, 2, 5, 6, 4, 3, 1, 8, 8, 2, 1, 1, 4, 7, 7, 8, 5, 9,\n",
            "        9, 7, 3, 4, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 6, 4, 2, 1, 8, 7, 1, 6,\n",
            "        1, 1, 4, 1, 8, 3, 9, 4, 6, 1, 6, 5, 2, 9, 9, 8, 1, 7, 4, 0, 5, 6, 9, 1,\n",
            "        2, 0, 9, 2, 3, 4, 1, 9])\n",
            "tensor([ 0,  0,  0,  0,  0, -6, -1,  0,  0,  0,  0,  7,  0,  0,  0,  0,  5,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -2,\n",
            "         0,  0,  0,  0, -2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0, -4,  0,  0,  0, -7,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,\n",
            "        -3,  0,  0, -3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,\n",
            "         0,  2,  0, -7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  2,  0,\n",
            "         0,  0,  0,  0,  0,  0,  3,  0, -5,  0,  0,  0,  0,  2,  0,  0,  3,  2,\n",
            "         0,  0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl3ZJCXc26-o",
        "outputId": "2e5c1feb-7aa5-41bb-fd32-df75c65ec829"
      },
      "source": [
        "newlabels[3] = 3\n",
        "newlabels[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG5aFHma4o6M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VzSKyPqNAD9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysVxNxriNAhA"
      },
      "source": [
        "# Playing with Gradient Variances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh38XCsWPlu4"
      },
      "source": [
        "Our goal here is to see if important examples are picked out by the variance of the gradient. Let's start with something simple first before we get into ResNets and CIFAR10. We'll look at MNIST dataset + FF network.\n",
        "\n",
        "The toy experiment below aims to address the following question: how correlated are variance-of-gradients with example difficulty?\n",
        "\n",
        "In the first part, we split the MNIST training data into a training set 55,000 and a validation set of 5,000. \n",
        "\n",
        "First, we calculate the variance of gradients over 10 training iterations and averaged over five initializations of the network. Note that we're specifically interested in gradients of the model outputs with respect to *weights*. Later, we'll want to compare these gradients against those used in, e.g., the paper by Hooker and collaborators. Each example is thus assigned a VoG value. Look at cosine similarity across times?\n",
        "\n",
        "Next, we reset the network and train normally except the training dataset is pruned (for example, keep 75%, 50%, 20% of top VoG examples). For each one, we can look at train, test, and validation accuracy. We can also look at which ones in validation set it does the worst on. We can also correlate the VoG values with forgetting scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKIN5hEG0QKS"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAzFckd3NDRr"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(28 * 28, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64,10)\n",
        ").cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkwJE7HW-W6E"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum = 0.9)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_size = 55000\n",
        "val_size = 5000\n",
        "train, val = random_split(train_data, [train_size, val_size]) #randomly split dataset into train, validation\n",
        "train_loader = DataLoader(train, batch_size=32)\n",
        "val_loader = DataLoader(val, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz7H1XXa2YGQ"
      },
      "source": [
        "Now let's compute the variances and store them.\n",
        "\n",
        "Note that for each example, we're going to compute the gradients of the model outputs w.r.t. model parameters d y_i / d w_{jk}. This gives us for each layer a matrix of dimension 10 x (dimensions of parameters for that layer).\n",
        "\n",
        "We could associate each example with a scalar value (for example, we could just sum over all parameters and components of model outputs i.e. sum over i, j, and k) or a vector value (sum over only j, k). The advantage of keeping a vector \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLutbbsI2Xi3"
      },
      "source": [
        "nb_ep_gradient = 10 #number of iterations over which we compute gradient variances\n",
        "\n",
        "acc_ot = list() #accuracy over training time\n",
        "loss_ot = list() #loss over training time\n",
        "grad_var = torch.zeros([train_size, nb_ep_gradient]) #this will store the gradient variances for each example\n",
        "\n",
        "\n",
        "for epoch in range(nb_ep_gradient):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x,y = batch\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1).cuda()\n",
        "\n",
        "        out = model(x)\n",
        "\n",
        "        J = loss(l, y.cuda())\n",
        "        model.zero_grad()\n",
        "        J.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(J.item())\n",
        "        accuracies.append(y.eq(out.detach().argmax(dim=1).cpu()).float().mean())\n",
        "\n",
        "    print(f'Epoch {epoch +1}, train loss: {torch.tensor(losses).mean():.2f}')\n",
        "    print(f'Training accuracy: {torch.tensor(accuracies).mean():.2f}')\n",
        "\n",
        "    acc_ot.append(torch.tensor(accuracies).mean())\n",
        "    loss_ot.append(torch.tensor(losses).mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VOO7UIsXBE6"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRTDt9q-HYu9"
      },
      "source": [
        "torch.tensor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zewNjdIVXCWl"
      },
      "source": [
        "# Gradient scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAaB8YejKljL"
      },
      "source": [
        "x,y = next(iter(train_loader))\n",
        "batch_size = x.size(0)\n",
        "x = x.view(batch_size, -1).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksi5l6S1KpxK"
      },
      "source": [
        "output = model(x)\n",
        "J = loss(output, y.cuda())\n",
        "model.zero_grad()\n",
        "J.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUuhL1REMm6F",
        "outputId": "89833a48-58c0-4d9b-8a2c-85ac91b6b523"
      },
      "source": [
        "len(output[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "8kVdwrFfPkOl",
        "outputId": "fc5abe57-f97d-40ea-90d5-f617f3cfc4c7"
      },
      "source": [
        "from torch.autograd import grad\n",
        "output = model(x)\n",
        "test = torch.zeros(10)\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    print(grad(output[1][i], model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e0f0e7ca42a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "YyowatnDjw8I",
        "outputId": "cd806086-57f5-40ac-8104-b649ec7b35fd"
      },
      "source": [
        "grad(output[2][0], model.parameters(), retain_graph = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-d5f35c8a9822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FWKtYbrS5yw"
      },
      "source": [
        "result = 0\n",
        "for grads in gradients:\n",
        "    result += grads.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir8isxgoTcFO",
        "outputId": "6c7c4085-3581-4f17-9ee8-ecb558603933"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(52.6584, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "o2m3yHtJKpqq",
        "outputId": "412ded5d-ecd9-44b6-9dc4-f595406b9d7d"
      },
      "source": [
        "d_output_dw = output[1].sum().backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c1f914eb1e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_output_dw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQB5t9pjXE7i",
        "outputId": "89a52a2b-f23c-4f54-d521-2fecf14556cc"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create some dummy data.\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "gt = torch.ones_like(x) * 16 - 0.5  # \"ground-truths\" \n",
        "\n",
        "# We will use MSELoss as an example.\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Do some computations.\n",
        "v = x + 2\n",
        "y = v ** 2\n",
        "\n",
        "# Compute loss.\n",
        "loss = loss_fn(y, gt)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "\n",
        "# Now compute gradients:\n",
        "d_loss_dx = grad(outputs=loss, inputs=x)\n",
        "print(f'dloss/dx:\\n {d_loss_dx}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 42.25\n",
            "dloss/dx:\n",
            " (tensor([[-19.5000, -19.5000],\n",
            "        [-19.5000, -19.5000]]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVGrEHgBcQ1V",
        "outputId": "539bdcc5-4389-4a51-ef32-322836afd30c"
      },
      "source": [
        "print(gt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[15.5000, 15.5000],\n",
            "        [15.5000, 15.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUggBVq3fksX",
        "outputId": "d71be98b-28ed-405b-9841-83224e11f444"
      },
      "source": [
        "((x+2)**2 - gt)*(x+2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-19.5000, -19.5000],\n",
              "        [-19.5000, -19.5000]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT_yZ7JljTIt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1HWQAUoj8b3"
      },
      "source": [
        "# Refactoring Forgetting Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeZFyC_1j_js"
      },
      "source": [
        "Steps:\n",
        "\n",
        "1. Remove config parser, just read from command line. Separate training step and data analysis step.\n",
        "2. Remove job system, we're just interested in training X number of models.\n",
        "3. Add pruning flag\n",
        "4. Processing should rank examples and compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3-pBud94S4r"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainHParams:\n",
        "    \"\"\"\n",
        "    This class contains the training hyperparameter details. These include:\n",
        "    \n",
        "    -dataset: the name of the dataset\n",
        "    -model: the model to train\n",
        "    -output_location: where to store the results\n",
        "    -lr: learning rate (default 1e-3)\n",
        "    -forget_thres: how many times an example needs to be forgotten to be recorded (default 3)\n",
        "    -granularity: when to log metrics, either 'by_iter' or 'by_ep' (default by iteration)\n",
        "    -momentum (defualt 0.9)\n",
        "    -num_ep: number of epochs to train (default 20)\n",
        "    -chkpoint_step: how often to save (default every 5ep)\n",
        "    It also contains a method for outputting this information to a .CSV file.\n",
        "\n",
        "    It takes as input TrainArgs (training arguments) and fills out the training hyperparameters.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model: str\n",
        "    dataset: str\n",
        "    output_location: str\n",
        "\n",
        "    lr: float = 1e-3\n",
        "    momentum: float = 0.9\n",
        "    num_ep: int = 20\n",
        "    chkpoint_step: int = 5\n",
        "    \n",
        "    #robustness params\n",
        "    forget_thres: int = 3\n",
        "    granularity: str = 'by_iter'\n",
        "\n",
        "    def save_to_file(self, output_location: str) -> None:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7vG4fojewd3"
      },
      "source": [
        "test = TrainHParams('hi', 'bye', 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vVtAhM5e3ia",
        "outputId": "c057ac0c-f10b-496f-b14d-8b895b35b5ca"
      },
      "source": [
        "test.__dict__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chkpoint_step': 5,\n",
              " 'dataset': 'bye',\n",
              " 'forget_thres': 3,\n",
              " 'granularity': 'by_iter',\n",
              " 'lr': 0.001,\n",
              " 'model': 'hi',\n",
              " 'momentum': 0.9,\n",
              " 'num_ep': 20,\n",
              " 'output_location': 'test'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ4cECww4VGn"
      },
      "source": [
        "test = TrainHParams('foo', 'bar', 'dir')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4daGWOdh4v7s",
        "outputId": "b42c01e9-2d0c-4d65-c5e2-b9d89ee2cc97"
      },
      "source": [
        "type(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AZOKlZIeg6y"
      },
      "source": [
        "test_dict = {'hm': str, 'hi': str, '2': int}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC2cpG2LetP3"
      },
      "source": [
        "import argparse\n",
        "\n",
        "class Train:\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef add_args(parser: argparse.ArgumentParser):\n",
        "\t\tallowed_params = {'--lr': float, '--save': bool}\n",
        "\n",
        "\t\tprint('keys:')\n",
        "\t\tprint(allowed_params.keys())\n",
        "\n",
        "\t\tfor param in allowed_params.keys():\n",
        "\t\t\tparser.add_argument(param,\n",
        "\t\t\t\t\t\t\t    type = allowed_params[param],\n",
        "\t\t\t\t\t\t\t    help = 'help text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N1a1V7V2QQJ",
        "outputId": "0ce79c00-1dfa-4708-8a6e-7e1266d7ad4a"
      },
      "source": [
        "globalargs = {'train': Train}\n",
        "parser = argparse.ArgumentParser(description='Do stuff.')\n",
        "#parser.add_argument()\n",
        "\n",
        "\n",
        "for global_arg in globalargs:\n",
        "    print(f'Adding {global_arg}')\n",
        "    train_group = parser.add_argument_group(global_arg, 'train a model')\n",
        "    globalargs[global_arg].add_args(train_group)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding train\n",
            "keys:\n",
            "dict_keys(['--lr', '--save'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz7QGyuI_bl1"
      },
      "source": [
        "!rm -r Forget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yl8rXYM_hh3"
      },
      "source": [
        "!rm -r open_lth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w5TdkRZTiIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcb9977-10e1-4271-cc8e-4ddce90f6cff"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/open_lth.git\n",
        "!git clone https://github.com/nikhilanand91/Forget.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'open_lth'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Total 112 (delta 0), reused 0 (delta 0), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (112/112), 88.23 KiB | 279.00 KiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "Cloning into 'Forget'...\n",
            "remote: Enumerating objects: 316, done.\u001b[K\n",
            "remote: Counting objects: 100% (316/316), done.\u001b[K\n",
            "remote: Compressing objects: 100% (214/214), done.\u001b[K\n",
            "remote: Total 316 (delta 131), reused 260 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (316/316), 58.30 KiB | 9.72 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j27wB0ssDon",
        "outputId": "bf7a4e56-d8e5-4704-93e7-d32ce2d3a346"
      },
      "source": [
        "!python Forget/robust.py train --model 'cifar_resnet_20' --dataset 'cifar10' --output_location '/content/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "           Training hyperparameters:\n",
            "--------------------------------------------------\n",
            "     Model name: cifar_resnet_20\n",
            "     Datset: cifar10\n",
            "     Output directory: /content/\n",
            "     Optimizer: SGD\n",
            "     Learning rate: 0.001\n",
            "     Momentum: 0.9\n",
            "     No. of train epochs: 10ep\n",
            "     Checkpoint every: 5ep\n",
            "--------------------------------------------------\n",
            "\n",
            "Files already downloaded and verified\n",
            "tensor(0.2950)\n",
            "tensor(0.3549)\n",
            "tensor(0.3931)\n",
            "tensor(0.4215)\n",
            "Traceback (most recent call last):\n",
            "  File \"Forget/robust.py\", line 31, in <module>\n",
            "  File \"Forget/robust.py\", line 28, in main\n",
            "    runner.run()\n",
            "  File \"/content/Forget/training/train_runner.py\", line 53, in run\n",
            "    train.train_loop(train_hparams = self.train_params)\n",
            "  File \"/content/Forget/training/train.py\", line 40, in train_loop\n",
            "    batch_accuracy.append(y.eq(outputs.detach().argmax(dim=1).cpu()).float().mean())\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqcbkM6gWQOo"
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.randn(5,5)\n",
        "b = torch.randn(5,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hytfa84aWVwR",
        "outputId": "da2ff2cd-9229-4052-acff-5293638a2cce"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4259,  0.4888, -0.5155, -1.0209, -0.0493],\n",
              "        [ 0.4785, -0.8111,  1.3032, -1.1130,  1.3356],\n",
              "        [ 0.0283, -1.8231, -0.3822, -0.1252,  1.0184],\n",
              "        [ 1.6683, -2.7644,  0.9075, -0.4638, -0.1695],\n",
              "        [-0.4201,  0.8403,  0.1470, -0.2414,  0.0555]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfdjQa2MWc46",
        "outputId": "7b6ff4f7-9b4d-487f-fdc4-c94ff1cfe0f9"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2116,  1.3016, -0.2657,  0.4517,  0.3203],\n",
              "        [-0.2667,  0.0156, -0.2504, -0.0493, -1.2371],\n",
              "        [-0.7678,  0.2582, -0.0264, -0.2437, -0.7065],\n",
              "        [ 1.1704,  1.9239,  1.5507, -1.1681, -2.2122],\n",
              "        [-1.0314,  0.0984,  0.3103, -0.1407,  0.2355]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyJ1XIDgWd3Y",
        "outputId": "349bfb14-e2a9-4398-9792-2efa9b1d006e"
      },
      "source": [
        "hash(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139670894778224"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Fs2pWaWgDJ"
      },
      "source": [
        "c = torch.tensor([[-0.4259,  0.4888, -0.5155, -1.0209, -0.0493],\n",
        "        [ 0.4785, -0.8111,  1.3032, -1.1130,  1.3356],\n",
        "        [ 0.0283, -1.8231, -0.3822, -0.1252,  1.0184],\n",
        "        [ 1.6683, -2.7644,  0.9075, -0.4638, -0.1695],\n",
        "        [-0.4201,  0.8403,  0.1470, -0.2414,  0.0555]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzbAOEI8Wk_n",
        "outputId": "a5089f36-8973-4dcc-f4c1-1af91672c810"
      },
      "source": [
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4259,  0.4888, -0.5155, -1.0209, -0.0493],\n",
              "        [ 0.4785, -0.8111,  1.3032, -1.1130,  1.3356],\n",
              "        [ 0.0283, -1.8231, -0.3822, -0.1252,  1.0184],\n",
              "        [ 1.6683, -2.7644,  0.9075, -0.4638, -0.1695],\n",
              "        [-0.4201,  0.8403,  0.1470, -0.2414,  0.0555]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIKV84QaWmHj",
        "outputId": "5a926bc9-3910-44b1-c747-172d02e631e7"
      },
      "source": [
        "hash(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139666501707440"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2ikNuG0Woc4",
        "outputId": "ef536565-ac10-4882-86a9-e5b0dd42cf89"
      },
      "source": [
        "hash(a) == hash(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhIJ__QMW8MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7157b35-4c39-4c8c-aa48-5ddd73db0b54"
      },
      "source": [
        "test = []\n",
        "test.append(4)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwZ7s5Lf3xGM"
      },
      "source": [
        "from torch.utils.data import Sampler\n",
        "import random\n",
        "\n",
        "class RandomSampler(Sampler):\n",
        "\tdef __init__(self, data_source):\n",
        "\t\tself.data_source = data_source\n",
        "\t\tself.shuffle()\n",
        "\t\tself.idx = None\n",
        "\n",
        "\tdef shuffle(self):\n",
        "\t\tself.idx = list(range(len(self.data_source)))\n",
        "\t\tself.seed = random.randint(0, 2**32-1)\n",
        "\t\trandom.Random(self.seed).shuffle(self.idx)\n",
        "\t\treturn self.idx\n",
        "\t\t\n",
        "\tdef __iter__(self):\n",
        "\t\treturn iter(self.idx)\n",
        "\n",
        "\tdef get_order(self):\n",
        "\t\treturn self.idx\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d8de3b2d654045e988864e058bd8393e",
            "ef6f44a7964641039d02c948ba04dfce",
            "04f77a870711447eb62d95dfcc8e34be",
            "d790bcb920fd4a4f8799548038d3e231",
            "9cba270f415e424bba34f02142130ceb",
            "03ebe18e32bb43f9b8e0b4df7d8832de",
            "61d10446d0f544ebb9de911502b77c03",
            "87387cb30dfe43bd951e2ac5799bec1b",
            "72b992f259ed4e899ef8c70c763e2b73",
            "022b8888f76f4ba7a8770cb761f25b66",
            "dba4e1a00b054aa883a001f011717153"
          ]
        },
        "id": "5mvqjD4236UQ",
        "outputId": "37343a52-fefd-441e-ebef-5855c22b4dbe"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "from torchvision import datasets, transforms\n",
        "from typing import List\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = datasets.CIFAR10(root = '/',\n",
        "                                train = True, \n",
        "                                download = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8de3b2d654045e988864e058bd8393e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /cifar-10-python.tar.gz to /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mlo8Psm4uJf"
      },
      "source": [
        "mysampler = RandomSampler(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRHtfTUR5pnP",
        "outputId": "00961413-ce74-42b8-b896-e4731979d110"
      },
      "source": [
        "mysampler.shuffle()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48796,\n",
              " 23298,\n",
              " 32602,\n",
              " 26124,\n",
              " 40088,\n",
              " 28479,\n",
              " 34781,\n",
              " 12851,\n",
              " 2414,\n",
              " 76,\n",
              " 42008,\n",
              " 37601,\n",
              " 21280,\n",
              " 3652,\n",
              " 20023,\n",
              " 42362,\n",
              " 47993,\n",
              " 42801,\n",
              " 30962,\n",
              " 25989,\n",
              " 14899,\n",
              " 30898,\n",
              " 20913,\n",
              " 18525,\n",
              " 19872,\n",
              " 26380,\n",
              " 10834,\n",
              " 47625,\n",
              " 34945,\n",
              " 30001,\n",
              " 34364,\n",
              " 8086,\n",
              " 28684,\n",
              " 3211,\n",
              " 35353,\n",
              " 2396,\n",
              " 7825,\n",
              " 39139,\n",
              " 42523,\n",
              " 38673,\n",
              " 34449,\n",
              " 15628,\n",
              " 47535,\n",
              " 26132,\n",
              " 33883,\n",
              " 40070,\n",
              " 28391,\n",
              " 39984,\n",
              " 6097,\n",
              " 4427,\n",
              " 6660,\n",
              " 44855,\n",
              " 27541,\n",
              " 28418,\n",
              " 49927,\n",
              " 48442,\n",
              " 6064,\n",
              " 26068,\n",
              " 45774,\n",
              " 20774,\n",
              " 18006,\n",
              " 36527,\n",
              " 6158,\n",
              " 27918,\n",
              " 33381,\n",
              " 47754,\n",
              " 709,\n",
              " 45984,\n",
              " 35441,\n",
              " 35366,\n",
              " 20931,\n",
              " 35396,\n",
              " 39422,\n",
              " 13277,\n",
              " 45644,\n",
              " 16393,\n",
              " 41191,\n",
              " 31213,\n",
              " 34468,\n",
              " 38539,\n",
              " 34517,\n",
              " 31734,\n",
              " 14982,\n",
              " 31292,\n",
              " 1010,\n",
              " 6882,\n",
              " 48347,\n",
              " 26922,\n",
              " 14387,\n",
              " 21738,\n",
              " 9259,\n",
              " 20971,\n",
              " 34156,\n",
              " 17056,\n",
              " 44918,\n",
              " 6187,\n",
              " 17255,\n",
              " 11928,\n",
              " 38224,\n",
              " 7690,\n",
              " 27376,\n",
              " 33957,\n",
              " 33782,\n",
              " 3741,\n",
              " 49104,\n",
              " 11755,\n",
              " 958,\n",
              " 28240,\n",
              " 6596,\n",
              " 26389,\n",
              " 13711,\n",
              " 39906,\n",
              " 16018,\n",
              " 26972,\n",
              " 11532,\n",
              " 27866,\n",
              " 41026,\n",
              " 33485,\n",
              " 40027,\n",
              " 6789,\n",
              " 41818,\n",
              " 15347,\n",
              " 11987,\n",
              " 14003,\n",
              " 49505,\n",
              " 16199,\n",
              " 31396,\n",
              " 35831,\n",
              " 14966,\n",
              " 47204,\n",
              " 18403,\n",
              " 43103,\n",
              " 2125,\n",
              " 34472,\n",
              " 20255,\n",
              " 385,\n",
              " 14582,\n",
              " 29232,\n",
              " 46215,\n",
              " 5719,\n",
              " 49455,\n",
              " 46713,\n",
              " 22353,\n",
              " 1627,\n",
              " 40813,\n",
              " 25348,\n",
              " 747,\n",
              " 41955,\n",
              " 49908,\n",
              " 27935,\n",
              " 10785,\n",
              " 35028,\n",
              " 26316,\n",
              " 40219,\n",
              " 11101,\n",
              " 23104,\n",
              " 44755,\n",
              " 25356,\n",
              " 34113,\n",
              " 32854,\n",
              " 13141,\n",
              " 4009,\n",
              " 33781,\n",
              " 8142,\n",
              " 15238,\n",
              " 35076,\n",
              " 2778,\n",
              " 13667,\n",
              " 39209,\n",
              " 36921,\n",
              " 4855,\n",
              " 2732,\n",
              " 17163,\n",
              " 43214,\n",
              " 35463,\n",
              " 40879,\n",
              " 36237,\n",
              " 49184,\n",
              " 456,\n",
              " 24633,\n",
              " 12385,\n",
              " 43405,\n",
              " 44054,\n",
              " 13546,\n",
              " 43967,\n",
              " 26997,\n",
              " 21748,\n",
              " 49390,\n",
              " 22251,\n",
              " 8552,\n",
              " 17123,\n",
              " 48181,\n",
              " 24382,\n",
              " 10905,\n",
              " 27764,\n",
              " 25330,\n",
              " 9048,\n",
              " 12977,\n",
              " 450,\n",
              " 43511,\n",
              " 3765,\n",
              " 6714,\n",
              " 13159,\n",
              " 1922,\n",
              " 29576,\n",
              " 39491,\n",
              " 15692,\n",
              " 26746,\n",
              " 1391,\n",
              " 35919,\n",
              " 44702,\n",
              " 21527,\n",
              " 29400,\n",
              " 25845,\n",
              " 9216,\n",
              " 32311,\n",
              " 18202,\n",
              " 1109,\n",
              " 16256,\n",
              " 44149,\n",
              " 41494,\n",
              " 28201,\n",
              " 18900,\n",
              " 32895,\n",
              " 6215,\n",
              " 1605,\n",
              " 31905,\n",
              " 27817,\n",
              " 47076,\n",
              " 44716,\n",
              " 7128,\n",
              " 43342,\n",
              " 28289,\n",
              " 21321,\n",
              " 20710,\n",
              " 7693,\n",
              " 34944,\n",
              " 31229,\n",
              " 25054,\n",
              " 33030,\n",
              " 26259,\n",
              " 49444,\n",
              " 28204,\n",
              " 28964,\n",
              " 30094,\n",
              " 38413,\n",
              " 12629,\n",
              " 36881,\n",
              " 44711,\n",
              " 40061,\n",
              " 49765,\n",
              " 36051,\n",
              " 11416,\n",
              " 130,\n",
              " 6956,\n",
              " 9060,\n",
              " 12872,\n",
              " 10733,\n",
              " 44857,\n",
              " 2752,\n",
              " 46169,\n",
              " 16468,\n",
              " 14916,\n",
              " 42555,\n",
              " 19367,\n",
              " 31682,\n",
              " 26947,\n",
              " 32650,\n",
              " 21904,\n",
              " 8338,\n",
              " 47931,\n",
              " 45678,\n",
              " 23620,\n",
              " 36613,\n",
              " 7679,\n",
              " 43130,\n",
              " 17298,\n",
              " 35532,\n",
              " 34451,\n",
              " 30567,\n",
              " 6969,\n",
              " 34302,\n",
              " 16092,\n",
              " 25624,\n",
              " 10228,\n",
              " 15622,\n",
              " 30076,\n",
              " 20343,\n",
              " 42483,\n",
              " 14039,\n",
              " 3771,\n",
              " 35590,\n",
              " 40343,\n",
              " 25360,\n",
              " 18362,\n",
              " 16339,\n",
              " 46937,\n",
              " 13130,\n",
              " 23494,\n",
              " 6092,\n",
              " 5255,\n",
              " 47402,\n",
              " 25256,\n",
              " 30977,\n",
              " 39220,\n",
              " 48731,\n",
              " 37281,\n",
              " 10167,\n",
              " 22665,\n",
              " 39101,\n",
              " 37325,\n",
              " 28354,\n",
              " 33803,\n",
              " 30156,\n",
              " 30309,\n",
              " 46034,\n",
              " 26439,\n",
              " 32523,\n",
              " 37203,\n",
              " 46376,\n",
              " 31432,\n",
              " 5183,\n",
              " 38344,\n",
              " 3331,\n",
              " 22899,\n",
              " 25941,\n",
              " 34854,\n",
              " 23839,\n",
              " 10819,\n",
              " 40531,\n",
              " 9017,\n",
              " 48569,\n",
              " 6625,\n",
              " 31685,\n",
              " 38120,\n",
              " 10609,\n",
              " 48608,\n",
              " 2097,\n",
              " 45918,\n",
              " 37535,\n",
              " 19500,\n",
              " 19893,\n",
              " 46698,\n",
              " 35849,\n",
              " 21242,\n",
              " 39138,\n",
              " 24492,\n",
              " 41211,\n",
              " 24380,\n",
              " 48336,\n",
              " 18283,\n",
              " 22472,\n",
              " 41585,\n",
              " 31552,\n",
              " 41604,\n",
              " 12384,\n",
              " 316,\n",
              " 13930,\n",
              " 40591,\n",
              " 40559,\n",
              " 40654,\n",
              " 38954,\n",
              " 2686,\n",
              " 1351,\n",
              " 1733,\n",
              " 10059,\n",
              " 30377,\n",
              " 6834,\n",
              " 16691,\n",
              " 45537,\n",
              " 28932,\n",
              " 6054,\n",
              " 9660,\n",
              " 1485,\n",
              " 27189,\n",
              " 17058,\n",
              " 2205,\n",
              " 342,\n",
              " 27171,\n",
              " 4345,\n",
              " 3658,\n",
              " 35823,\n",
              " 21810,\n",
              " 24425,\n",
              " 41608,\n",
              " 6763,\n",
              " 23983,\n",
              " 1392,\n",
              " 35740,\n",
              " 27680,\n",
              " 508,\n",
              " 33373,\n",
              " 4923,\n",
              " 24405,\n",
              " 4141,\n",
              " 16204,\n",
              " 18240,\n",
              " 34268,\n",
              " 1680,\n",
              " 26210,\n",
              " 26715,\n",
              " 25956,\n",
              " 18790,\n",
              " 8849,\n",
              " 32326,\n",
              " 38283,\n",
              " 10015,\n",
              " 30949,\n",
              " 31058,\n",
              " 18054,\n",
              " 35397,\n",
              " 6668,\n",
              " 3520,\n",
              " 36883,\n",
              " 38741,\n",
              " 37710,\n",
              " 4699,\n",
              " 10694,\n",
              " 17703,\n",
              " 22881,\n",
              " 18266,\n",
              " 24356,\n",
              " 45873,\n",
              " 45164,\n",
              " 15512,\n",
              " 19792,\n",
              " 15773,\n",
              " 2449,\n",
              " 17003,\n",
              " 49014,\n",
              " 15213,\n",
              " 1204,\n",
              " 32922,\n",
              " 36557,\n",
              " 46564,\n",
              " 29198,\n",
              " 18693,\n",
              " 13413,\n",
              " 47346,\n",
              " 33565,\n",
              " 9693,\n",
              " 17773,\n",
              " 12963,\n",
              " 42605,\n",
              " 47659,\n",
              " 38962,\n",
              " 33911,\n",
              " 48062,\n",
              " 15191,\n",
              " 34725,\n",
              " 6309,\n",
              " 4732,\n",
              " 35230,\n",
              " 43824,\n",
              " 39224,\n",
              " 41437,\n",
              " 34880,\n",
              " 2063,\n",
              " 34682,\n",
              " 38203,\n",
              " 32041,\n",
              " 46731,\n",
              " 13503,\n",
              " 48869,\n",
              " 45399,\n",
              " 4424,\n",
              " 12152,\n",
              " 2496,\n",
              " 20265,\n",
              " 23039,\n",
              " 6216,\n",
              " 14673,\n",
              " 33129,\n",
              " 26058,\n",
              " 11750,\n",
              " 5554,\n",
              " 27833,\n",
              " 13082,\n",
              " 1425,\n",
              " 47061,\n",
              " 252,\n",
              " 43529,\n",
              " 3738,\n",
              " 43928,\n",
              " 23712,\n",
              " 24783,\n",
              " 8787,\n",
              " 18931,\n",
              " 26816,\n",
              " 48401,\n",
              " 46882,\n",
              " 28126,\n",
              " 17275,\n",
              " 30840,\n",
              " 41740,\n",
              " 19477,\n",
              " 7820,\n",
              " 19356,\n",
              " 26122,\n",
              " 9304,\n",
              " 28556,\n",
              " 12067,\n",
              " 9880,\n",
              " 15752,\n",
              " 5415,\n",
              " 17091,\n",
              " 20770,\n",
              " 30114,\n",
              " 45710,\n",
              " 11271,\n",
              " 7462,\n",
              " 48024,\n",
              " 23211,\n",
              " 27829,\n",
              " 29367,\n",
              " 17870,\n",
              " 3178,\n",
              " 38382,\n",
              " 21019,\n",
              " 20970,\n",
              " 13674,\n",
              " 22181,\n",
              " 31867,\n",
              " 6176,\n",
              " 49627,\n",
              " 7186,\n",
              " 42839,\n",
              " 26912,\n",
              " 3709,\n",
              " 12611,\n",
              " 29260,\n",
              " 21250,\n",
              " 5145,\n",
              " 13779,\n",
              " 38731,\n",
              " 11270,\n",
              " 34603,\n",
              " 36157,\n",
              " 35809,\n",
              " 47771,\n",
              " 47526,\n",
              " 18981,\n",
              " 15634,\n",
              " 12577,\n",
              " 25968,\n",
              " 47254,\n",
              " 49088,\n",
              " 16587,\n",
              " 9930,\n",
              " 25977,\n",
              " 38988,\n",
              " 19787,\n",
              " 34605,\n",
              " 20667,\n",
              " 31244,\n",
              " 42870,\n",
              " 30073,\n",
              " 9893,\n",
              " 33454,\n",
              " 30200,\n",
              " 37762,\n",
              " 11542,\n",
              " 17867,\n",
              " 29840,\n",
              " 15800,\n",
              " 233,\n",
              " 47773,\n",
              " 22704,\n",
              " 27973,\n",
              " 25264,\n",
              " 24026,\n",
              " 27115,\n",
              " 47321,\n",
              " 11154,\n",
              " 35917,\n",
              " 31,\n",
              " 2375,\n",
              " 27864,\n",
              " 10231,\n",
              " 11534,\n",
              " 7930,\n",
              " 25939,\n",
              " 10196,\n",
              " 3019,\n",
              " 35887,\n",
              " 10748,\n",
              " 42949,\n",
              " 2155,\n",
              " 21993,\n",
              " 39888,\n",
              " 40547,\n",
              " 17087,\n",
              " 36822,\n",
              " 24986,\n",
              " 2017,\n",
              " 2079,\n",
              " 8723,\n",
              " 25352,\n",
              " 27734,\n",
              " 11555,\n",
              " 6290,\n",
              " 36715,\n",
              " 21921,\n",
              " 14738,\n",
              " 10022,\n",
              " 43355,\n",
              " 22735,\n",
              " 32029,\n",
              " 28156,\n",
              " 6888,\n",
              " 28027,\n",
              " 38031,\n",
              " 12339,\n",
              " 38298,\n",
              " 19090,\n",
              " 8172,\n",
              " 4132,\n",
              " 32900,\n",
              " 48477,\n",
              " 16081,\n",
              " 17786,\n",
              " 26395,\n",
              " 14711,\n",
              " 14800,\n",
              " 43111,\n",
              " 4289,\n",
              " 19436,\n",
              " 25660,\n",
              " 22053,\n",
              " 29510,\n",
              " 23257,\n",
              " 17630,\n",
              " 2323,\n",
              " 14613,\n",
              " 19490,\n",
              " 25773,\n",
              " 25893,\n",
              " 17928,\n",
              " 30974,\n",
              " 15079,\n",
              " 23413,\n",
              " 29059,\n",
              " 22677,\n",
              " 16492,\n",
              " 43226,\n",
              " 47157,\n",
              " 25197,\n",
              " 27773,\n",
              " 27708,\n",
              " 14689,\n",
              " 48289,\n",
              " 46522,\n",
              " 8772,\n",
              " 40274,\n",
              " 9187,\n",
              " 16967,\n",
              " 32620,\n",
              " 8706,\n",
              " 39628,\n",
              " 20270,\n",
              " 22268,\n",
              " 21914,\n",
              " 9973,\n",
              " 29203,\n",
              " 12313,\n",
              " 45835,\n",
              " 34371,\n",
              " 39131,\n",
              " 26412,\n",
              " 11604,\n",
              " 16418,\n",
              " 28160,\n",
              " 41593,\n",
              " 26717,\n",
              " 17235,\n",
              " 964,\n",
              " 26880,\n",
              " 15912,\n",
              " 39961,\n",
              " 17496,\n",
              " 35784,\n",
              " 2130,\n",
              " 39500,\n",
              " 8341,\n",
              " 48925,\n",
              " 37180,\n",
              " 32725,\n",
              " 43528,\n",
              " 30523,\n",
              " 6270,\n",
              " 45141,\n",
              " 42030,\n",
              " 16760,\n",
              " 30924,\n",
              " 32526,\n",
              " 11761,\n",
              " 45909,\n",
              " 44027,\n",
              " 47327,\n",
              " 34833,\n",
              " 43554,\n",
              " 21153,\n",
              " 26504,\n",
              " 28504,\n",
              " 3593,\n",
              " 15457,\n",
              " 25994,\n",
              " 39940,\n",
              " 13833,\n",
              " 13690,\n",
              " 38196,\n",
              " 47225,\n",
              " 6423,\n",
              " 5137,\n",
              " 13656,\n",
              " 39345,\n",
              " 20156,\n",
              " 36484,\n",
              " 22529,\n",
              " 31222,\n",
              " 46900,\n",
              " 19664,\n",
              " 37453,\n",
              " 2347,\n",
              " 5730,\n",
              " 24347,\n",
              " 23653,\n",
              " 27843,\n",
              " 24913,\n",
              " 35862,\n",
              " 22533,\n",
              " 47789,\n",
              " 31225,\n",
              " 24066,\n",
              " 6070,\n",
              " 49556,\n",
              " 11103,\n",
              " 32259,\n",
              " 39379,\n",
              " 30767,\n",
              " 20312,\n",
              " 528,\n",
              " 43608,\n",
              " 47235,\n",
              " 20121,\n",
              " 638,\n",
              " 37191,\n",
              " 13304,\n",
              " 1000,\n",
              " 13220,\n",
              " 30450,\n",
              " 27018,\n",
              " 33766,\n",
              " 19657,\n",
              " 35326,\n",
              " 12411,\n",
              " 14791,\n",
              " 18953,\n",
              " 2904,\n",
              " 19935,\n",
              " 1008,\n",
              " 32466,\n",
              " 24644,\n",
              " 13207,\n",
              " 11641,\n",
              " 32765,\n",
              " 482,\n",
              " 7299,\n",
              " 30743,\n",
              " 33835,\n",
              " 31990,\n",
              " 26175,\n",
              " 4818,\n",
              " 36966,\n",
              " 22115,\n",
              " 26038,\n",
              " 26605,\n",
              " 21279,\n",
              " 21303,\n",
              " 39679,\n",
              " 21388,\n",
              " 23177,\n",
              " 5330,\n",
              " 3612,\n",
              " 14389,\n",
              " 39167,\n",
              " 12239,\n",
              " 26331,\n",
              " 48740,\n",
              " 35954,\n",
              " 17624,\n",
              " 30267,\n",
              " 5937,\n",
              " 13408,\n",
              " 49371,\n",
              " 16475,\n",
              " 10520,\n",
              " 7806,\n",
              " 21105,\n",
              " 36558,\n",
              " 44773,\n",
              " 31474,\n",
              " 32477,\n",
              " 32467,\n",
              " 20581,\n",
              " 5024,\n",
              " 4405,\n",
              " 47970,\n",
              " 7702,\n",
              " 17137,\n",
              " 44033,\n",
              " 11930,\n",
              " 18359,\n",
              " 39418,\n",
              " 37827,\n",
              " 47134,\n",
              " 7116,\n",
              " 30685,\n",
              " 8546,\n",
              " 28264,\n",
              " 28588,\n",
              " 44743,\n",
              " 39364,\n",
              " 18793,\n",
              " 8785,\n",
              " 24623,\n",
              " 38852,\n",
              " 34344,\n",
              " 22831,\n",
              " 16899,\n",
              " 29487,\n",
              " 18945,\n",
              " 28108,\n",
              " 15589,\n",
              " 41367,\n",
              " 47727,\n",
              " 33925,\n",
              " 36197,\n",
              " 43077,\n",
              " 40369,\n",
              " 14994,\n",
              " 38680,\n",
              " 33783,\n",
              " 18189,\n",
              " 32101,\n",
              " 12953,\n",
              " 14538,\n",
              " 24748,\n",
              " 42275,\n",
              " 5845,\n",
              " 35759,\n",
              " 283,\n",
              " 40751,\n",
              " 34917,\n",
              " 44950,\n",
              " 17319,\n",
              " 40277,\n",
              " 16357,\n",
              " 23078,\n",
              " 8897,\n",
              " 48325,\n",
              " 2101,\n",
              " 10147,\n",
              " 27504,\n",
              " 17075,\n",
              " 9264,\n",
              " 9512,\n",
              " 42308,\n",
              " 40330,\n",
              " 38967,\n",
              " 3431,\n",
              " 17728,\n",
              " 33262,\n",
              " 18586,\n",
              " 49693,\n",
              " 39021,\n",
              " 45895,\n",
              " 4610,\n",
              " 9842,\n",
              " 16675,\n",
              " 49124,\n",
              " 37628,\n",
              " 560,\n",
              " 42468,\n",
              " 31065,\n",
              " 29363,\n",
              " 23439,\n",
              " 16208,\n",
              " 22547,\n",
              " 19840,\n",
              " 20402,\n",
              " 13598,\n",
              " 24655,\n",
              " 21102,\n",
              " 37760,\n",
              " 34494,\n",
              " 31466,\n",
              " 44182,\n",
              " 1248,\n",
              " 16300,\n",
              " 37583,\n",
              " 46525,\n",
              " 38542,\n",
              " 21313,\n",
              " 15183,\n",
              " 10406,\n",
              " 35099,\n",
              " 47480,\n",
              " 22094,\n",
              " 9796,\n",
              " 31521,\n",
              " 28970,\n",
              " 6724,\n",
              " 27500,\n",
              " 3785,\n",
              " 18862,\n",
              " 34303,\n",
              " 44457,\n",
              " 28972,\n",
              " 33725,\n",
              " 29044,\n",
              " 1174,\n",
              " 28341,\n",
              " 26421,\n",
              " 27616,\n",
              " 34249,\n",
              " 27277,\n",
              " 3071,\n",
              " 24684,\n",
              " 37840,\n",
              " 24427,\n",
              " 30894,\n",
              " 14380,\n",
              " 26056,\n",
              " 14066,\n",
              " 35071,\n",
              " 7172,\n",
              " 15904,\n",
              " 30619,\n",
              " 35043,\n",
              " 7765,\n",
              " 45474,\n",
              " 28577,\n",
              " 44296,\n",
              " 40749,\n",
              " 13751,\n",
              " 21520,\n",
              " 39548,\n",
              " 9797,\n",
              " 48858,\n",
              " 21194,\n",
              " 14758,\n",
              " 30660,\n",
              " 42264,\n",
              " 16693,\n",
              " 18114,\n",
              " 15563,\n",
              " 30298,\n",
              " 5718,\n",
              " 8418,\n",
              " 14037,\n",
              " 29864,\n",
              " 15206,\n",
              " 288,\n",
              " 8616,\n",
              " 34933,\n",
              " 47274,\n",
              " 814,\n",
              " 47934,\n",
              " 7803,\n",
              " 18663,\n",
              " 33450,\n",
              " 49161,\n",
              " 15499,\n",
              " 44446,\n",
              " 25782,\n",
              " 35993,\n",
              " 18463,\n",
              " 15029,\n",
              " 42125,\n",
              " 3696,\n",
              " 1607,\n",
              " 20538,\n",
              " 1408,\n",
              " 3725,\n",
              " 1273,\n",
              " 8452,\n",
              " 14729,\n",
              " 18943,\n",
              " 19573,\n",
              " 47710,\n",
              " 31698,\n",
              " 35499,\n",
              " 21998,\n",
              " 32701,\n",
              " 162,\n",
              " 21095,\n",
              " 4028,\n",
              " 19350,\n",
              " 23009,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRXhJRo44Lu2"
      },
      "source": [
        "mydataloader = DataLoader(dataset, batch_size = 32, sampler = mysampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhJhtdgi5E0K"
      },
      "source": [
        "mysampler.get_order()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEDkkTvMIviE",
        "outputId": "271ee4e2-90fb-4b16-8eb0-3d3f8c6344ee"
      },
      "source": [
        "test = iter(range(10))\n",
        "test2 = iter(range(10))\n",
        "\n",
        "for i, j in zip(test, test2):\n",
        "    print(i,j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "1 1\n",
            "2 2\n",
            "3 3\n",
            "4 4\n",
            "5 5\n",
            "6 6\n",
            "7 7\n",
            "8 8\n",
            "9 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N4aG5A_UYFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c534dc0-5d2a-4cd7-ead4-b1eefecd1e37"
      },
      "source": [
        "type(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT-QobxmUZJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3763784-7201-4366-ec97-dccd119b2901"
      },
      "source": [
        "type(range(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwvoYHOhO9Wi",
        "outputId": "d6baa884-c667-4d78-b7f8-51387830e2f6"
      },
      "source": [
        "!git clone https://github.com/nikhilanand91/Forget.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Forget'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Counting objects: 100% (456/456), done.\u001b[K\n",
            "remote: Compressing objects: 100% (303/303), done.\u001b[K\n",
            "remote: Total 456 (delta 231), reused 349 (delta 127), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (456/456), 75.91 KiB | 9.49 MiB/s, done.\n",
            "Resolving deltas: 100% (231/231), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSaz_vYQHiat"
      },
      "source": [
        "!rm -r Forget/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CbCNVAAPEXd",
        "outputId": "40c52fd9-9bac-4f28-c6db-747031f65967"
      },
      "source": [
        "!python Forget/robust.py train --model 'cifar_resnet_20' --dataset 'cifar10' --output_location '/testdir/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "tensor(0.3106)\n",
            "tensor(0.3632)\n",
            "tensor(0.3969)\n",
            "tensor(0.4230)\n",
            "tensor(0.4445)\n",
            "tensor(0.4628)\n",
            "tensor(0.4789)\n",
            "tensor(0.4933)\n",
            "tensor(0.5063)\n",
            "tensor(0.5181)\n",
            "Saving to /testdir/CorrectExamples.pkl\n",
            "Done!\n",
            "Saving to /testdir/ExampleOrder.pkl\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBsLNhgpZMrA"
      },
      "source": [
        "my_list = range(10)\n",
        "chunk_size = 2\n",
        "def chunk(chunk_size):\n",
        "    return [my_list[i:i + chunk_size] for i in range(0, len(my_list), chunk_size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhvJfM-lV3Tt"
      },
      "source": [
        "import pickle\n",
        "file = open('/testdir/CorrectExamples.pkl', 'rb')\n",
        "loaded_object = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB1IPFITaS7R",
        "outputId": "e340d8d0-60ef-4d6d-b667-b2bf255ebbd9"
      },
      "source": [
        "list(loaded_object.keys())[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 3909)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khDo7IvmrsIA",
        "outputId": "fad539af-4970-48b2-949c-678bb177a111"
      },
      "source": [
        "print(loaded_object[0,80][0:100])\n",
        "print(len(loaded_object[0,80]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
            "        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1.])\n",
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ohTuvKWs80N",
        "outputId": "efbf47a3-8b85-418c-ae92-dacf02832978"
      },
      "source": [
        "ct = 0\n",
        "for i in loaded_object[0, 0]:\n",
        "    if i==1. or i==1:\n",
        "        ct+=1\n",
        "print(ct/50000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.62498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IC8vNv-sSbI",
        "outputId": "14d6130b-6577-4e79-d926-ae4efa0309c8"
      },
      "source": [
        "ct = 0\n",
        "for i in loaded_object[9, 3909]:\n",
        "    if i==1. or i==1:\n",
        "        ct+=1\n",
        "print(ct/50000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.62498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6vVWFZ1tAtO",
        "outputId": "05ed2329-6491-4e23-d5b0-e11375391595"
      },
      "source": [
        "all(loaded_object[0, 0] == loaded_object[9, 3909])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bibRs2lNs0Yi",
        "outputId": "4b95e5bd-fa4b-4301-d303-932ea1a075a8"
      },
      "source": [
        "print(ct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaJQDK5TD68"
      },
      "source": [
        "def test(cond):\n",
        "    return 2 if cond else 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if20ThTOTKTt",
        "outputId": "cd64b0c8-dee5-4cf8-9c28-6eaf82189478"
      },
      "source": [
        "test(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = {}\n",
        "test[3,4] = 3"
      ],
      "metadata": {
        "id": "nBtf587ZNNmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mykeys = [key for key in list(test.keys()) if key[0] == 3]"
      ],
      "metadata": {
        "id": "zCyR3RDBNPh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mykeys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtCTSNGVOjxh",
        "outputId": "5c978186-e493-4fb8-a905-08dab3beaef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ghFxP5ihRh"
      },
      "source": [
        "# Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aSlqPUeijT7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}